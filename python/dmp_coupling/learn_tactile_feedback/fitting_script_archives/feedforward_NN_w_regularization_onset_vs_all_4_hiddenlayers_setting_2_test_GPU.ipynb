{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Feedforward Neural Network with Regularization\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First load the data dumped by MATLAB (*.mat file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [],
   "source": [
    "# X_init_offset_cancelled = sio.loadmat('scraping/X_init_offset_cancelled_scraping.mat', struct_as_record=True)['X_init_offset_cancelled']\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_phasePSI_scraping.mat', struct_as_record=True)['Xioc_phasePSI']\n",
    "#X_234_all\n",
    "X_init_offset_cancelled_all= sio.loadmat('scraping/X_gauss_basis_func_scraping.mat', struct_as_record=True)['X_gauss_basis_func'].astype(np.float32)\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_PD_ratio_mean_3std_scraping.mat', struct_as_record=True)['Xioc_PD_ratio_mean_3std']\n",
    "# Ct_target = sio.loadmat('scraping/Ct_target_scraping.mat', struct_as_record=True)['Ct_target']\n",
    "\n",
    "#X_234\n",
    "X_init_offset_cancelled = sio.loadmat('scraping/X_gauss_basis_func_scraping_setting_2.mat', struct_as_record=True)['X_gauss_basis_func'].astype(np.float32)\n",
    "#Ct_target_234\n",
    "Ct_target = sio.loadmat('scraping/Ct_target_filt_scraping_setting_2.mat', struct_as_record=True)['Ct_target_filt'].astype(np.float32)\n",
    "\n",
    "# Dataset for Extrapolation Test\n",
    "#X_5toend\n",
    "# X_extrapolate_test = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_test.mat', struct_as_record=True)['X_gauss_basis_func_test'].astype(np.float32)\n",
    "#Ct_5toend\n",
    "# Ctt_extrapolate_test = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_test.mat', struct_as_record=True)['Ct_target_filt_test'].astype(np.float32)\n",
    "\n",
    "# Dummy Data for learning simulation/verification:\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/dummy_X.mat', struct_as_record=True)['X']\n",
    "# Ct_target = sio.loadmat('scraping/dummy_Ct.mat', struct_as_record=True)['Ct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Verify the dimensions are correct and shuffle the data (for Stochastic Gradient Descent (SGD)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init_offset_cancelled.shape = (16095, 250)\n",
      "Ct_target.shape = (16095, 6)\n",
      "N_data   = 16095\n",
      "D_input  = 250\n",
      "D_output = 6\n",
      "N_train_dataset_bin = 13500\n",
      "N_valid_dataset_bin = 1200\n",
      "N_test_dataset_bin  = 1200\n"
     ]
    }
   ],
   "source": [
    "# N_data_extrapolate_test = Ctt_extrapolate_test.shape[0]\n",
    "# permutation_extrapolate_test = np.random.permutation(N_data_extrapolate_test)\n",
    "# permutation_extrapolate_test_select = permutation_extrapolate_test[1:1000]\n",
    "# X_extrapt = X_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "# Ctt_extrapt = Ctt_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "\n",
    "print('X_init_offset_cancelled.shape =', X_init_offset_cancelled.shape)\n",
    "print('Ct_target.shape =', Ct_target.shape)\n",
    "\n",
    "N_data = Ct_target.shape[0]\n",
    "D_input = X_init_offset_cancelled.shape[1]\n",
    "D_output = Ct_target.shape[1]\n",
    "print('N_data   =', N_data)\n",
    "print('D_input  =', D_input)\n",
    "print('D_output =', D_output)\n",
    "\n",
    "# print('X_extrapolate_test.shape =', X_extrapolate_test.shape)\n",
    "# print('Ctt_extrapolate_test.shape =', Ctt_extrapolate_test.shape)\n",
    "\n",
    "random.seed(38)\n",
    "np.random.seed(38)\n",
    "\n",
    "X_init_offset_cancelled = X_init_offset_cancelled\n",
    "X_init_offset_cancelled_all = X_init_offset_cancelled_all\n",
    "\n",
    "# permutation = np.random.permutation(N_data)\n",
    "# X_shuffled = X_init_offset_cancelled[permutation,:]\n",
    "# Ct_target_shuffled = Ct_target[permutation,:]\n",
    "\n",
    "fraction_train_dataset = 0.85\n",
    "fraction_test_dataset  = 0.075\n",
    "\n",
    "# N_train_dataset = np.round(fraction_train_dataset * N_data).astype(int)\n",
    "# N_test_dataset = np.round(fraction_test_dataset * N_data).astype(int)\n",
    "# N_valid_dataset = N_data - N_train_dataset - N_test_dataset\n",
    "# print('N_train_dataset =', N_train_dataset)\n",
    "# print('N_valid_dataset =', N_valid_dataset)\n",
    "# print('N_test_dataset  =', N_test_dataset)\n",
    "\n",
    "# X_train_dataset = X_shuffled[0:N_train_dataset,:]\n",
    "# Ct_train = Ct_target_shuffled[0:N_train_dataset,:]\n",
    "# X_valid_dataset = X_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "# Ct_valid = Ct_target_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "# X_test_dataset = X_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]\n",
    "# Ct_test = Ct_target_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]\n",
    "bin_size = 300\n",
    "original_index = np.arange(N_data)\n",
    "begin_index = np.arange(np.floor(N_data/bin_size).astype(int))*bin_size\n",
    "np.random.shuffle(begin_index)\n",
    "X_train_bin_num = round(begin_index.shape[0]*fraction_train_dataset)\n",
    "X_test_bin_num = round(begin_index.shape[0]*fraction_test_dataset)\n",
    "train_bin_shuffled_index = np.array([])\n",
    "test_bin_shuffled_index = np.array([])\n",
    "valid_bin_shuffled_index = np.array([])\n",
    "for index in range(0, X_train_bin_num):\n",
    "    tmp = original_index[begin_index[index]:begin_index[index]+bin_size:1]\n",
    "    train_bin_shuffled_index = np.concatenate([train_bin_shuffled_index, tmp])\n",
    "for index in range(X_train_bin_num, X_train_bin_num+X_test_bin_num):\n",
    "    tmp = original_index[begin_index[index]:begin_index[index]+bin_size:1]\n",
    "    test_bin_shuffled_index = np.concatenate([test_bin_shuffled_index, tmp])   \n",
    "for index in range(X_train_bin_num+X_test_bin_num, X_train_bin_num+2*X_test_bin_num):\n",
    "    tmp = original_index[begin_index[index]:begin_index[index]+bin_size:1]\n",
    "    valid_bin_shuffled_index = np.concatenate([valid_bin_shuffled_index, tmp])   \n",
    "print('N_train_dataset_bin =', train_bin_shuffled_index.size)\n",
    "print('N_valid_dataset_bin =', valid_bin_shuffled_index.size)\n",
    "print('N_test_dataset_bin  =', test_bin_shuffled_index.size)\n",
    "X_train_dataset_bin = X_init_offset_cancelled[train_bin_shuffled_index.astype(int),:]\n",
    "Ct_train_bin = Ct_target[train_bin_shuffled_index.astype(int),:]\n",
    "X_valid_dataset_bin = X_init_offset_cancelled[valid_bin_shuffled_index.astype(int),:]\n",
    "Ct_valid_bin = Ct_target[valid_bin_shuffled_index.astype(int),:]\n",
    "X_test_dataset_bin = X_init_offset_cancelled[test_bin_shuffled_index.astype(int),:]\n",
    "Ct_test_bin = Ct_target[test_bin_shuffled_index.astype(int),:]\n",
    "X_train_dataset = X_train_dataset_bin\n",
    "Ct_train = Ct_train_bin\n",
    "X_valid_dataset = X_valid_dataset_bin\n",
    "Ct_valid = Ct_valid_bin\n",
    "X_test_dataset = X_test_dataset_bin\n",
    "Ct_test = Ct_test_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def computeNMSE(predictions, labels):\n",
    "    mse = np.mean(np.square(predictions-labels), axis=0);\n",
    "    var_labels = np.var(labels, axis=0)\n",
    "    nmse = np.divide(mse, var_labels)\n",
    "    return (nmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Feed-Forward Neural Network Model\n",
    "---------\n",
    "\n",
    "Here it goes:\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 310075.625000\n",
      "Training NMSE  :  [ 1.0076561   1.04297042  1.02857709  1.12066329  1.20315635  1.18259013]\n",
      "Validation NMSE:  [ 1.01670468  1.07225275  1.13163805  1.07940805  1.12625098  1.06128192]\n",
      "Minibatch loss at step 500: 247328.312500\n",
      "Training NMSE  :  [ 1.00316751  1.01608527  1.00586891  1.07848001  1.17413139  1.14262462]\n",
      "Validation NMSE:  [ 1.01006866  1.03454697  1.06499004  1.04218721  1.09379447  1.03136671]\n",
      "Minibatch loss at step 1000: 155780.968750\n",
      "Training NMSE  :  [ 1.00206971  1.0109098   1.00326324  1.06506193  1.16263103  1.12832916]\n",
      "Validation NMSE:  [ 1.00817716  1.02650714  1.05390584  1.03129232  1.08162165  1.02236402]\n",
      "Minibatch loss at step 1500: 31027.410156\n",
      "Training NMSE  :  [ 1.00161421  1.00815821  1.0019325   1.05625665  1.15444386  1.11852014]\n",
      "Validation NMSE:  [ 1.0073055   1.02197182  1.04700589  1.02451861  1.07323241  1.0168308 ]\n",
      "Minibatch loss at step 2000: 11555.747070\n",
      "Training NMSE  :  [ 1.00096118  1.00601768  1.00117218  1.05009937  1.14768577  1.11059558]\n",
      "Validation NMSE:  [ 1.00592601  1.01825547  1.04221094  1.01999307  1.06648636  1.0127964 ]\n",
      "Minibatch loss at step 2500: 70097.812500\n",
      "Training NMSE  :  [ 1.00082386  1.00497925  1.00084078  1.04468143  1.14185429  1.10389221]\n",
      "Validation NMSE:  [ 1.00560272  1.01635182  1.03974211  1.01619339  1.06081462  1.00972092]\n",
      "Minibatch loss at step 3000: 140932.375000\n",
      "Training NMSE  :  [ 1.00056767  1.00392878  1.00052094  1.04038763  1.13720536  1.09859979]\n",
      "Validation NMSE:  [ 1.00494862  1.01434326  1.03693831  1.01331854  1.05639708  1.00753117]\n",
      "Minibatch loss at step 3500: 112907.937500\n",
      "Training NMSE  :  [ 1.00044668  1.00322294  1.00040829  1.03687823  1.13276732  1.09365392]\n",
      "Validation NMSE:  [ 1.00460899  1.01291847  1.03574407  1.01107955  1.05226636  1.00569701]\n",
      "Minibatch loss at step 4000: 34163.347656\n",
      "Training NMSE  :  [ 1.00046551  1.00281107  1.00032556  1.03361917  1.12875795  1.08925343]\n",
      "Validation NMSE:  [ 1.00465035  1.01204741  1.0347774   1.00909972  1.04861081  1.00424528]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-714c700ec304>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# in the list passed to sess.run() and the value tensors will be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# returned in the tuple from the call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# write log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 769\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    770\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 967\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    968\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1017\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1018\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1005\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 64\n",
    "num_steps  = 700001\n",
    "\n",
    "# Number of units in hidden layer\n",
    "N_HIDDEN1_UNITS = 250\n",
    "N_HIDDEN2_UNITS = 125\n",
    "N_HIDDEN3_UNITS = 64\n",
    "N_HIDDEN4_UNITS = 32\n",
    "\n",
    "# L2 Regularizer constant\n",
    "beta1 = 0.0000000001\n",
    "# beta1 = 0.0\n",
    "\n",
    "logs_path = \"/tmp/ffnn/\"\n",
    "\n",
    "def defineFeedForwardNeuralNetworkModel(input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, num_hidden4_units, output_size):\n",
    "    # Hidden 1 Layer\n",
    "    with tf.variable_scope('hidden1', reuse=False):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 2 Layer\n",
    "    with tf.variable_scope('hidden2', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 3 Layer\n",
    "    with tf.variable_scope('hidden3', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 4 Layer\n",
    "    with tf.variable_scope('hidden4', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, num_hidden4_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden4_units], initializer=tf.constant_initializer(0))\n",
    "    # Linear (Output) Layer\n",
    "    with tf.variable_scope('linear', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden4_units, output_size], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [output_size], initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Build prediction graph.\n",
    "def performFeedForwardNeuralNetworkPrediction(train_dataset, input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, num_hidden4_units, output_size, dropout_keep_prob):\n",
    "    \"\"\"Build the Feed-Forward Neural Network model for prediction.\n",
    "    Args:\n",
    "        train_dataset: training dataset's placeholder.\n",
    "        num_hidden1_units: Size of the 1st hidden layer.\n",
    "    Returns:\n",
    "        outputs: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1 = tf.nn.relu(tf.matmul(train_dataset, weights) + biases)\n",
    "#         hidden1 = tf.matmul(train_dataset, weights) + biases\n",
    "        hidden1_drop = tf.nn.dropout(hidden1, dropout_keep_prob)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1_drop, weights) + biases)\n",
    "        hidden2_drop = tf.nn.dropout(hidden2, dropout_keep_prob)\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2_drop, weights) + biases)\n",
    "        hidden3_drop = tf.nn.dropout(hidden3, dropout_keep_prob)\n",
    "    # Hidden 4\n",
    "    with tf.variable_scope('hidden4', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, num_hidden4_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden4_units])\n",
    "        hidden4 = tf.nn.relu(tf.matmul(hidden3_drop, weights) + biases)\n",
    "        hidden4_drop = tf.nn.dropout(hidden4, dropout_keep_prob)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden4_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        outputs = tf.matmul(hidden4_drop, weights) + biases\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Build training graph.\n",
    "def performFeedForwardNeuralNetworkTraining(outputs, labels, initial_learning_rate, input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, num_hidden4_units, output_size):\n",
    "    \"\"\"Build the training graph.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Output tensor, float - [BATCH_SIZE, output_size].\n",
    "        labels : Labels tensor, float - [BATCH_SIZE, output_size].\n",
    "        initial_learning_rate: The initial learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates L2 prediction loss.\n",
    "    pred_l2_loss = tf.nn.l2_loss(outputs - labels, name='my_pred_l2_loss')\n",
    "    \n",
    "    # Create an operation that calculates L2 loss.\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        hidden2_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        hidden3_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 4\n",
    "    with tf.variable_scope('hidden4', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, num_hidden4_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden4_units])\n",
    "        hidden4_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden4_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        output_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    \n",
    "    loss = tf.reduce_mean(pred_l2_loss, name='my_pred_l2_loss_mean') + (beta1 * (hidden1_layer_l2_loss + hidden2_layer_l2_loss + hidden3_layer_l2_loss + hidden4_layer_l2_loss + output_layer_l2_loss))\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Exponentially-decaying learning rate:\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, 0.1)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "#     train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     train_op = tf.train.MomentumOptimizer(learning_rate, momentum=learning_rate/4.0, use_nesterov=True).minimize(loss, global_step=global_step)\n",
    "    train_op = tf.train.AdagradOptimizer(initial_learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return train_op, loss, learning_rate\n",
    "\n",
    "# Save model.\n",
    "def saveFeedForwardNeuralNetworkToMATLABMatFile(input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, num_hidden4_units, output_size):\n",
    "    model_params={}\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        model_params['weights_1']=weights.eval()\n",
    "        model_params['biases_1']=biases.eval()\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        model_params['weights_2']=weights.eval()\n",
    "        model_params['biases_2']=biases.eval()\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        model_params['weights_3']=weights.eval()\n",
    "        model_params['biases_3']=biases.eval()\n",
    "    # Hidden 4\n",
    "    with tf.variable_scope('hidden4', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, num_hidden4_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden4_units])\n",
    "        model_params['weights_4']=weights.eval()\n",
    "        model_params['biases_4']=biases.eval()\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden4_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        model_params['weights_out']=weights.eval()\n",
    "        model_params['biases_out']=biases.eval()\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "# def matmul_on_gpu(n):\n",
    "#   if n.type == \"MatMul\":\n",
    "#     return \"/gpu:0\"\n",
    "#   else:\n",
    "#     return \"/cpu:0\"\n",
    "# Build the complete graph for feeding inputs, training, and saving checkpoints.\n",
    "ff_nn_graph = tf.Graph()\n",
    "with ff_nn_graph.as_default():\n",
    "#     with ff_nn_graph.device('/cpu:0'):\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=[batch_size, D_input], name=\"tf_train_dataset_placeholder\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=[batch_size, D_output], name=\"tf_train_labels_placeholder\")\n",
    "    tf_train_all_dataset = tf.constant(X_train_dataset, name=\"tf_train_all_dataset_constant\")\n",
    "    tf_valid_dataset = tf.constant(X_valid_dataset, name=\"tf_valid_dataset_constant\")\n",
    "    tf_test_dataset = tf.constant(X_test_dataset, name=\"tf_test_dataset_constant\")\n",
    "    #     tf_whole_dataset = tf.constant(X_init_offset_cancelled, name=\"tf_whole_dataset_constant\")\n",
    "    #     tf_whole_all_dataset = tf.constant(X_init_offset_cancelled_all, name=\"tf_whole_all_dataset_constant\")\n",
    "    #     tf_extrapolate_test_dataset = tf.constant(X_extrapt, name=\"tf_extrapolate_test_dataset_constant\")\n",
    "\n",
    "    # Currently turn off dropouts:\n",
    "    tf_train_dropout_keep_prob = 0.77\n",
    "#     tf_train_dropout_keep_prob = 1.0\n",
    "    \n",
    "    # Define the Neural Network model.\n",
    "    defineFeedForwardNeuralNetworkModel(D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output)\n",
    "\n",
    "    # Build the Prediction Graph (that computes predictions from the inference model).\n",
    "    tf_outputs = performFeedForwardNeuralNetworkPrediction(tf_train_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, tf_train_dropout_keep_prob)\n",
    "\n",
    "    # Build the Training Graph (that calculate and apply gradients).\n",
    "    train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.1, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output)\n",
    "    #     train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.00001, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output)\n",
    "\n",
    "    # Create a summary:\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf_outputs\n",
    "    train_all_prediction = performFeedForwardNeuralNetworkPrediction(tf_train_all_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    valid_prediction = performFeedForwardNeuralNetworkPrediction(tf_valid_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    test_prediction  = performFeedForwardNeuralNetworkPrediction(tf_test_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    #     whole_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    #     whole_all_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_all_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    #     extrapolate_test_prediction = performFeedForwardNeuralNetworkPrediction(tf_extrapolate_test_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "\n",
    "# Run training for num_steps and save checkpoint at the end.\n",
    "with tf.Session(graph=ff_nn_graph) as session:\n",
    "    # Run the Op to initialize the variables.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(num_steps):\n",
    "#         start_time = time.time()\n",
    "        # Read a batch of input dataset and labels.\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (Ct_train.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = X_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = Ct_train[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value, predictions, summary = session.run([train_op, loss, train_prediction, summary_op], feed_dict=feed_dict)\n",
    "        \n",
    "        # write log\n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, loss_value))\n",
    "#             print(\"Minibatch NMSE : \", computeNMSE(predictions, batch_labels))\n",
    "            print(\"Training NMSE  : \", computeNMSE(train_all_prediction.eval(), Ct_train))\n",
    "            print(\"Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "#             print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "\n",
    "#             print(\"Extrapolation NMSE: \", computeNMSE(extrapolate_test_prediction.eval(), Ctt_extrapt))\n",
    "#         if (step % 5000 == 0):\n",
    "#             model_params = saveFeedForwardNeuralNetworkToMATLABMatFile(D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output)\n",
    "#             print(\"Logging model_params.mat ...\")\n",
    "#             sio.savemat('model_params/model_params_4hl.mat', model_params)\n",
    "            \n",
    "#             whole_prediction_result = whole_prediction.eval()\n",
    "#             whole_prediction_result_dict={}\n",
    "#             whole_prediction_result_dict['whole_prediction_result'] = whole_prediction_result\n",
    "#             print(\"Logging Ct_fit_onset.mat ...\")\n",
    "#             sio.savemat('scraping/Ct_fit_onset_4hl.mat', whole_prediction_result_dict)\n",
    "#             whole_all_prediction_result = whole_all_prediction.eval()\n",
    "#             whole_all_prediction_result_dict={}\n",
    "#             whole_all_prediction_result_dict['whole_all_prediction_result'] = whole_all_prediction_result\n",
    "#             print(\"Logging Ct_fit_all.mat ...\")\n",
    "#             sio.savemat('scraping/Ct_fit_all_4hl.mat', whole_all_prediction_result_dict)\n",
    "    print(\"Final Training NMSE  : \", computeNMSE(train_all_prediction.eval(), Ct_train))\n",
    "    print(\"Final Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "    print(\"Final Test NMSE      : \", computeNMSE(test_prediction.eval(), Ct_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
