{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Feedforward Neural Network with Regularization\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First load the data dumped by MATLAB (*.mat file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [],
   "source": [
    "# X_init_offset_cancelled = sio.loadmat('scraping/X_init_offset_cancelled_scraping.mat', struct_as_record=True)['X_init_offset_cancelled']\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_phasePSI_scraping.mat', struct_as_record=True)['Xioc_phasePSI']\n",
    "#X_234_all\n",
    "X_init_offset_cancelled_all= sio.loadmat('scraping/X_gauss_basis_func_scraping.mat', struct_as_record=True)['X_gauss_basis_func'].astype(np.float32)\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_PD_ratio_mean_3std_scraping.mat', struct_as_record=True)['Xioc_PD_ratio_mean_3std']\n",
    "# Ct_target = sio.loadmat('scraping/Ct_target_scraping.mat', struct_as_record=True)['Ct_target']\n",
    "\n",
    "#X_234\n",
    "X_init_offset_cancelled = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_train.mat', struct_as_record=True)['X_gauss_basis_func_train'].astype(np.float32)\n",
    "#Ct_target_234\n",
    "Ct_target = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_train.mat', struct_as_record=True)['Ct_target_filt_train'].astype(np.float32)\n",
    "\n",
    "# Dataset for Extrapolation Test\n",
    "#X_5toend\n",
    "X_extrapolate_test = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_test.mat', struct_as_record=True)['X_gauss_basis_func_test'].astype(np.float32)\n",
    "#Ct_5toend\n",
    "Ctt_extrapolate_test = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_test.mat', struct_as_record=True)['Ct_target_filt_test'].astype(np.float32)\n",
    "\n",
    "# Dummy Data for learning simulation/verification:\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/dummy_X.mat', struct_as_record=True)['X']\n",
    "# Ct_target = sio.loadmat('scraping/dummy_Ct.mat', struct_as_record=True)['Ct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Verify the dimensions are correct and shuffle the data (for Stochastic Gradient Descent (SGD)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init_offset_cancelled.shape = (198563, 250)\n",
      "Ct_target.shape = (198563, 6)\n",
      "N_data   = 198563\n",
      "D_input  = 250\n",
      "D_output = 6\n",
      "X_extrapolate_test.shape = (128105, 250)\n",
      "Ctt_extrapolate_test.shape = (128105, 6)\n",
      "N_train_dataset = 168779\n",
      "N_valid_dataset = 14892\n",
      "N_test_dataset  = 14892\n"
     ]
    }
   ],
   "source": [
    "N_data_extrapolate_test = Ctt_extrapolate_test.shape[0]\n",
    "permutation_extrapolate_test = np.random.permutation(N_data_extrapolate_test)\n",
    "permutation_extrapolate_test_select = permutation_extrapolate_test[1:1000]\n",
    "X_extrapt = X_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "Ctt_extrapt = Ctt_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "\n",
    "print('X_init_offset_cancelled.shape =', X_init_offset_cancelled.shape)\n",
    "print('Ct_target.shape =', Ct_target.shape)\n",
    "\n",
    "N_data = Ct_target.shape[0]\n",
    "D_input = X_init_offset_cancelled.shape[1]\n",
    "D_output = Ct_target.shape[1]\n",
    "print('N_data   =', N_data)\n",
    "print('D_input  =', D_input)\n",
    "print('D_output =', D_output)\n",
    "\n",
    "print('X_extrapolate_test.shape =', X_extrapolate_test.shape)\n",
    "print('Ctt_extrapolate_test.shape =', Ctt_extrapolate_test.shape)\n",
    "\n",
    "random.seed(38)\n",
    "np.random.seed(38)\n",
    "\n",
    "X_init_offset_cancelled = X_init_offset_cancelled\n",
    "X_init_offset_cancelled_all = X_init_offset_cancelled_all\n",
    "\n",
    "permutation = np.random.permutation(N_data)\n",
    "X_shuffled = X_init_offset_cancelled[permutation,:]\n",
    "Ct_target_shuffled = Ct_target[permutation,:]\n",
    "\n",
    "fraction_train_dataset = 0.85\n",
    "fraction_test_dataset  = 0.075\n",
    "\n",
    "N_train_dataset = np.round(fraction_train_dataset * N_data).astype(int)\n",
    "N_test_dataset = np.round(fraction_test_dataset * N_data).astype(int)\n",
    "N_valid_dataset = N_data - N_train_dataset - N_test_dataset\n",
    "print('N_train_dataset =', N_train_dataset)\n",
    "print('N_valid_dataset =', N_valid_dataset)\n",
    "print('N_test_dataset  =', N_test_dataset)\n",
    "\n",
    "X_train_dataset = X_shuffled[0:N_train_dataset,:]\n",
    "Ct_train = Ct_target_shuffled[0:N_train_dataset,:]\n",
    "X_valid_dataset = X_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "Ct_valid = Ct_target_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "X_test_dataset = X_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]\n",
    "Ct_test = Ct_target_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def computeNMSE(predictions, labels):\n",
    "    mse = np.mean(np.square(predictions-labels), axis=0);\n",
    "    var_labels = np.var(labels, axis=0)\n",
    "    nmse = np.divide(mse, var_labels)\n",
    "    return (nmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Feed-Forward Neural Network Model\n",
    "---------\n",
    "\n",
    "Here it goes:\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 235617.687500\n",
      "Minibatch NMSE:  [ 1.1175257   1.03361785  1.00007999  1.05540442  1.00128663  1.03085387]\n",
      "Validation NMSE:  [ 1.00311339  1.00260222  1.00766826  1.04254067  1.00115383  1.00440347]\n",
      "Extrapolation NMSE:  [ 1.00288296  1.01737714  1.00361621  1.07060885  1.00194728  1.00200832]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 500: 264059.750000\n",
      "Minibatch NMSE:  [ 0.72291255  0.84252208  1.04588842  0.9510318   0.81109577  0.91382223]\n",
      "Validation NMSE:  [ 0.78776628  0.8740449   1.00243962  0.93705142  0.8966524   0.93360835]\n",
      "Extrapolation NMSE:  [ 0.89153451  0.89194405  1.00994706  0.96655422  0.93324161  0.94310671]\n",
      "Minibatch loss at step 1000: 372951.937500\n",
      "Minibatch NMSE:  [ 0.73656517  0.74443209  1.01455522  0.96211809  0.85463637  0.85013217]\n",
      "Validation NMSE:  [ 0.77086246  0.87236285  0.9978615   0.92815286  0.85033584  0.90452182]\n",
      "Extrapolation NMSE:  [ 0.85757059  0.88951939  0.99496388  0.96263695  0.91433704  0.93328983]\n",
      "Minibatch loss at step 1500: 446948.500000\n",
      "Minibatch NMSE:  [ 1.04207754  1.00776124  1.01977432  0.94493431  1.1304915   1.19921041]\n",
      "Validation NMSE:  [ 0.80398226  0.88323182  0.99709451  0.91755652  0.82977283  0.89610916]\n",
      "Extrapolation NMSE:  [ 0.85255522  0.88981926  1.0033555   0.95634025  0.91848707  0.943609  ]\n",
      "Minibatch loss at step 2000: 419826.625000\n",
      "Minibatch NMSE:  [ 0.79538018  0.96015775  0.98321939  0.82979465  0.7919609   0.84530127]\n",
      "Validation NMSE:  [ 0.80724287  0.88444418  0.99620926  0.90367591  0.81722081  0.88170117]\n",
      "Extrapolation NMSE:  [ 0.85922033  0.89076191  1.00203788  0.944677    0.91750717  0.92960191]\n",
      "Minibatch loss at step 2500: 153803.187500\n",
      "Minibatch NMSE:  [ 0.82605761  0.83635533  1.06099355  0.85131413  0.88739061  0.89826006]\n",
      "Validation NMSE:  [ 0.78934181  0.87233341  0.9938615   0.88786823  0.80403215  0.88049597]\n",
      "Extrapolation NMSE:  [ 0.84653276  0.87440741  1.00779295  0.92739242  0.92128879  0.93501943]\n",
      "Minibatch loss at step 3000: 303256.656250\n",
      "Minibatch NMSE:  [ 1.01801348  0.98293513  0.99018407  0.90784305  0.81653863  0.89202642]\n",
      "Validation NMSE:  [ 0.7895664   0.86589223  0.99628645  0.88880372  0.80356878  0.87126052]\n",
      "Extrapolation NMSE:  [ 0.85450011  0.86573094  1.00626218  0.93153483  0.90376967  0.92278063]\n",
      "Minibatch loss at step 3500: 363412.500000\n",
      "Minibatch NMSE:  [ 1.11179113  1.08125532  1.07183433  1.04475963  0.87936616  0.86718047]\n",
      "Validation NMSE:  [ 0.80562019  0.87891585  0.99378705  0.87954998  0.78996277  0.86846644]\n",
      "Extrapolation NMSE:  [ 0.84701025  0.87942356  0.99873     0.9227556   0.9031952   0.93111002]\n",
      "Minibatch loss at step 4000: 258078.156250\n",
      "Minibatch NMSE:  [ 1.04901159  0.99623871  1.00818682  0.89319348  0.63416755  0.82009906]\n",
      "Validation NMSE:  [ 0.80188769  0.87203288  0.99185115  0.87239736  0.77411628  0.85541779]\n",
      "Extrapolation NMSE:  [ 0.84903479  0.87673461  0.99913394  0.91908777  0.90106618  0.92281842]\n",
      "Minibatch loss at step 4500: 314490.406250\n",
      "Minibatch NMSE:  [ 0.84045404  0.92864871  0.99713808  0.86807674  0.84966516  0.80631453]\n",
      "Validation NMSE:  [ 0.81688607  0.8844642   0.99198896  0.87123901  0.76622725  0.85232645]\n",
      "Extrapolation NMSE:  [ 0.85622239  0.88291866  0.99729615  0.92008364  0.90960008  0.93491912]\n",
      "Minibatch loss at step 5000: 296980.687500\n",
      "Minibatch NMSE:  [ 0.60388267  0.82900423  0.96867925  0.89431792  0.89241773  0.83377522]\n",
      "Validation NMSE:  [ 0.78317833  0.86518574  0.99068958  0.8637045   0.76856816  0.85492289]\n",
      "Extrapolation NMSE:  [ 0.84207118  0.8696925   0.99036622  0.91289169  0.90814596  0.92760825]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 5500: 287300.687500\n",
      "Minibatch NMSE:  [ 1.07038045  1.00458264  1.03404355  1.00519753  0.70642698  0.91116613]\n",
      "Validation NMSE:  [ 0.78731585  0.86962056  0.99250448  0.86393458  0.75312388  0.83976716]\n",
      "Extrapolation NMSE:  [ 0.84560984  0.87488663  0.9955709   0.92073357  0.91030854  0.91920054]\n",
      "Minibatch loss at step 6000: 182852.937500\n",
      "Minibatch NMSE:  [ 0.60977119  0.81649649  1.03956568  0.72415739  0.74606895  0.84311676]\n",
      "Validation NMSE:  [ 0.796202    0.87233084  0.99384981  0.86012357  0.7540648   0.84049231]\n",
      "Extrapolation NMSE:  [ 0.8473559   0.87590951  0.99917603  0.92040205  0.90182698  0.92299497]\n",
      "Minibatch loss at step 6500: 214827.000000\n",
      "Minibatch NMSE:  [ 1.30819845  1.16723931  1.02698791  0.96214902  0.78682715  0.85115051]\n",
      "Validation NMSE:  [ 0.80369055  0.87395209  0.99121231  0.85335577  0.75362271  0.84151638]\n",
      "Extrapolation NMSE:  [ 0.85038489  0.87473583  0.99362838  0.91106713  0.89782512  0.92357165]\n",
      "Minibatch loss at step 7000: 255408.937500\n",
      "Minibatch NMSE:  [ 0.82794172  0.87885356  1.01080704  0.84697342  0.80940509  0.84134459]\n",
      "Validation NMSE:  [ 0.815844    0.88508922  0.99145085  0.85209894  0.7374565   0.83096826]\n",
      "Extrapolation NMSE:  [ 0.85684323  0.88224876  0.99465144  0.91638958  0.90531641  0.92810583]\n",
      "Minibatch loss at step 7500: 230960.265625\n",
      "Minibatch NMSE:  [ 1.11400867  1.00754428  1.02058315  0.9293865   0.9923566   0.90543401]\n",
      "Validation NMSE:  [ 0.79738051  0.87370479  0.99116236  0.84929162  0.73631561  0.83253497]\n",
      "Extrapolation NMSE:  [ 0.85082376  0.87473398  0.99772185  0.91299844  0.90789354  0.92934304]\n",
      "Minibatch loss at step 8000: 326920.500000\n",
      "Minibatch NMSE:  [ 0.99123067  1.04242361  0.99933624  0.8330639   1.17029667  1.20903862]\n",
      "Validation NMSE:  [ 0.77233505  0.86056811  0.98927259  0.8429119   0.73035449  0.8264792 ]\n",
      "Extrapolation NMSE:  [ 0.86402512  0.87248284  0.99566752  0.91040534  0.90689212  0.92488486]\n",
      "Minibatch loss at step 8500: 247167.093750\n",
      "Minibatch NMSE:  [ 1.26439679  1.06135547  1.00900269  0.90778941  0.76671565  0.8383199 ]\n",
      "Validation NMSE:  [ 0.7848596   0.86711043  0.99152482  0.84299159  0.72864866  0.82592523]\n",
      "Extrapolation NMSE:  [ 0.85454178  0.87463588  0.99812233  0.90948361  0.91134018  0.9339022 ]\n",
      "Minibatch loss at step 9000: 553287.500000\n",
      "Minibatch NMSE:  [ 0.77441454  0.85255724  0.92782462  0.96815223  0.73823386  0.79029876]\n",
      "Validation NMSE:  [ 0.78405124  0.86782509  0.99105072  0.84462345  0.71478033  0.81485415]\n",
      "Extrapolation NMSE:  [ 0.85043108  0.87628084  0.99733782  0.90266556  0.92413366  0.93929768]\n",
      "Minibatch loss at step 9500: 362859.968750\n",
      "Minibatch NMSE:  [ 0.70682907  0.8501882   1.01175475  0.79304856  1.19866109  1.19907343]\n",
      "Validation NMSE:  [ 0.7963528   0.87595302  0.99044544  0.84477776  0.72199064  0.82171404]\n",
      "Extrapolation NMSE:  [ 0.85144174  0.88159949  0.99286354  0.91317201  0.92587709  0.94060749]\n",
      "Minibatch loss at step 10000: 246315.562500\n",
      "Minibatch NMSE:  [ 0.88139963  0.89473641  0.98834217  0.91488421  0.74088818  0.7980994 ]\n",
      "Validation NMSE:  [ 0.79149419  0.86951494  0.98994482  0.84560907  0.72865832  0.8245874 ]\n",
      "Extrapolation NMSE:  [ 0.84898657  0.8752144   0.99279082  0.91485572  0.91090816  0.93216634]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 10500: 270283.531250\n",
      "Minibatch NMSE:  [ 0.98326677  0.94240648  1.11826444  0.97735256  0.87135822  1.05098367]\n",
      "Validation NMSE:  [ 0.7971049   0.87346059  0.99220479  0.84252268  0.70954579  0.81418532]\n",
      "Extrapolation NMSE:  [ 0.85350233  0.87894529  1.00010884  0.91057444  0.92036241  0.93097049]\n",
      "Minibatch loss at step 11000: 454492.687500\n",
      "Minibatch NMSE:  [ 0.7396189   0.85035062  1.01724279  0.81779498  0.69382882  0.80293584]\n",
      "Validation NMSE:  [ 0.79097724  0.87170297  0.99066025  0.8412292   0.70596397  0.8106299 ]\n",
      "Extrapolation NMSE:  [ 0.85210329  0.88101864  0.99479705  0.91067034  0.92934978  0.93720943]\n",
      "Minibatch loss at step 11500: 474717.968750\n",
      "Minibatch NMSE:  [ 1.0956881   1.03342509  1.01130164  0.90438676  0.90785134  0.98438239]\n",
      "Validation NMSE:  [ 0.79076999  0.87259656  0.99044043  0.84318453  0.7112875   0.81335592]\n",
      "Extrapolation NMSE:  [ 0.85177135  0.8801114   0.99088365  0.91178787  0.9206444   0.93330848]\n",
      "Minibatch loss at step 12000: 260326.718750\n",
      "Minibatch NMSE:  [ 0.75234532  0.87330914  0.99865383  0.99552894  0.35830536  0.46725091]\n",
      "Validation NMSE:  [ 0.78264219  0.86619085  0.99035412  0.84380299  0.70324129  0.80698788]\n",
      "Extrapolation NMSE:  [ 0.85543269  0.87357229  0.99257255  0.91723931  0.92563552  0.93516397]\n",
      "Minibatch loss at step 12500: 231063.437500\n",
      "Minibatch NMSE:  [ 1.04372931  0.99721771  0.98161215  0.89477891  0.57568091  0.79170978]\n",
      "Validation NMSE:  [ 0.81525248  0.88636309  0.99284059  0.83997071  0.70494956  0.81023103]\n",
      "Extrapolation NMSE:  [ 0.86115623  0.88753337  0.9979732   0.91190159  0.92636663  0.93612671]\n",
      "Minibatch loss at step 13000: 282298.687500\n",
      "Minibatch NMSE:  [ 0.82471091  1.05942988  1.07394457  0.95519823  0.79270947  0.87777102]\n",
      "Validation NMSE:  [ 0.80329341  0.87793469  0.99182701  0.84047538  0.70758313  0.8132906 ]\n",
      "Extrapolation NMSE:  [ 0.862921    0.87916023  0.99804211  0.90832359  0.92404372  0.9342072 ]\n",
      "Minibatch loss at step 13500: 176207.562500\n",
      "Minibatch NMSE:  [ 1.08688951  0.95178962  1.06175029  0.82438213  1.02721596  0.99579281]\n",
      "Validation NMSE:  [ 0.80629736  0.88186657  0.99245989  0.84091735  0.69868261  0.8038668 ]\n",
      "Extrapolation NMSE:  [ 0.86331552  0.88625044  0.99545938  0.90500921  0.93018788  0.9394303 ]\n",
      "Minibatch loss at step 14000: 643189.250000\n",
      "Minibatch NMSE:  [ 0.99597478  1.01146185  1.00934386  0.93638831  0.84911579  0.84265071]\n",
      "Validation NMSE:  [ 0.79772639  0.87432879  0.99194574  0.83614194  0.69882077  0.80457515]\n",
      "Extrapolation NMSE:  [ 0.86100703  0.88208437  0.99544913  0.90307343  0.93995363  0.95058382]\n",
      "Minibatch loss at step 14500: 182105.015625\n",
      "Minibatch NMSE:  [ 1.03927755  1.03704488  0.97202152  0.84312516  0.72877181  1.08439207]\n",
      "Validation NMSE:  [ 0.81359047  0.88536686  0.99146736  0.84000927  0.70318013  0.81026506]\n",
      "Extrapolation NMSE:  [ 0.86250949  0.89108932  0.99094301  0.90955538  0.92696935  0.94278383]\n",
      "Minibatch loss at step 15000: 280286.500000\n",
      "Minibatch NMSE:  [ 1.04451406  1.02195704  0.9617936   0.77205139  0.53583473  0.8604629 ]\n",
      "Validation NMSE:  [ 0.79174465  0.87689918  0.99081755  0.83176231  0.69014823  0.79634202]\n",
      "Extrapolation NMSE:  [ 0.86155558  0.88990837  0.99149221  0.90463835  0.93435705  0.94198602]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 15500: 177610.375000\n",
      "Minibatch NMSE:  [ 0.76915091  0.78186411  1.00543356  0.83448166  0.9642508   0.88418072]\n",
      "Validation NMSE:  [ 0.80487096  0.88037187  0.98972142  0.83798182  0.69564426  0.80425197]\n",
      "Extrapolation NMSE:  [ 0.86394161  0.88799918  0.99286258  0.90929127  0.92356241  0.9378953 ]\n",
      "Minibatch loss at step 16000: 168341.281250\n",
      "Minibatch NMSE:  [ 1.1426909   0.99069244  0.98755825  1.01997972  1.22837698  1.25496089]\n",
      "Validation NMSE:  [ 0.78384012  0.87118298  0.99073619  0.83364147  0.68339086  0.79091501]\n",
      "Extrapolation NMSE:  [ 0.86102831  0.88387769  0.99570948  0.9096415   0.93519568  0.9445498 ]\n",
      "Minibatch loss at step 16500: 326673.531250\n",
      "Minibatch NMSE:  [ 0.81914037  0.92989326  1.01103556  0.9780488   0.65807205  0.76057774]\n",
      "Validation NMSE:  [ 0.81154323  0.88634229  0.99122638  0.831119    0.67910236  0.78699881]\n",
      "Extrapolation NMSE:  [ 0.86303687  0.89008057  0.99800551  0.91090596  0.95433694  0.957021  ]\n",
      "Minibatch loss at step 17000: 314932.843750\n",
      "Minibatch NMSE:  [ 1.0277921   1.04070032  1.01075661  0.87        0.68148923  0.80680388]\n",
      "Validation NMSE:  [ 0.81066251  0.8852092   0.99031299  0.83138072  0.68479633  0.79464126]\n",
      "Extrapolation NMSE:  [ 0.86410147  0.88811076  0.99334455  0.90607619  0.9271996   0.94521326]\n",
      "Minibatch loss at step 17500: 279816.437500\n",
      "Minibatch NMSE:  [ 0.8000561   0.83388662  0.9593693   0.87288153  0.94075453  1.1097635 ]\n",
      "Validation NMSE:  [ 0.80968893  0.88276219  0.99006557  0.83338284  0.68329227  0.79262942]\n",
      "Extrapolation NMSE:  [ 0.86299896  0.88797039  0.99361342  0.90794188  0.94030976  0.94610387]\n",
      "Minibatch loss at step 18000: 257006.375000\n",
      "Minibatch NMSE:  [ 1.0658462   0.99614841  0.95651281  1.02036083  1.0171684   1.1253463 ]\n",
      "Validation NMSE:  [ 0.79312456  0.87416714  0.98983002  0.82921898  0.67956012  0.78853697]\n",
      "Extrapolation NMSE:  [ 0.85531998  0.883564    0.99206871  0.90678602  0.92621714  0.9449771 ]\n",
      "Minibatch loss at step 18500: 195195.687500\n",
      "Minibatch NMSE:  [ 0.63616794  0.66388947  1.0146513   0.85222697  0.72196525  0.85109121]\n",
      "Validation NMSE:  [ 0.78532356  0.87004811  0.99106455  0.83046472  0.68037033  0.79110026]\n",
      "Extrapolation NMSE:  [ 0.85865682  0.87959129  0.99576718  0.90412015  0.93225914  0.94323027]\n",
      "Minibatch loss at step 19000: 144017.156250\n",
      "Minibatch NMSE:  [ 0.555161    0.59162152  1.01019764  0.77031296  1.02605391  0.94729817]\n",
      "Validation NMSE:  [ 0.78613997  0.87163311  0.9909246   0.8290503   0.67153901  0.78297639]\n",
      "Extrapolation NMSE:  [ 0.85415006  0.87962657  0.99544978  0.90859044  0.95071936  0.95229149]\n",
      "Minibatch loss at step 19500: 200341.312500\n",
      "Minibatch NMSE:  [ 0.97995412  1.1351043   1.00531161  0.90797287  0.70359015  1.0388993 ]\n",
      "Validation NMSE:  [ 0.78307438  0.86844975  0.9913103   0.82753599  0.66801077  0.77911973]\n",
      "Extrapolation NMSE:  [ 0.85891515  0.87828749  0.99206674  0.9027558   0.9641788   0.96084142]\n",
      "Minibatch loss at step 20000: 446036.625000\n",
      "Minibatch NMSE:  [ 0.90768528  0.98862344  1.00573742  0.8731066   0.93750983  1.03495061]\n",
      "Validation NMSE:  [ 0.79615176  0.8740769   0.99084479  0.8315295   0.67301965  0.78331035]\n",
      "Extrapolation NMSE:  [ 0.8572787   0.88171321  0.99291313  0.91082859  0.96394247  0.96471363]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 20500: 476219.187500\n",
      "Minibatch NMSE:  [ 1.00222218  0.9967975   0.99412686  0.8352676   0.66487551  0.85623628]\n",
      "Validation NMSE:  [ 0.79249549  0.87324512  0.98996383  0.82734638  0.66658896  0.77841854]\n",
      "Extrapolation NMSE:  [ 0.85949063  0.88042772  0.99242574  0.90431249  0.96860272  0.96628964]\n",
      "Minibatch loss at step 21000: 637317.875000\n",
      "Minibatch NMSE:  [ 1.30848169  1.10106385  0.97119677  0.87919885  0.99074191  0.94687152]\n",
      "Validation NMSE:  [ 0.7961629   0.87685633  0.99091184  0.82438976  0.66602069  0.78261119]\n",
      "Extrapolation NMSE:  [ 0.86163384  0.88160926  0.99237752  0.9020406   0.95138842  0.95913243]\n",
      "Minibatch loss at step 21500: 306556.093750\n",
      "Minibatch NMSE:  [ 0.69970125  0.89284235  1.00782752  0.75931871  0.96815276  1.10623872]\n",
      "Validation NMSE:  [ 0.79921293  0.87643951  0.99170732  0.82481003  0.6622172   0.77729756]\n",
      "Extrapolation NMSE:  [ 0.86111844  0.8814947   0.99627113  0.90277708  0.95514798  0.95438498]\n",
      "Minibatch loss at step 22000: 274235.656250\n",
      "Minibatch NMSE:  [ 0.82656288  0.88600957  1.05290616  0.96110696  0.63543379  0.57464594]\n",
      "Validation NMSE:  [ 0.79400736  0.87492132  0.99069136  0.82216001  0.66185933  0.77589488]\n",
      "Extrapolation NMSE:  [ 0.86069554  0.87911224  0.99230313  0.90987366  0.95111436  0.95884359]\n",
      "Minibatch loss at step 22500: 338928.187500\n",
      "Minibatch NMSE:  [ 0.97773641  0.95464003  1.03903997  0.91363221  0.50224888  0.68237001]\n",
      "Validation NMSE:  [ 0.8223421   0.88894898  0.99012029  0.82389843  0.66438776  0.77881932]\n",
      "Extrapolation NMSE:  [ 0.86927062  0.88923502  0.99201375  0.90687126  0.95874029  0.96536553]\n",
      "Minibatch loss at step 23000: 246034.171875\n",
      "Minibatch NMSE:  [ 0.97956169  0.98811334  0.97734612  0.80609304  0.99658352  0.95651788]\n",
      "Validation NMSE:  [ 0.8045947   0.88429743  0.99058825  0.82310206  0.67176998  0.78259844]\n",
      "Extrapolation NMSE:  [ 0.86251026  0.88866782  0.99250066  0.90592521  0.94378257  0.95820588]\n",
      "Minibatch loss at step 23500: 395929.531250\n",
      "Minibatch NMSE:  [ 0.84599376  1.00287473  0.99452001  0.86718476  0.87408316  0.88704181]\n",
      "Validation NMSE:  [ 0.7848627   0.87081403  0.99057466  0.81965339  0.66488892  0.78112113]\n",
      "Extrapolation NMSE:  [ 0.86868685  0.87847924  0.9907676   0.90489078  0.94418865  0.9553203 ]\n",
      "Minibatch loss at step 24000: 225773.531250\n",
      "Minibatch NMSE:  [ 0.99743301  0.92479473  1.01649392  0.95741153  0.84650534  0.91026753]\n",
      "Validation NMSE:  [ 0.79004598  0.87604636  0.99151653  0.82158113  0.65728492  0.77207083]\n",
      "Extrapolation NMSE:  [ 0.86570644  0.88549876  0.99448442  0.90666169  0.95175356  0.95849645]\n",
      "Minibatch loss at step 24500: 217502.562500\n",
      "Minibatch NMSE:  [ 0.98164493  1.00724649  1.01938295  1.06393683  0.8007046   0.93696803]\n",
      "Validation NMSE:  [ 0.81242758  0.8878147   0.9906168   0.82196283  0.65262538  0.77046418]\n",
      "Extrapolation NMSE:  [ 0.86876613  0.89255655  0.99589419  0.91115397  0.96707833  0.96718979]\n",
      "Minibatch loss at step 25000: 564886.750000\n",
      "Minibatch NMSE:  [ 0.69230807  0.89572549  1.02773917  0.7778554   1.14245701  1.19173825]\n",
      "Validation NMSE:  [ 0.79113376  0.87718892  0.98942977  0.81702095  0.65711099  0.77129155]\n",
      "Extrapolation NMSE:  [ 0.85839647  0.88332289  0.98932457  0.90984935  0.95523715  0.95988053]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 25500: 252058.640625\n",
      "Minibatch NMSE:  [ 0.78248316  0.77896351  0.99936754  0.95087159  0.77470106  0.81321204]\n",
      "Validation NMSE:  [ 0.82577795  0.89329118  0.989362    0.81359756  0.65201998  0.76889396]\n",
      "Extrapolation NMSE:  [ 0.87477189  0.8912707   0.99185324  0.90456504  0.96258289  0.97568554]\n",
      "Minibatch loss at step 26000: 278503.687500\n",
      "Minibatch NMSE:  [ 0.79368031  0.84201843  0.95175415  0.87375224  0.7817601   0.78974682]\n",
      "Validation NMSE:  [ 0.78848022  0.87432683  0.98957837  0.81154633  0.65794194  0.77496958]\n",
      "Extrapolation NMSE:  [ 0.86545455  0.88300687  0.99032223  0.8949638   0.95738566  0.96786326]\n",
      "Minibatch loss at step 26500: 163314.593750\n",
      "Minibatch NMSE:  [ 0.76586294  0.84728456  0.99470365  0.91712719  0.4623881   0.73815465]\n",
      "Validation NMSE:  [ 0.78300261  0.87173873  0.98859394  0.81011415  0.64750504  0.76534241]\n",
      "Extrapolation NMSE:  [ 0.85966641  0.87876409  0.99046838  0.8996678   0.97626436  0.97267872]\n",
      "Minibatch loss at step 27000: 400912.125000\n",
      "Minibatch NMSE:  [ 1.15900493  1.07285082  1.01092017  0.91708183  0.65566331  0.74255687]\n",
      "Validation NMSE:  [ 0.79164505  0.87587845  0.98895597  0.8133871   0.64678305  0.76501733]\n",
      "Extrapolation NMSE:  [ 0.86328709  0.88314712  0.99057102  0.90356082  0.97727466  0.97641885]\n",
      "Minibatch loss at step 27500: 204568.406250\n",
      "Minibatch NMSE:  [ 0.62878364  0.92008185  1.01220548  0.85568893  0.52395624  0.6506024 ]\n",
      "Validation NMSE:  [ 0.80554652  0.88402408  0.98914343  0.81263781  0.64764971  0.76411945]\n",
      "Extrapolation NMSE:  [ 0.86430269  0.88713485  0.98937672  0.90289605  0.96059525  0.96926326]\n",
      "Minibatch loss at step 28000: 328041.875000\n",
      "Minibatch NMSE:  [ 0.8466661   0.85069799  1.03949082  0.76034015  1.13112175  1.07951081]\n",
      "Validation NMSE:  [ 0.7969541   0.87768644  0.98905194  0.81017578  0.6518839   0.76938987]\n",
      "Extrapolation NMSE:  [ 0.86235034  0.88420653  0.98787683  0.90109235  0.97328663  0.97927356]\n",
      "Minibatch loss at step 28500: 296535.187500\n",
      "Minibatch NMSE:  [ 0.93117636  0.90521491  1.01413524  0.90708566  1.02327955  0.96268976]\n",
      "Validation NMSE:  [ 0.79749471  0.88008004  0.98930287  0.80991858  0.65442687  0.76975787]\n",
      "Extrapolation NMSE:  [ 0.86597139  0.88715774  0.98922563  0.90333021  0.95102006  0.96505332]\n",
      "Minibatch loss at step 29000: 220810.000000\n",
      "Minibatch NMSE:  [ 0.73176801  0.864034    1.00246501  0.86646712  0.55677527  0.64747494]\n",
      "Validation NMSE:  [ 0.80284518  0.88466394  0.99008191  0.81021243  0.64135385  0.76148379]\n",
      "Extrapolation NMSE:  [ 0.86701739  0.89222479  0.99313444  0.90296781  0.97023505  0.9741177 ]\n",
      "Minibatch loss at step 29500: 496046.875000\n",
      "Minibatch NMSE:  [ 1.33044648  1.4731828   0.95904738  0.99179953  0.78247058  0.850348  ]\n",
      "Validation NMSE:  [ 0.81066525  0.88522232  0.99124402  0.81259775  0.64191008  0.7593587 ]\n",
      "Extrapolation NMSE:  [ 0.87203234  0.88895488  0.99399662  0.89833081  0.97699237  0.9746502 ]\n",
      "Minibatch loss at step 30000: 223776.906250\n",
      "Minibatch NMSE:  [ 0.98323655  0.99212027  0.98925161  0.81284344  0.88537884  0.85052156]\n",
      "Validation NMSE:  [ 0.79076034  0.87747192  0.99083203  0.81188542  0.64346129  0.7612049 ]\n",
      "Extrapolation NMSE:  [ 0.86372674  0.88766229  0.99171686  0.90901756  0.96510321  0.97276753]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 30500: 206946.906250\n",
      "Minibatch NMSE:  [ 0.94863909  1.05554307  0.99147499  0.92966747  0.84096444  0.97900742]\n",
      "Validation NMSE:  [ 0.81031597  0.88859153  0.98971456  0.81329256  0.6365177   0.75673872]\n",
      "Extrapolation NMSE:  [ 0.86921167  0.89213401  0.9910776   0.90763658  0.99400097  0.99686879]\n",
      "Minibatch loss at step 31000: 247598.187500\n",
      "Minibatch NMSE:  [ 0.8597433   0.91193759  0.9472509   0.82306576  0.89229119  0.99220568]\n",
      "Validation NMSE:  [ 0.80058676  0.88413668  0.99018931  0.81146973  0.63917965  0.76014501]\n",
      "Extrapolation NMSE:  [ 0.86692184  0.89028174  0.99255717  0.90482849  0.97367024  0.97917652]\n",
      "Minibatch loss at step 31500: 193542.218750\n",
      "Minibatch NMSE:  [ 1.07198203  1.07179058  0.98236352  0.91233301  0.5818733   0.75568193]\n",
      "Validation NMSE:  [ 0.79614598  0.88107729  0.98973864  0.81134319  0.64299619  0.76241714]\n",
      "Extrapolation NMSE:  [ 0.86637592  0.88656896  0.99246716  0.90539759  0.96698713  0.97951758]\n",
      "Minibatch loss at step 32000: 400799.062500\n",
      "Minibatch NMSE:  [ 0.66008854  0.89700007  1.00086153  0.8149122   0.69858891  0.79072756]\n",
      "Validation NMSE:  [ 0.78276068  0.87437266  0.99091095  0.81064224  0.64080739  0.75937414]\n",
      "Extrapolation NMSE:  [ 0.86557204  0.88367629  0.99355388  0.90828758  0.95717508  0.9636842 ]\n",
      "Minibatch loss at step 32500: 218496.640625\n",
      "Minibatch NMSE:  [ 0.84018922  0.8905248   1.02927554  0.90748787  0.92488813  0.97539169]\n",
      "Validation NMSE:  [ 0.79941857  0.88315636  0.98969334  0.8098678   0.63412553  0.7536875 ]\n",
      "Extrapolation NMSE:  [ 0.86687785  0.89060718  0.99219573  0.91549176  0.97847265  0.98406053]\n",
      "Minibatch loss at step 33000: 350207.687500\n",
      "Minibatch NMSE:  [ 0.99766022  1.00459683  0.98465186  0.92864084  0.52643222  0.65265179]\n",
      "Validation NMSE:  [ 0.81592     0.89118534  0.98993582  0.81294334  0.64089805  0.76033413]\n",
      "Extrapolation NMSE:  [ 0.870049    0.89393336  0.99061686  0.91289479  0.96373451  0.97558534]\n",
      "Minibatch loss at step 33500: 183132.781250\n",
      "Minibatch NMSE:  [ 0.87354237  0.98486072  1.00442946  0.75554603  0.70083237  0.926615  ]\n",
      "Validation NMSE:  [ 0.7982893   0.88321793  0.98996329  0.81041485  0.63456744  0.75493318]\n",
      "Extrapolation NMSE:  [ 0.87217844  0.88913804  0.99164408  0.90769047  0.9726814   0.97841096]\n",
      "Minibatch loss at step 34000: 136711.312500\n",
      "Minibatch NMSE:  [ 1.45455241  1.08352292  1.02880788  0.87156051  0.55304754  0.71030504]\n",
      "Validation NMSE:  [ 0.79945165  0.88213199  0.98896486  0.8109476   0.64104527  0.76037836]\n",
      "Extrapolation NMSE:  [ 0.86942703  0.89169186  0.98825336  0.90829414  0.9742924   0.9833768 ]\n",
      "Minibatch loss at step 34500: 134120.515625\n",
      "Minibatch NMSE:  [ 0.89430088  0.98738843  1.02068269  0.91347551  0.69410527  1.1126641 ]\n",
      "Validation NMSE:  [ 0.7850194   0.87568563  0.99030823  0.81381762  0.63360429  0.75582516]\n",
      "Extrapolation NMSE:  [ 0.86683655  0.88539678  0.99115402  0.9130407   0.95388258  0.9691903 ]\n",
      "Minibatch loss at step 35000: 271387.781250\n",
      "Minibatch NMSE:  [ 0.76876503  0.84327912  0.99938315  0.79020625  0.79662132  1.04421675]\n",
      "Validation NMSE:  [ 0.78809786  0.87611049  0.99097317  0.8076604   0.63108641  0.75471652]\n",
      "Extrapolation NMSE:  [ 0.86805433  0.88265961  0.99352539  0.90761876  0.96554106  0.97187132]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 35500: 244712.015625\n",
      "Minibatch NMSE:  [ 0.83566624  0.98901576  1.03299832  0.82652634  0.83635378  0.92437261]\n",
      "Validation NMSE:  [ 0.81273538  0.88897336  0.99066675  0.81040949  0.63916701  0.76158136]\n",
      "Extrapolation NMSE:  [ 0.87760824  0.89309007  0.99201012  0.9018656   0.9571225   0.96732175]\n",
      "Minibatch loss at step 36000: 279488.562500\n",
      "Minibatch NMSE:  [ 0.94114393  0.91725039  1.00355947  1.0338763   0.73131746  0.84454107]\n",
      "Validation NMSE:  [ 0.81215894  0.88871872  0.98972392  0.8114062   0.63321072  0.75397831]\n",
      "Extrapolation NMSE:  [ 0.86727685  0.8924669   0.99106652  0.90736514  0.97826368  0.98368025]\n",
      "Minibatch loss at step 36500: 328046.062500\n",
      "Minibatch NMSE:  [ 0.93333554  0.94249427  0.98929656  0.89974475  1.17759514  1.19938612]\n",
      "Validation NMSE:  [ 0.80357891  0.88458669  0.99023819  0.81400424  0.63525784  0.75515789]\n",
      "Extrapolation NMSE:  [ 0.8648231   0.89311701  0.9915154   0.90785432  0.96902764  0.97577655]\n",
      "Minibatch loss at step 37000: 203789.375000\n",
      "Minibatch NMSE:  [ 0.96918285  0.84331352  0.93810451  0.90904927  0.49648637  0.79243904]\n",
      "Validation NMSE:  [ 0.76802784  0.87698114  0.98866749  0.81079602  0.63548571  0.75678492]\n",
      "Extrapolation NMSE:  [ 0.83064157  0.88491815  0.99937713  0.90393847  0.95947331  0.97195446]\n",
      "Minibatch loss at step 37500: 201220.062500\n",
      "Minibatch NMSE:  [ 0.51568455  0.78864956  0.9794758   0.78276634  0.62643677  0.68102843]\n",
      "Validation NMSE:  [ 0.7782045   0.88376957  0.98862559  0.81010759  0.63009822  0.75081813]\n",
      "Extrapolation NMSE:  [ 0.84017342  0.89152992  0.98956919  0.90634495  0.97337937  0.97793245]\n",
      "Minibatch loss at step 38000: 348325.500000\n",
      "Minibatch NMSE:  [ 0.75396264  0.80000019  0.95194077  0.72860783  0.82269382  0.92694122]\n",
      "Validation NMSE:  [ 0.76814008  0.8787601   0.98637247  0.80583978  0.62557054  0.74775088]\n",
      "Extrapolation NMSE:  [ 0.82932448  0.88761902  0.98737401  0.90201789  0.96454841  0.97102088]\n",
      "Minibatch loss at step 38500: 315377.187500\n",
      "Minibatch NMSE:  [ 0.7332316   0.85091388  0.96795994  0.76433879  1.02496445  1.08839524]\n",
      "Validation NMSE:  [ 0.78515393  0.88746166  0.98758882  0.8049987   0.62727582  0.74990243]\n",
      "Extrapolation NMSE:  [ 0.83731776  0.89110196  0.98777819  0.90736109  0.98836231  0.98945606]\n",
      "Minibatch loss at step 39000: 216941.421875\n",
      "Minibatch NMSE:  [ 0.95357215  0.94033289  1.05023074  0.93647778  0.70405209  0.852615  ]\n",
      "Validation NMSE:  [ 0.78186488  0.8869344   0.98611629  0.80665511  0.62505996  0.74522591]\n",
      "Extrapolation NMSE:  [ 0.83507717  0.89432758  0.98682016  0.90717417  0.98847109  0.99343365]\n",
      "Minibatch loss at step 39500: 319982.625000\n",
      "Minibatch NMSE:  [ 0.9832685   0.98220199  0.97444963  0.88112533  0.66909194  0.77838784]\n",
      "Validation NMSE:  [ 0.7945475   0.88830256  0.98780185  0.80619979  0.6251868   0.74915987]\n",
      "Extrapolation NMSE:  [ 0.85014403  0.89230216  0.99059886  0.90356332  0.97553802  0.98377103]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-849da8f0a815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# in the list passed to sess.run() and the value tensors will be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# returned in the tuple from the call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# write log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 64\n",
    "num_steps  = 700001\n",
    "\n",
    "# Number of units in hidden layer\n",
    "N_HIDDEN1_UNITS = 38\n",
    "N_HIDDEN2_UNITS = 15\n",
    "\n",
    "# L2 Regularizer constant\n",
    "beta1 = 0.00000001\n",
    "\n",
    "logs_path = \"/tmp/ffnn/\"\n",
    "\n",
    "def defineFeedForwardNeuralNetworkModel(input_size, num_hidden1_units, num_hidden2_units, output_size):\n",
    "    # Hidden 1 Layer\n",
    "    with tf.variable_scope('hidden1', reuse=False):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 2 Layer\n",
    "    with tf.variable_scope('hidden2', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units], initializer=tf.constant_initializer(0))\n",
    "    # Linear (Output) Layer\n",
    "    with tf.variable_scope('linear', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, output_size], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [output_size], initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Build prediction graph.\n",
    "def performFeedForwardNeuralNetworkPrediction(train_dataset, input_size, num_hidden1_units, num_hidden2_units, output_size, dropout_keep_prob):\n",
    "    \"\"\"Build the Feed-Forward Neural Network model for prediction.\n",
    "    Args:\n",
    "        train_dataset: training dataset's placeholder.\n",
    "        num_hidden1_units: Size of the 1st hidden layer.\n",
    "    Returns:\n",
    "        outputs: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1 = tf.nn.relu(tf.matmul(train_dataset, weights) + biases)\n",
    "#         hidden1 = tf.matmul(train_dataset, weights) + biases\n",
    "        hidden1_drop = tf.nn.dropout(hidden1, dropout_keep_prob)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1_drop, weights) + biases)\n",
    "        hidden2_drop = tf.nn.dropout(hidden2, dropout_keep_prob)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        outputs = tf.matmul(hidden2_drop, weights) + biases\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Build training graph.\n",
    "def performFeedForwardNeuralNetworkTraining(outputs, labels, initial_learning_rate, input_size, num_hidden1_units, num_hidden2_units, output_size):\n",
    "    \"\"\"Build the training graph.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Output tensor, float - [BATCH_SIZE, output_size].\n",
    "        labels : Labels tensor, float - [BATCH_SIZE, output_size].\n",
    "        initial_learning_rate: The initial learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates L2 prediction loss.\n",
    "    pred_l2_loss = tf.nn.l2_loss(outputs - labels, name='my_pred_l2_loss')\n",
    "    \n",
    "    # Create an operation that calculates L2 loss.\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        hidden2_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        output_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    \n",
    "    loss = tf.reduce_mean(pred_l2_loss, name='my_pred_l2_loss_mean') + (beta1 * (hidden1_layer_l2_loss + hidden2_layer_l2_loss + output_layer_l2_loss))\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Exponentially-decaying learning rate:\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, 0.1)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "#     train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     train_op = tf.train.MomentumOptimizer(learning_rate, momentum=learning_rate/4.0, use_nesterov=True).minimize(loss, global_step=global_step)\n",
    "    train_op = tf.train.AdagradOptimizer(initial_learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return train_op, loss, learning_rate\n",
    "\n",
    "# Save model.\n",
    "def saveFeedForwardNeuralNetworkToMATLABMatFile(input_size, num_hidden1_units, num_hidden2_units, output_size):\n",
    "    model_params={}\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        model_params['weights_1']=weights.eval()\n",
    "        model_params['biases_1']=biases.eval()\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        model_params['weights_2']=weights.eval()\n",
    "        model_params['biases_2']=biases.eval()\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        model_params['weights_out']=weights.eval()\n",
    "        model_params['biases_out']=biases.eval()\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "# Build the complete graph for feeding inputs, training, and saving checkpoints.\n",
    "ff_nn_graph = tf.Graph()\n",
    "with ff_nn_graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=[batch_size, D_input], name=\"tf_train_dataset_placeholder\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=[batch_size, D_output], name=\"tf_train_labels_placeholder\")\n",
    "    tf_train_all_dataset = tf.constant(X_train_dataset, name=\"tf_train_all_dataset_constant\")\n",
    "    tf_valid_dataset = tf.constant(X_valid_dataset, name=\"tf_valid_dataset_constant\")\n",
    "    tf_test_dataset = tf.constant(X_test_dataset, name=\"tf_test_dataset_constant\")\n",
    "    tf_whole_dataset = tf.constant(X_init_offset_cancelled, name=\"tf_whole_dataset_constant\")\n",
    "    tf_whole_all_dataset = tf.constant(X_init_offset_cancelled_all, name=\"tf_whole_all_dataset_constant\")\n",
    "    tf_extrapolate_test_dataset = tf.constant(X_extrapt, name=\"tf_extrapolate_test_dataset_constant\")\n",
    "    \n",
    "    # Currently turn off dropouts:\n",
    "    tf_train_dropout_keep_prob = 0.77\n",
    "    \n",
    "    # Define the Neural Network model.\n",
    "    defineFeedForwardNeuralNetworkModel(D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output)\n",
    "    \n",
    "    # Build the Prediction Graph (that computes predictions from the inference model).\n",
    "    tf_outputs = performFeedForwardNeuralNetworkPrediction(tf_train_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output, tf_train_dropout_keep_prob)\n",
    "    \n",
    "    # Build the Training Graph (that calculate and apply gradients).\n",
    "    train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.1, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output)\n",
    "#     train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.00001, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output)\n",
    "    \n",
    "    # Create a summary:\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "    \n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf_outputs\n",
    "    train_all_prediction = performFeedForwardNeuralNetworkPrediction(tf_train_all_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output, 1.0)\n",
    "    valid_prediction = performFeedForwardNeuralNetworkPrediction(tf_valid_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output, 1.0)\n",
    "    test_prediction  = performFeedForwardNeuralNetworkPrediction(tf_test_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output, 1.0)\n",
    "    whole_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output, 1.0)\n",
    "    whole_all_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_all_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output, 1.0)\n",
    "    extrapolate_test_prediction = performFeedForwardNeuralNetworkPrediction(tf_extrapolate_test_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output, 1.0)\n",
    "\n",
    "# Run training for num_steps and save checkpoint at the end.\n",
    "with tf.Session(graph=ff_nn_graph) as session:\n",
    "    # Run the Op to initialize the variables.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(num_steps):\n",
    "        # Read a batch of input dataset and labels.\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (Ct_train.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = X_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = Ct_train[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value, predictions, summary = session.run([train_op, loss, train_prediction, summary_op], feed_dict=feed_dict)\n",
    "        \n",
    "        # write log\n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, loss_value))\n",
    "            print(\"Minibatch NMSE: \", computeNMSE(predictions, batch_labels))\n",
    "            print(\"Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "            print(\"Extrapolation NMSE: \", computeNMSE(extrapolate_test_prediction.eval(), Ctt_extrapt))\n",
    "        if (step % 5000 == 0):\n",
    "            model_params = saveFeedForwardNeuralNetworkToMATLABMatFile(D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, D_output)\n",
    "            print(\"Logging model_params.mat ...\")\n",
    "            sio.savemat('model_params/model_params_'+str(step)+'.mat', model_params)\n",
    "            \n",
    "            whole_prediction_result = whole_prediction.eval()\n",
    "            whole_prediction_result_dict={}\n",
    "            whole_prediction_result_dict['whole_prediction_result'] = whole_prediction_result\n",
    "            print(\"Logging Ct_fit_onset.mat ...\")\n",
    "            sio.savemat('scraping/Ct_fit_onset_'+str(step)+'.mat', whole_prediction_result_dict)\n",
    "            whole_all_prediction_result = whole_all_prediction.eval()\n",
    "            whole_all_prediction_result_dict={}\n",
    "            whole_all_prediction_result_dict['whole_all_prediction_result'] = whole_all_prediction_result\n",
    "            print(\"Logging Ct_fit_all.mat ...\")\n",
    "            sio.savemat('scraping/Ct_fit_all_'+str(step)+'.mat', whole_all_prediction_result_dict)\n",
    "    print(\"Final Training NMSE  : \", computeNMSE(train_all_prediction.eval(), Ct_train))\n",
    "    print(\"Final Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "    print(\"Final Test NMSE      : \", computeNMSE(test_prediction.eval(), Ct_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
