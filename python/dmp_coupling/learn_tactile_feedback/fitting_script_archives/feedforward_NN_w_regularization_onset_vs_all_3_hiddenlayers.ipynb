{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Feedforward Neural Network with Regularization\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First load the data dumped by MATLAB (*.mat file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [],
   "source": [
    "# X_init_offset_cancelled = sio.loadmat('scraping/X_init_offset_cancelled_scraping.mat', struct_as_record=True)['X_init_offset_cancelled']\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_phasePSI_scraping.mat', struct_as_record=True)['Xioc_phasePSI']\n",
    "#X_234_all\n",
    "X_init_offset_cancelled_all= sio.loadmat('scraping/X_gauss_basis_func_scraping.mat', struct_as_record=True)['X_gauss_basis_func'].astype(np.float32)\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_PD_ratio_mean_3std_scraping.mat', struct_as_record=True)['Xioc_PD_ratio_mean_3std']\n",
    "# Ct_target = sio.loadmat('scraping/Ct_target_scraping.mat', struct_as_record=True)['Ct_target']\n",
    "\n",
    "#X_234\n",
    "X_init_offset_cancelled = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_train.mat', struct_as_record=True)['X_gauss_basis_func_train'].astype(np.float32)\n",
    "#Ct_target_234\n",
    "Ct_target = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_train.mat', struct_as_record=True)['Ct_target_filt_train'].astype(np.float32)\n",
    "\n",
    "# Dataset for Extrapolation Test\n",
    "#X_5toend\n",
    "X_extrapolate_test = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_test.mat', struct_as_record=True)['X_gauss_basis_func_test'].astype(np.float32)\n",
    "#Ct_5toend\n",
    "Ctt_extrapolate_test = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_test.mat', struct_as_record=True)['Ct_target_filt_test'].astype(np.float32)\n",
    "\n",
    "# Dummy Data for learning simulation/verification:\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/dummy_X.mat', struct_as_record=True)['X']\n",
    "# Ct_target = sio.loadmat('scraping/dummy_Ct.mat', struct_as_record=True)['Ct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Verify the dimensions are correct and shuffle the data (for Stochastic Gradient Descent (SGD)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init_offset_cancelled.shape = (198563, 250)\n",
      "Ct_target.shape = (198563, 6)\n",
      "N_data   = 198563\n",
      "D_input  = 250\n",
      "D_output = 6\n",
      "X_extrapolate_test.shape = (128105, 250)\n",
      "Ctt_extrapolate_test.shape = (128105, 6)\n",
      "N_train_dataset = 168779\n",
      "N_valid_dataset = 14892\n",
      "N_test_dataset  = 14892\n"
     ]
    }
   ],
   "source": [
    "N_data_extrapolate_test = Ctt_extrapolate_test.shape[0]\n",
    "permutation_extrapolate_test = np.random.permutation(N_data_extrapolate_test)\n",
    "permutation_extrapolate_test_select = permutation_extrapolate_test[1:1000]\n",
    "X_extrapt = X_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "Ctt_extrapt = Ctt_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "\n",
    "print('X_init_offset_cancelled.shape =', X_init_offset_cancelled.shape)\n",
    "print('Ct_target.shape =', Ct_target.shape)\n",
    "\n",
    "N_data = Ct_target.shape[0]\n",
    "D_input = X_init_offset_cancelled.shape[1]\n",
    "D_output = Ct_target.shape[1]\n",
    "print('N_data   =', N_data)\n",
    "print('D_input  =', D_input)\n",
    "print('D_output =', D_output)\n",
    "\n",
    "print('X_extrapolate_test.shape =', X_extrapolate_test.shape)\n",
    "print('Ctt_extrapolate_test.shape =', Ctt_extrapolate_test.shape)\n",
    "\n",
    "random.seed(38)\n",
    "np.random.seed(38)\n",
    "\n",
    "X_init_offset_cancelled = X_init_offset_cancelled\n",
    "X_init_offset_cancelled_all = X_init_offset_cancelled_all\n",
    "\n",
    "permutation = np.random.permutation(N_data)\n",
    "X_shuffled = X_init_offset_cancelled[permutation,:]\n",
    "Ct_target_shuffled = Ct_target[permutation,:]\n",
    "\n",
    "fraction_train_dataset = 0.85\n",
    "fraction_test_dataset  = 0.075\n",
    "\n",
    "N_train_dataset = np.round(fraction_train_dataset * N_data).astype(int)\n",
    "N_test_dataset = np.round(fraction_test_dataset * N_data).astype(int)\n",
    "N_valid_dataset = N_data - N_train_dataset - N_test_dataset\n",
    "print('N_train_dataset =', N_train_dataset)\n",
    "print('N_valid_dataset =', N_valid_dataset)\n",
    "print('N_test_dataset  =', N_test_dataset)\n",
    "\n",
    "X_train_dataset = X_shuffled[0:N_train_dataset,:]\n",
    "Ct_train = Ct_target_shuffled[0:N_train_dataset,:]\n",
    "X_valid_dataset = X_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "Ct_valid = Ct_target_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "X_test_dataset = X_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]\n",
    "Ct_test = Ct_target_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def computeNMSE(predictions, labels):\n",
    "    mse = np.mean(np.square(predictions-labels), axis=0);\n",
    "    var_labels = np.var(labels, axis=0)\n",
    "    nmse = np.divide(mse, var_labels)\n",
    "    return (nmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Feed-Forward Neural Network Model\n",
    "---------\n",
    "\n",
    "Here it goes:\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 235617.687500\n",
      "Minibatch NMSE:  [ 1.1175257   1.03361785  1.00007999  1.05540442  1.00128663  1.03085387]\n",
      "Validation NMSE:  [ 1.00311339  1.00260222  1.00766826  1.04254067  1.00115383  1.00440347]\n",
      "Extrapolation NMSE:  [ 1.00300813  1.00572312  1.00657475  1.08814526  1.00019789  1.01724339]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 500: 288271.375000\n",
      "Minibatch NMSE:  [ 0.92830443  0.92393476  1.06912315  0.98220372  0.91094601  0.97219622]\n",
      "Validation NMSE:  [ 0.79589629  0.87058198  1.00832033  0.92965031  0.93063504  0.95658249]\n",
      "Extrapolation NMSE:  [ 0.84826797  0.88653344  1.00056231  0.97551119  0.95926034  0.96967381]\n",
      "Minibatch loss at step 1000: 379459.031250\n",
      "Minibatch NMSE:  [ 0.82034498  0.84815204  1.01405978  0.95983952  0.86485511  0.86401385]\n",
      "Validation NMSE:  [ 0.79715085  0.88027322  1.00164139  0.92050308  0.86801356  0.91561753]\n",
      "Extrapolation NMSE:  [ 0.84787577  0.8851608   0.98884112  0.9551602   0.93557608  0.95293897]\n",
      "Minibatch loss at step 1500: 410682.937500\n",
      "Minibatch NMSE:  [ 1.02932239  0.99606639  1.02708662  0.94304723  1.03494847  1.03520787]\n",
      "Validation NMSE:  [ 0.82605803  0.89524323  0.99556398  0.91492784  0.83522463  0.90407097]\n",
      "Extrapolation NMSE:  [ 0.86930746  0.89602655  0.99751258  0.94678277  0.92892051  0.95835793]\n",
      "Minibatch loss at step 2000: 405407.593750\n",
      "Minibatch NMSE:  [ 0.59146279  0.80106944  0.98339248  0.80227637  0.76987165  0.81002951]\n",
      "Validation NMSE:  [ 0.8190062   0.89674938  0.99949008  0.90519291  0.81473929  0.88828701]\n",
      "Extrapolation NMSE:  [ 0.86588657  0.9001987   1.00417662  0.92545617  0.93059152  0.95977384]\n",
      "Minibatch loss at step 2500: 140928.468750\n",
      "Minibatch NMSE:  [ 0.80542707  0.7830568   1.04147315  0.90550524  0.78132862  0.763412  ]\n",
      "Validation NMSE:  [ 0.81120664  0.87921959  0.99414927  0.89588541  0.81009996  0.88510728]\n",
      "Extrapolation NMSE:  [ 0.85065907  0.87994438  0.99879366  0.92954928  0.90838259  0.95125765]\n",
      "Minibatch loss at step 3000: 288551.187500\n",
      "Minibatch NMSE:  [ 0.96890503  1.04057169  0.99890852  0.95539606  0.7783156   0.78296959]\n",
      "Validation NMSE:  [ 0.80048406  0.88177204  0.99469739  0.88967657  0.7849679   0.86495209]\n",
      "Extrapolation NMSE:  [ 0.84098554  0.88162643  0.99790931  0.91090184  0.89295316  0.92933208]\n",
      "Minibatch loss at step 3500: 321724.656250\n",
      "Minibatch NMSE:  [ 1.16039455  1.04532564  1.03426123  0.9047448   0.77204275  0.77405524]\n",
      "Validation NMSE:  [ 0.8193354   0.88674891  0.99415725  0.87872082  0.77716351  0.86125416]\n",
      "Extrapolation NMSE:  [ 0.86677128  0.89501923  0.99446642  0.90763658  0.88906288  0.93391323]\n",
      "Minibatch loss at step 4000: 260105.109375\n",
      "Minibatch NMSE:  [ 1.35491943  1.37444556  0.99261707  0.93431729  0.62062889  0.83543289]\n",
      "Validation NMSE:  [ 0.82375968  0.89255053  0.99192727  0.87383246  0.76741397  0.85320646]\n",
      "Extrapolation NMSE:  [ 0.86446661  0.90029114  0.99553668  0.89799184  0.9019745   0.94274729]\n",
      "Minibatch loss at step 4500: 313050.750000\n",
      "Minibatch NMSE:  [ 0.85943192  0.93163508  1.01241016  0.96769077  0.81841117  0.83101511]\n",
      "Validation NMSE:  [ 0.8312276   0.89004248  0.99407077  0.86963969  0.75526261  0.84359354]\n",
      "Extrapolation NMSE:  [ 0.87278348  0.8913362   0.99788362  0.90072054  0.90725499  0.94760251]\n",
      "Minibatch loss at step 5000: 264867.125000\n",
      "Minibatch NMSE:  [ 0.5907501   0.78449929  0.96534288  0.9148891   0.76557869  0.771604  ]\n",
      "Validation NMSE:  [ 0.80755574  0.88290209  0.99219042  0.85263723  0.76734751  0.85516161]\n",
      "Extrapolation NMSE:  [ 0.84520388  0.88138503  0.99454629  0.88116413  0.89064431  0.93528253]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 5500: 278331.312500\n",
      "Minibatch NMSE:  [ 1.05951071  1.00204372  1.01937032  0.92684686  0.66401303  0.95881027]\n",
      "Validation NMSE:  [ 0.80764252  0.87888002  0.99381304  0.85908431  0.74393058  0.83814859]\n",
      "Extrapolation NMSE:  [ 0.85056216  0.88370734  0.99437904  0.88714367  0.8912822   0.93663567]\n",
      "Minibatch loss at step 6000: 187972.843750\n",
      "Minibatch NMSE:  [ 0.80071276  0.89666981  1.03637874  0.81177437  0.73416066  0.86801457]\n",
      "Validation NMSE:  [ 0.79413533  0.87534219  0.99557364  0.85435027  0.74070913  0.83488864]\n",
      "Extrapolation NMSE:  [ 0.84112823  0.87976229  0.99918169  0.90703565  0.89786977  0.94339198]\n",
      "Minibatch loss at step 6500: 212636.687500\n",
      "Minibatch NMSE:  [ 0.86665583  1.07472062  1.05827725  0.89474368  0.8051948   0.82837081]\n",
      "Validation NMSE:  [ 0.82456207  0.88447815  0.99524456  0.84638345  0.72904116  0.8282845 ]\n",
      "Extrapolation NMSE:  [ 0.86880833  0.89339852  0.9960615   0.89149517  0.90329432  0.94748312]\n",
      "Minibatch loss at step 7000: 240673.046875\n",
      "Minibatch NMSE:  [ 0.86439192  0.89381796  0.97414047  0.73169655  0.75405616  0.82259625]\n",
      "Validation NMSE:  [ 0.83313555  0.88894159  0.99228078  0.84411091  0.72970998  0.82626569]\n",
      "Extrapolation NMSE:  [ 0.87671775  0.89693695  0.99312735  0.88794428  0.91246569  0.94691581]\n",
      "Minibatch loss at step 7500: 218654.703125\n",
      "Minibatch NMSE:  [ 1.15718412  0.98160541  1.02417302  0.91029608  0.89971477  0.90905571]\n",
      "Validation NMSE:  [ 0.80734271  0.88155943  0.99291176  0.84370619  0.71503675  0.81955236]\n",
      "Extrapolation NMSE:  [ 0.8527987   0.88702184  0.99474478  0.89779997  0.90395242  0.9467029 ]\n",
      "Minibatch loss at step 8000: 244911.593750\n",
      "Minibatch NMSE:  [ 0.71902311  0.88682336  1.00328481  0.80829519  0.82594603  0.89910078]\n",
      "Validation NMSE:  [ 0.76892734  0.86022353  0.99212551  0.83356822  0.71737552  0.8183797 ]\n",
      "Extrapolation NMSE:  [ 0.82094216  0.87463808  0.9935841   0.87265134  0.90223724  0.94859284]\n",
      "Minibatch loss at step 8500: 241111.562500\n",
      "Minibatch NMSE:  [ 1.21882141  1.04996765  1.02448487  0.89198548  0.7493515   0.80968124]\n",
      "Validation NMSE:  [ 0.80924702  0.88238096  0.99332386  0.83265454  0.70436293  0.80745757]\n",
      "Extrapolation NMSE:  [ 0.84552777  0.8875246   0.99695069  0.87448686  0.91184914  0.9613362 ]\n",
      "Minibatch loss at step 9000: 558278.375000\n",
      "Minibatch NMSE:  [ 0.72648132  0.85109568  0.98008966  0.94049782  0.75143194  0.7867884 ]\n",
      "Validation NMSE:  [ 0.80161285  0.8745597   0.99005461  0.834167    0.69548327  0.80180615]\n",
      "Extrapolation NMSE:  [ 0.8469606   0.88178885  0.99319875  0.88228822  0.91488487  0.95493358]\n",
      "Minibatch loss at step 9500: 251197.750000\n",
      "Minibatch NMSE:  [ 0.99138653  1.02757394  1.0035218   0.77935672  0.73558456  0.82422674]\n",
      "Validation NMSE:  [ 0.82656723  0.88977808  0.99097049  0.83254659  0.7045567   0.80874902]\n",
      "Extrapolation NMSE:  [ 0.86993176  0.89505619  0.99063784  0.87167984  0.91469908  0.9574123 ]\n",
      "Minibatch loss at step 10000: 246504.625000\n",
      "Minibatch NMSE:  [ 0.72150904  0.75977027  1.01416314  0.93194872  0.72347301  0.8489185 ]\n",
      "Validation NMSE:  [ 0.8038938   0.88623846  0.98972934  0.83126497  0.70935273  0.81284755]\n",
      "Extrapolation NMSE:  [ 0.84649813  0.88665837  0.99112707  0.86554652  0.91335237  0.95465499]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 10500: 285701.875000\n",
      "Minibatch NMSE:  [ 0.85474128  0.9274683   1.11551821  0.91991019  0.9127534   1.2300992 ]\n",
      "Validation NMSE:  [ 0.8205452   0.89262813  0.99435395  0.82486802  0.69393599  0.80318975]\n",
      "Extrapolation NMSE:  [ 0.86571968  0.89917326  1.00240111  0.87473613  0.91663188  0.95678473]\n",
      "Minibatch loss at step 11000: 518084.312500\n",
      "Minibatch NMSE:  [ 0.94330013  1.03876626  0.98794907  0.94448811  0.81191766  0.86442906]\n",
      "Validation NMSE:  [ 0.80791122  0.88452244  0.988563    0.82029182  0.6812349   0.7918781 ]\n",
      "Extrapolation NMSE:  [ 0.85142207  0.89024824  0.99139565  0.86855561  0.94082284  0.97265625]\n",
      "Minibatch loss at step 11500: 310404.125000\n",
      "Minibatch NMSE:  [ 1.31067598  1.07821643  0.99455547  0.81733412  0.52328551  0.73477972]\n",
      "Validation NMSE:  [ 0.78643781  0.87413687  0.99070054  0.8192274   0.68170923  0.79071081]\n",
      "Extrapolation NMSE:  [ 0.8397513   0.88296944  0.9882772   0.86988282  0.91510576  0.95044595]\n",
      "Minibatch loss at step 12000: 290464.812500\n",
      "Minibatch NMSE:  [ 1.04419386  1.06611753  0.99588126  0.93522727  0.39548767  0.55361706]\n",
      "Validation NMSE:  [ 0.80609393  0.88240463  0.98806667  0.82072139  0.6801433   0.79073375]\n",
      "Extrapolation NMSE:  [ 0.85864502  0.88925499  0.99098355  0.87185997  0.91773605  0.95143563]\n",
      "Minibatch loss at step 12500: 246316.953125\n",
      "Minibatch NMSE:  [ 0.98295033  0.99081469  0.97571188  0.92861295  0.63276404  0.81333458]\n",
      "Validation NMSE:  [ 0.84020138  0.90117067  0.99009055  0.82662106  0.69164646  0.80275083]\n",
      "Extrapolation NMSE:  [ 0.8789373   0.90230799  0.99055952  0.86978859  0.92262846  0.95681858]\n",
      "Minibatch loss at step 13000: 225414.343750\n",
      "Minibatch NMSE:  [ 0.89743131  1.06929898  1.0781095   0.81504709  0.6142      0.70965153]\n",
      "Validation NMSE:  [ 0.79482752  0.87742871  0.99046445  0.8093912   0.67592525  0.79082221]\n",
      "Extrapolation NMSE:  [ 0.83097488  0.88110089  0.99485517  0.86098146  0.91349006  0.95284939]\n",
      "Minibatch loss at step 13500: 153388.171875\n",
      "Minibatch NMSE:  [ 0.85162812  0.91510296  1.03047681  0.74274868  0.86166567  0.89963222]\n",
      "Validation NMSE:  [ 0.81714678  0.88992572  0.99345124  0.81561863  0.66608888  0.77868849]\n",
      "Extrapolation NMSE:  [ 0.85345572  0.89158052  0.99373573  0.86812037  0.91764963  0.95610756]\n",
      "Minibatch loss at step 14000: 524251.125000\n",
      "Minibatch NMSE:  [ 0.79296649  0.84794766  1.02298701  0.89887393  0.6741966   0.70229894]\n",
      "Validation NMSE:  [ 0.8314566   0.89417511  0.98861653  0.81305873  0.66925001  0.78581452]\n",
      "Extrapolation NMSE:  [ 0.8673619   0.89671278  0.99033201  0.86456561  0.91171467  0.94536233]\n",
      "Minibatch loss at step 14500: 166459.593750\n",
      "Minibatch NMSE:  [ 0.87507653  0.87290728  0.99789983  0.89444298  0.61352485  0.9976263 ]\n",
      "Validation NMSE:  [ 0.80677098  0.87904435  0.98964578  0.81079262  0.66588402  0.77831042]\n",
      "Extrapolation NMSE:  [ 0.84734708  0.88260597  0.99156559  0.86121571  0.93084395  0.96833903]\n",
      "Minibatch loss at step 15000: 271582.812500\n",
      "Minibatch NMSE:  [ 0.94412822  0.99863535  0.98613805  0.77308357  0.55390376  0.7518689 ]\n",
      "Validation NMSE:  [ 0.81356251  0.88705802  0.98848826  0.81465256  0.6588586   0.77389342]\n",
      "Extrapolation NMSE:  [ 0.85293162  0.88966233  0.99047697  0.8738507   0.93443394  0.97210664]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 15500: 172161.031250\n",
      "Minibatch NMSE:  [ 0.98335159  0.9553324   0.98222578  0.8176741   0.89792109  0.92741448]\n",
      "Validation NMSE:  [ 0.80349147  0.88178664  0.98916382  0.80866736  0.65944523  0.77555817]\n",
      "Extrapolation NMSE:  [ 0.84197104  0.88497132  0.99187851  0.85533619  0.91549158  0.95272422]\n",
      "Minibatch loss at step 16000: 148379.812500\n",
      "Minibatch NMSE:  [ 0.83080709  0.90243608  1.01184738  0.93758476  1.1060797   1.03549874]\n",
      "Validation NMSE:  [ 0.80301332  0.87911028  0.98769361  0.80535656  0.65183198  0.76949894]\n",
      "Extrapolation NMSE:  [ 0.84570056  0.88538873  0.99120706  0.85955089  0.91306418  0.95228374]\n",
      "Minibatch loss at step 16500: 307266.812500\n",
      "Minibatch NMSE:  [ 0.83254695  0.90882921  1.0167625   0.90976572  0.62140888  0.71072072]\n",
      "Validation NMSE:  [ 0.81588596  0.88670599  0.98898298  0.80422866  0.64566958  0.76454532]\n",
      "Extrapolation NMSE:  [ 0.85298705  0.89093024  0.99097824  0.84620833  0.94086349  0.97481471]\n",
      "Minibatch loss at step 17000: 328584.625000\n",
      "Minibatch NMSE:  [ 1.0635401   0.99386626  1.00197804  0.93515486  0.74073899  0.76087469]\n",
      "Validation NMSE:  [ 0.8221761   0.88889545  0.98908037  0.80219561  0.6619314   0.78023916]\n",
      "Extrapolation NMSE:  [ 0.8597787   0.89421052  0.98968184  0.85154659  0.91244566  0.95492738]\n",
      "Minibatch loss at step 17500: 236909.781250\n",
      "Minibatch NMSE:  [ 1.03368604  1.09541059  0.96876937  0.93492216  0.60946804  1.06712556]\n",
      "Validation NMSE:  [ 0.81942648  0.88762587  0.98882312  0.80625886  0.64507002  0.76346892]\n",
      "Extrapolation NMSE:  [ 0.86001122  0.89193779  0.98977923  0.85115194  0.91495895  0.95115888]\n",
      "Minibatch loss at step 18000: 187172.718750\n",
      "Minibatch NMSE:  [ 0.46929201  0.69758022  0.96982622  0.91096795  0.74655342  0.73999071]\n",
      "Validation NMSE:  [ 0.80259001  0.88420457  0.98771065  0.79887706  0.64244431  0.76125878]\n",
      "Extrapolation NMSE:  [ 0.83989376  0.89145064  0.98854899  0.85239732  0.92487729  0.95658386]\n",
      "Minibatch loss at step 18500: 164634.843750\n",
      "Minibatch NMSE:  [ 0.77875632  0.79578596  1.02451289  0.84943908  0.53393453  0.72957486]\n",
      "Validation NMSE:  [ 0.81239545  0.887707    0.9867292   0.79647672  0.63224459  0.75559831]\n",
      "Extrapolation NMSE:  [ 0.8509143   0.89412874  0.99061465  0.84969205  0.91735071  0.96212757]\n",
      "Minibatch loss at step 19000: 145869.421875\n",
      "Minibatch NMSE:  [ 0.80309862  0.79922354  0.94587785  0.93012834  1.04187405  0.80433381]\n",
      "Validation NMSE:  [ 0.79665345  0.87790209  0.98878419  0.79112762  0.64173096  0.76327306]\n",
      "Extrapolation NMSE:  [ 0.83949631  0.88582826  0.99191934  0.84491622  0.91770369  0.9537766 ]\n",
      "Minibatch loss at step 19500: 167623.562500\n",
      "Minibatch NMSE:  [ 1.04373777  1.15193987  0.99335337  0.90177578  0.54889923  0.88454151]\n",
      "Validation NMSE:  [ 0.81805927  0.88859111  0.98662877  0.79535508  0.62882316  0.75249177]\n",
      "Extrapolation NMSE:  [ 0.86177635  0.89556855  0.99015534  0.85107827  0.93009675  0.96380258]\n",
      "Minibatch loss at step 20000: 413395.875000\n",
      "Minibatch NMSE:  [ 0.60190928  0.76415002  1.0398519   0.81948215  0.87520057  0.95715773]\n",
      "Validation NMSE:  [ 0.8100304   0.88547146  0.98629457  0.79379749  0.63810116  0.75953817]\n",
      "Extrapolation NMSE:  [ 0.85010207  0.89056355  0.98703957  0.84995455  0.92264664  0.96179807]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 20500: 577332.875000\n",
      "Minibatch NMSE:  [ 0.77133733  0.87348711  0.97807807  0.71884596  0.87112415  0.98861092]\n",
      "Validation NMSE:  [ 0.82329875  0.89582992  0.98531342  0.79587239  0.62080425  0.74185407]\n",
      "Extrapolation NMSE:  [ 0.85438484  0.89763659  0.98910338  0.8504104   0.94173539  0.97629738]\n",
      "Minibatch loss at step 21000: 530589.250000\n",
      "Minibatch NMSE:  [ 1.21256447  1.06583333  0.99754959  0.89332521  0.7910347   0.81951761]\n",
      "Validation NMSE:  [ 0.80597436  0.88302135  0.98831284  0.79205477  0.62627059  0.74957085]\n",
      "Extrapolation NMSE:  [ 0.83977038  0.88883376  0.9926877   0.85201913  0.92149919  0.96305662]\n",
      "Minibatch loss at step 21500: 319596.031250\n",
      "Minibatch NMSE:  [ 0.56705397  0.96430755  0.98616302  0.73798043  1.02085352  1.15354812]\n",
      "Validation NMSE:  [ 0.80271089  0.88033903  0.98629969  0.79344642  0.61896604  0.74739158]\n",
      "Extrapolation NMSE:  [ 0.84130383  0.88589323  0.99313951  0.84714854  0.9384234   0.96626878]\n",
      "Minibatch loss at step 22000: 240911.671875\n",
      "Minibatch NMSE:  [ 1.17268002  1.04684448  1.09491026  0.75385863  0.53948748  0.57468402]\n",
      "Validation NMSE:  [ 0.8136366   0.88399196  0.98744845  0.78669894  0.62308139  0.75073856]\n",
      "Extrapolation NMSE:  [ 0.84779769  0.88904488  0.99181789  0.84581447  0.92972356  0.96354282]\n",
      "Minibatch loss at step 22500: 405010.937500\n",
      "Minibatch NMSE:  [ 0.95674437  0.91008866  1.03619373  0.82009035  0.68465298  0.75650412]\n",
      "Validation NMSE:  [ 0.82264924  0.88845003  0.98671693  0.7917763   0.62746555  0.75414312]\n",
      "Extrapolation NMSE:  [ 0.85924292  0.89517879  0.98800254  0.84384495  0.93057251  0.96321446]\n",
      "Minibatch loss at step 23000: 225432.312500\n",
      "Minibatch NMSE:  [ 0.78010881  1.10917807  0.95118594  0.85079962  0.84113258  1.04114652]\n",
      "Validation NMSE:  [ 0.82121247  0.88231474  0.98592931  0.79794925  0.61333424  0.73960543]\n",
      "Extrapolation NMSE:  [ 0.86201656  0.88951534  0.98965615  0.85251909  0.92445993  0.96238565]\n",
      "Minibatch loss at step 23500: 374632.437500\n",
      "Minibatch NMSE:  [ 0.88881797  0.90178668  1.04607332  0.93796384  0.81738293  0.8228631 ]\n",
      "Validation NMSE:  [ 0.81874889  0.88449001  0.98715407  0.78914434  0.62712473  0.75285131]\n",
      "Extrapolation NMSE:  [ 0.85182732  0.88954788  0.99078584  0.85660195  0.91499132  0.95498294]\n",
      "Minibatch loss at step 24000: 214142.328125\n",
      "Minibatch NMSE:  [ 0.76991117  0.92142785  1.05446982  0.72620136  0.86061019  0.84455723]\n",
      "Validation NMSE:  [ 0.80549121  0.8734138   0.98722035  0.78685772  0.61056674  0.73887467]\n",
      "Extrapolation NMSE:  [ 0.84445512  0.88208312  0.99257648  0.85250151  0.91761738  0.95659101]\n",
      "Minibatch loss at step 24500: 218046.125000\n",
      "Minibatch NMSE:  [ 0.73518598  0.87197977  1.04249585  0.86817157  0.82124346  1.01607549]\n",
      "Validation NMSE:  [ 0.81744224  0.88575143  0.98763084  0.78587055  0.60716629  0.73603696]\n",
      "Extrapolation NMSE:  [ 0.85134804  0.89406461  0.99367338  0.84698141  0.92858028  0.96223503]\n",
      "Minibatch loss at step 25000: 518548.562500\n",
      "Minibatch NMSE:  [ 0.51445311  0.79679525  1.08136976  0.79190296  1.05975246  1.0444541 ]\n",
      "Validation NMSE:  [ 0.80440575  0.87702024  0.98870522  0.78899312  0.60747981  0.73674405]\n",
      "Extrapolation NMSE:  [ 0.83884305  0.8863706   0.99104249  0.85067552  0.93441319  0.96961069]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 25500: 334811.750000\n",
      "Minibatch NMSE:  [ 1.3431319   1.26405919  0.98776788  1.06743217  1.0205586   1.18168044]\n",
      "Validation NMSE:  [ 0.81406164  0.88499123  0.98619145  0.78903306  0.60808563  0.73673165]\n",
      "Extrapolation NMSE:  [ 0.84763217  0.89580762  0.99106598  0.85589796  0.94025302  0.97167373]\n",
      "Minibatch loss at step 26000: 245574.500000\n",
      "Minibatch NMSE:  [ 0.7464779   0.77778792  0.94943094  0.77655339  0.62996167  0.79008061]\n",
      "Validation NMSE:  [ 0.80343306  0.87710303  0.9878819   0.78505331  0.61610425  0.74545616]\n",
      "Extrapolation NMSE:  [ 0.83548176  0.88340592  0.99349636  0.84999597  0.90958393  0.94760436]\n",
      "Minibatch loss at step 26500: 194988.312500\n",
      "Minibatch NMSE:  [ 0.87437546  0.92871267  1.01271462  0.75950313  0.76669621  0.82822007]\n",
      "Validation NMSE:  [ 0.80369174  0.88244796  0.98505241  0.78318596  0.60199124  0.73301613]\n",
      "Extrapolation NMSE:  [ 0.84501725  0.89375091  0.99256176  0.84681582  0.90845013  0.95129091]\n",
      "Minibatch loss at step 27000: 482921.812500\n",
      "Minibatch NMSE:  [ 0.69970751  0.88488036  0.99517381  0.89271891  0.82381427  0.88445073]\n",
      "Validation NMSE:  [ 0.81517214  0.88997781  0.98676807  0.78346282  0.60144931  0.73339242]\n",
      "Extrapolation NMSE:  [ 0.85632557  0.89869702  0.9930771   0.83792228  0.91803634  0.9569242 ]\n",
      "Minibatch loss at step 27500: 211045.671875\n",
      "Minibatch NMSE:  [ 1.00438237  1.00064337  1.02233541  0.8436746   0.54880553  0.65884495]\n",
      "Validation NMSE:  [ 0.80778021  0.88266295  0.98645723  0.77689409  0.5920676   0.72081012]\n",
      "Extrapolation NMSE:  [ 0.84785813  0.89545816  0.99036533  0.83680767  0.93063551  0.97425818]\n",
      "Minibatch loss at step 28000: 263724.062500\n",
      "Minibatch NMSE:  [ 0.88464886  0.92795265  1.04752409  0.90474707  0.80385864  0.90966278]\n",
      "Validation NMSE:  [ 0.80747104  0.88473052  0.98528439  0.77907294  0.60211974  0.73150069]\n",
      "Extrapolation NMSE:  [ 0.84556127  0.89186102  0.98806864  0.83492708  0.92216372  0.96205068]\n",
      "Minibatch loss at step 28500: 269074.218750\n",
      "Minibatch NMSE:  [ 0.86457962  0.84782302  1.01642632  0.90146744  0.88872981  0.91091073]\n",
      "Validation NMSE:  [ 0.79319656  0.87468064  0.98587316  0.78158832  0.60542464  0.73432767]\n",
      "Extrapolation NMSE:  [ 0.83496851  0.88481849  0.98877555  0.83049196  0.92340887  0.95943612]\n",
      "Minibatch loss at step 29000: 204936.234375\n",
      "Minibatch NMSE:  [ 0.98705369  1.00073147  1.02047968  0.98968738  0.44406256  0.67743808]\n",
      "Validation NMSE:  [ 0.81861043  0.88988715  0.98406339  0.78404444  0.59333903  0.72513616]\n",
      "Extrapolation NMSE:  [ 0.86304033  0.89954334  0.99025708  0.8401829   0.93529177  0.9710567 ]\n",
      "Minibatch loss at step 29500: 515551.812500\n",
      "Minibatch NMSE:  [ 1.30860424  1.33176863  0.98824811  0.92029887  0.81593263  0.90366882]\n",
      "Validation NMSE:  [ 0.81813556  0.89038992  0.98594236  0.7809841   0.58502001  0.71933448]\n",
      "Extrapolation NMSE:  [ 0.8521378   0.89759886  0.99093294  0.84850132  0.94533551  0.98143262]\n",
      "Minibatch loss at step 30000: 168642.453125\n",
      "Minibatch NMSE:  [ 0.71694487  0.79613703  0.89299327  0.60059857  0.6416353   0.68864459]\n",
      "Validation NMSE:  [ 0.79003239  0.87270796  0.9841904   0.77548832  0.5872578   0.71891427]\n",
      "Extrapolation NMSE:  [ 0.83531201  0.88453889  0.98753703  0.83571982  0.927091    0.9643867 ]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 30500: 161123.921875\n",
      "Minibatch NMSE:  [ 0.94813246  1.07319033  0.98991138  0.94712019  0.50839281  0.77291697]\n",
      "Validation NMSE:  [ 0.8304444   0.89586437  0.98486298  0.78001624  0.58559561  0.72092098]\n",
      "Extrapolation NMSE:  [ 0.86552817  0.90257406  0.99030751  0.83716041  0.93052989  0.96970564]\n",
      "Minibatch loss at step 31000: 148486.187500\n",
      "Minibatch NMSE:  [ 0.9418956   0.86969298  0.95422912  0.767232    0.46686378  0.62855262]\n",
      "Validation NMSE:  [ 0.82915843  0.89479238  0.98487848  0.78342789  0.59002155  0.7238276 ]\n",
      "Extrapolation NMSE:  [ 0.85761636  0.90044188  0.99162263  0.84248322  0.94574672  0.98028058]\n",
      "Minibatch loss at step 31500: 159171.234375\n",
      "Minibatch NMSE:  [ 1.01839185  1.03912282  0.98018545  0.90976185  0.45770621  0.59015524]\n",
      "Validation NMSE:  [ 0.81212938  0.88578188  0.98549002  0.77749616  0.59598541  0.72831607]\n",
      "Extrapolation NMSE:  [ 0.84766001  0.89340085  0.99092352  0.83682698  0.92371613  0.96339059]\n",
      "Minibatch loss at step 32000: 366979.687500\n",
      "Minibatch NMSE:  [ 0.85244346  1.00226712  1.00730801  0.84619474  0.60643202  0.77980006]\n",
      "Validation NMSE:  [ 0.80631578  0.88366383  0.98770082  0.77763766  0.59752285  0.73002166]\n",
      "Extrapolation NMSE:  [ 0.84059745  0.89158297  0.99211591  0.8397544   0.91599047  0.96163386]\n",
      "Minibatch loss at step 32500: 199341.062500\n",
      "Minibatch NMSE:  [ 0.70068502  0.80165637  0.98670328  0.82027018  0.82540929  0.94961858]\n",
      "Validation NMSE:  [ 0.82138813  0.89317745  0.98472381  0.77453274  0.581554    0.7161268 ]\n",
      "Extrapolation NMSE:  [ 0.85568458  0.89812452  0.98892409  0.82578677  0.90661103  0.9547978 ]\n",
      "Minibatch loss at step 33000: 222286.687500\n",
      "Minibatch NMSE:  [ 0.97668219  0.99391836  0.9989627   0.8617962   0.31113067  0.38238803]\n",
      "Validation NMSE:  [ 0.82174605  0.89295548  0.98314035  0.77593106  0.57623744  0.71026844]\n",
      "Extrapolation NMSE:  [ 0.85386831  0.89780962  0.98686373  0.82751238  0.91826552  0.96229368]\n",
      "Minibatch loss at step 33500: 184904.656250\n",
      "Minibatch NMSE:  [ 0.70877934  0.79550755  1.03408349  0.91493744  0.69148618  0.89806551]\n",
      "Validation NMSE:  [ 0.82584053  0.89273298  0.98320645  0.77873623  0.57398081  0.70900542]\n",
      "Extrapolation NMSE:  [ 0.86030513  0.90108788  0.99006432  0.83837438  0.92640686  0.97113258]\n",
      "Minibatch loss at step 34000: 149449.250000\n",
      "Minibatch NMSE:  [ 1.22676468  1.0291543   1.01530516  0.88449442  0.61236632  0.84505606]\n",
      "Validation NMSE:  [ 0.81339842  0.88727337  0.9834792   0.77167171  0.58210081  0.71777672]\n",
      "Extrapolation NMSE:  [ 0.85064071  0.89836305  0.98861694  0.83013326  0.91260833  0.95720643]\n",
      "Minibatch loss at step 34500: 139140.515625\n",
      "Minibatch NMSE:  [ 0.92086226  0.79562318  0.99018025  0.81357968  0.85334897  1.03103876]\n",
      "Validation NMSE:  [ 0.78591841  0.87437195  0.98505896  0.77049708  0.57381207  0.71027696]\n",
      "Extrapolation NMSE:  [ 0.82748467  0.8845228   0.98703492  0.82961094  0.91494435  0.95799941]\n",
      "Minibatch loss at step 35000: 231567.328125\n",
      "Minibatch NMSE:  [ 0.92480433  0.97404522  0.9941715   0.78944826  0.6113584   0.97201848]\n",
      "Validation NMSE:  [ 0.81646216  0.88889343  0.98376995  0.7713691   0.57172596  0.70785367]\n",
      "Extrapolation NMSE:  [ 0.85254991  0.89672762  0.99081033  0.83917642  0.93011338  0.96946543]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 35500: 231081.734375\n",
      "Minibatch NMSE:  [ 0.76888508  1.05486381  1.00623953  0.76009876  0.80084527  0.85883182]\n",
      "Validation NMSE:  [ 0.80438644  0.87965584  0.98504752  0.7699008   0.56438869  0.70491797]\n",
      "Extrapolation NMSE:  [ 0.83624804  0.88911366  0.99060059  0.84014165  0.9413023   0.98204124]\n",
      "Minibatch loss at step 36000: 274121.062500\n",
      "Minibatch NMSE:  [ 0.91839474  0.95075881  0.97251099  0.91337085  0.71270669  0.87608522]\n",
      "Validation NMSE:  [ 0.81361109  0.88528067  0.98283458  0.7700814   0.56482458  0.70477849]\n",
      "Extrapolation NMSE:  [ 0.85059071  0.89356261  0.98877698  0.83097214  0.92094874  0.96425092]\n",
      "Minibatch loss at step 36500: 248918.171875\n",
      "Minibatch NMSE:  [ 0.88999987  0.90809244  0.97917396  0.74729145  0.8316974   0.94336516]\n",
      "Validation NMSE:  [ 0.82286465  0.89191169  0.98491389  0.77598572  0.58049327  0.71588093]\n",
      "Extrapolation NMSE:  [ 0.85453707  0.89606088  0.9919374   0.83964336  0.90571284  0.95318586]\n",
      "Minibatch loss at step 37000: 188143.687500\n",
      "Minibatch NMSE:  [ 0.87254858  0.88730514  1.02503228  0.91711771  0.42925045  0.75390583]\n",
      "Validation NMSE:  [ 0.78237593  0.8667981   0.98643917  0.77142483  0.57473469  0.71538866]\n",
      "Extrapolation NMSE:  [ 0.82056874  0.88028145  0.99151152  0.83478194  0.90309298  0.94817257]\n",
      "Minibatch loss at step 37500: 210881.843750\n",
      "Minibatch NMSE:  [ 0.65858823  0.90122288  1.03524983  0.87857485  0.65466738  0.69383276]\n",
      "Validation NMSE:  [ 0.80566454  0.88030422  0.98510468  0.77327347  0.5719685   0.70853716]\n",
      "Extrapolation NMSE:  [ 0.84238023  0.89024585  0.99054009  0.83552992  0.90735936  0.95277083]\n",
      "Minibatch loss at step 38000: 334313.406250\n",
      "Minibatch NMSE:  [ 0.53025109  0.62616831  0.95529938  0.66207665  0.82198179  0.87089497]\n",
      "Validation NMSE:  [ 0.80954808  0.88106102  0.98349684  0.7688511   0.56376696  0.70056403]\n",
      "Extrapolation NMSE:  [ 0.84705728  0.89117885  0.98914015  0.82872176  0.9203245   0.96166432]\n",
      "Minibatch loss at step 38500: 247253.187500\n",
      "Minibatch NMSE:  [ 0.73309654  0.87921989  0.96580434  0.86028171  0.7630865   0.82821077]\n",
      "Validation NMSE:  [ 0.80165762  0.87630916  0.98442155  0.76826209  0.5549109   0.69410348]\n",
      "Extrapolation NMSE:  [ 0.83824134  0.88529503  0.98898113  0.82299858  0.93265408  0.96762508]\n",
      "Minibatch loss at step 39000: 179841.031250\n",
      "Minibatch NMSE:  [ 0.84023273  0.86082864  1.00704539  0.81957114  0.58374202  0.67625058]\n",
      "Validation NMSE:  [ 0.81393564  0.8853547   0.98364353  0.7779721   0.56690782  0.70603961]\n",
      "Extrapolation NMSE:  [ 0.84573019  0.89136922  0.98844332  0.83744228  0.92901576  0.96452659]\n",
      "Minibatch loss at step 39500: 243796.890625\n",
      "Minibatch NMSE:  [ 1.09344101  0.99792945  0.97035855  0.83703828  0.46073928  0.6254366 ]\n",
      "Validation NMSE:  [ 0.81156927  0.88235891  0.98368096  0.77408224  0.55681551  0.697321  ]\n",
      "Extrapolation NMSE:  [ 0.84335625  0.89098948  0.99014419  0.82865328  0.94559264  0.98073423]\n",
      "Minibatch loss at step 40000: 308671.312500\n",
      "Minibatch NMSE:  [ 0.66515368  0.96658689  1.03364193  0.89182645  0.92268908  1.02950561]\n",
      "Validation NMSE:  [ 0.80845398  0.88309044  0.98364794  0.76972532  0.55175805  0.69207108]\n",
      "Extrapolation NMSE:  [ 0.84343261  0.89234346  0.98968351  0.82525837  0.94425529  0.9802413 ]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 40500: 314631.781250\n",
      "Minibatch NMSE:  [ 0.77804172  0.8697533   1.04411221  0.8975758   0.81742716  0.88342851]\n",
      "Validation NMSE:  [ 0.80704874  0.88001317  0.98420835  0.76763219  0.55462122  0.69349045]\n",
      "Extrapolation NMSE:  [ 0.83905923  0.88641387  0.9875676   0.82204932  0.93578488  0.97147518]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f18546ddcc1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# in the list passed to sess.run() and the value tensors will be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# returned in the tuple from the call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# write log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \"\"\"\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \"\"\"\n\u001b[1;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \"\"\"\n\u001b[1;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    226\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001b[1;32m    227\u001b[0m                       (fetch, type(fetch)))\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 64\n",
    "num_steps  = 700001\n",
    "\n",
    "# Number of units in hidden layer\n",
    "N_HIDDEN1_UNITS = 125\n",
    "N_HIDDEN2_UNITS = 64\n",
    "N_HIDDEN3_UNITS = 32\n",
    "\n",
    "# L2 Regularizer constant\n",
    "beta1 = 0.0000000001\n",
    "\n",
    "logs_path = \"/tmp/ffnn/\"\n",
    "\n",
    "def defineFeedForwardNeuralNetworkModel(input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, output_size):\n",
    "    # Hidden 1 Layer\n",
    "    with tf.variable_scope('hidden1', reuse=False):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 2 Layer\n",
    "    with tf.variable_scope('hidden2', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 3 Layer\n",
    "    with tf.variable_scope('hidden3', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units], initializer=tf.constant_initializer(0))\n",
    "    # Linear (Output) Layer\n",
    "    with tf.variable_scope('linear', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, output_size], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [output_size], initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Build prediction graph.\n",
    "def performFeedForwardNeuralNetworkPrediction(train_dataset, input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, output_size, dropout_keep_prob):\n",
    "    \"\"\"Build the Feed-Forward Neural Network model for prediction.\n",
    "    Args:\n",
    "        train_dataset: training dataset's placeholder.\n",
    "        num_hidden1_units: Size of the 1st hidden layer.\n",
    "    Returns:\n",
    "        outputs: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1 = tf.nn.relu(tf.matmul(train_dataset, weights) + biases)\n",
    "#         hidden1 = tf.matmul(train_dataset, weights) + biases\n",
    "        hidden1_drop = tf.nn.dropout(hidden1, dropout_keep_prob)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1_drop, weights) + biases)\n",
    "        hidden2_drop = tf.nn.dropout(hidden2, dropout_keep_prob)\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2_drop, weights) + biases)\n",
    "        hidden3_drop = tf.nn.dropout(hidden3, dropout_keep_prob)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        outputs = tf.matmul(hidden3_drop, weights) + biases\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Build training graph.\n",
    "def performFeedForwardNeuralNetworkTraining(outputs, labels, initial_learning_rate, input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, output_size):\n",
    "    \"\"\"Build the training graph.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Output tensor, float - [BATCH_SIZE, output_size].\n",
    "        labels : Labels tensor, float - [BATCH_SIZE, output_size].\n",
    "        initial_learning_rate: The initial learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates L2 prediction loss.\n",
    "    pred_l2_loss = tf.nn.l2_loss(outputs - labels, name='my_pred_l2_loss')\n",
    "    \n",
    "    # Create an operation that calculates L2 loss.\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        hidden2_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        hidden3_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        output_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    \n",
    "    loss = tf.reduce_mean(pred_l2_loss, name='my_pred_l2_loss_mean') + (beta1 * (hidden1_layer_l2_loss + hidden2_layer_l2_loss + hidden3_layer_l2_loss + output_layer_l2_loss))\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Exponentially-decaying learning rate:\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, 0.1)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "#     train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     train_op = tf.train.MomentumOptimizer(learning_rate, momentum=learning_rate/4.0, use_nesterov=True).minimize(loss, global_step=global_step)\n",
    "    train_op = tf.train.AdagradOptimizer(initial_learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return train_op, loss, learning_rate\n",
    "\n",
    "# Save model.\n",
    "def saveFeedForwardNeuralNetworkToMATLABMatFile(input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, output_size):\n",
    "    model_params={}\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        model_params['weights_1']=weights.eval()\n",
    "        model_params['biases_1']=biases.eval()\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        model_params['weights_2']=weights.eval()\n",
    "        model_params['biases_2']=biases.eval()\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        model_params['weights_3']=weights.eval()\n",
    "        model_params['biases_3']=biases.eval()\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        model_params['weights_out']=weights.eval()\n",
    "        model_params['biases_out']=biases.eval()\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "# Build the complete graph for feeding inputs, training, and saving checkpoints.\n",
    "ff_nn_graph = tf.Graph()\n",
    "with ff_nn_graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=[batch_size, D_input], name=\"tf_train_dataset_placeholder\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=[batch_size, D_output], name=\"tf_train_labels_placeholder\")\n",
    "    tf_train_all_dataset = tf.constant(X_train_dataset, name=\"tf_train_all_dataset_constant\")\n",
    "    tf_valid_dataset = tf.constant(X_valid_dataset, name=\"tf_valid_dataset_constant\")\n",
    "    tf_test_dataset = tf.constant(X_test_dataset, name=\"tf_test_dataset_constant\")\n",
    "    tf_whole_dataset = tf.constant(X_init_offset_cancelled, name=\"tf_whole_dataset_constant\")\n",
    "    tf_whole_all_dataset = tf.constant(X_init_offset_cancelled_all, name=\"tf_whole_all_dataset_constant\")\n",
    "    tf_extrapolate_test_dataset = tf.constant(X_extrapt, name=\"tf_extrapolate_test_dataset_constant\")\n",
    "    \n",
    "    # Currently turn off dropouts:\n",
    "    tf_train_dropout_keep_prob = 0.77\n",
    "    \n",
    "    # Define the Neural Network model.\n",
    "    defineFeedForwardNeuralNetworkModel(D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output)\n",
    "    \n",
    "    # Build the Prediction Graph (that computes predictions from the inference model).\n",
    "    tf_outputs = performFeedForwardNeuralNetworkPrediction(tf_train_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output, tf_train_dropout_keep_prob)\n",
    "    \n",
    "    # Build the Training Graph (that calculate and apply gradients).\n",
    "    train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.1, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output)\n",
    "#     train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.00001, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output)\n",
    "    \n",
    "    # Create a summary:\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "    \n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf_outputs\n",
    "    train_all_prediction = performFeedForwardNeuralNetworkPrediction(tf_train_all_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output, 1.0)\n",
    "    valid_prediction = performFeedForwardNeuralNetworkPrediction(tf_valid_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output, 1.0)\n",
    "    test_prediction  = performFeedForwardNeuralNetworkPrediction(tf_test_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output, 1.0)\n",
    "    whole_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output, 1.0)\n",
    "    whole_all_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_all_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output, 1.0)\n",
    "    extrapolate_test_prediction = performFeedForwardNeuralNetworkPrediction(tf_extrapolate_test_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output, 1.0)\n",
    "\n",
    "# Run training for num_steps and save checkpoint at the end.\n",
    "with tf.Session(graph=ff_nn_graph) as session:\n",
    "    # Run the Op to initialize the variables.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(num_steps):\n",
    "        # Read a batch of input dataset and labels.\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (Ct_train.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = X_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = Ct_train[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value, predictions, summary = session.run([train_op, loss, train_prediction, summary_op], feed_dict=feed_dict)\n",
    "        \n",
    "        # write log\n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, loss_value))\n",
    "            print(\"Minibatch NMSE: \", computeNMSE(predictions, batch_labels))\n",
    "            print(\"Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "            print(\"Extrapolation NMSE: \", computeNMSE(extrapolate_test_prediction.eval(), Ctt_extrapt))\n",
    "        if (step % 5000 == 0):\n",
    "            model_params = saveFeedForwardNeuralNetworkToMATLABMatFile(D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, D_output)\n",
    "            print(\"Logging model_params.mat ...\")\n",
    "            sio.savemat('model_params/model_params.mat', model_params)\n",
    "            \n",
    "            whole_prediction_result = whole_prediction.eval()\n",
    "            whole_prediction_result_dict={}\n",
    "            whole_prediction_result_dict['whole_prediction_result'] = whole_prediction_result\n",
    "            print(\"Logging Ct_fit_onset.mat ...\")\n",
    "            sio.savemat('scraping/Ct_fit_onset.mat', whole_prediction_result_dict)\n",
    "            whole_all_prediction_result = whole_all_prediction.eval()\n",
    "            whole_all_prediction_result_dict={}\n",
    "            whole_all_prediction_result_dict['whole_all_prediction_result'] = whole_all_prediction_result\n",
    "            print(\"Logging Ct_fit_all.mat ...\")\n",
    "            sio.savemat('scraping/Ct_fit_all.mat', whole_all_prediction_result_dict)\n",
    "    print(\"Final Training NMSE  : \", computeNMSE(train_all_prediction.eval(), Ct_train))\n",
    "    print(\"Final Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "    print(\"Final Test NMSE      : \", computeNMSE(test_prediction.eval(), Ct_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
