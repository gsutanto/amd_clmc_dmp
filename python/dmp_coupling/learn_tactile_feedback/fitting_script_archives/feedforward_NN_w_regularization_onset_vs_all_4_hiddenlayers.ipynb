{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Feedforward Neural Network with Regularization\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First load the data dumped by MATLAB (*.mat file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [],
   "source": [
    "# X_init_offset_cancelled = sio.loadmat('scraping/X_init_offset_cancelled_scraping.mat', struct_as_record=True)['X_init_offset_cancelled']\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_phasePSI_scraping.mat', struct_as_record=True)['Xioc_phasePSI']\n",
    "#X_234_all\n",
    "X_init_offset_cancelled_all= sio.loadmat('scraping/X_gauss_basis_func_scraping.mat', struct_as_record=True)['X_gauss_basis_func'].astype(np.float32)\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_PD_ratio_mean_3std_scraping.mat', struct_as_record=True)['Xioc_PD_ratio_mean_3std']\n",
    "# Ct_target = sio.loadmat('scraping/Ct_target_scraping.mat', struct_as_record=True)['Ct_target']\n",
    "\n",
    "#X_234\n",
    "X_init_offset_cancelled = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_train.mat', struct_as_record=True)['X_gauss_basis_func_train'].astype(np.float32)\n",
    "#Ct_target_234\n",
    "Ct_target = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_train.mat', struct_as_record=True)['Ct_target_filt_train'].astype(np.float32)\n",
    "\n",
    "# Dataset for Extrapolation Test\n",
    "#X_5toend\n",
    "X_extrapolate_test = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_test.mat', struct_as_record=True)['X_gauss_basis_func_test'].astype(np.float32)\n",
    "#Ct_5toend\n",
    "Ctt_extrapolate_test = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_test.mat', struct_as_record=True)['Ct_target_filt_test'].astype(np.float32)\n",
    "\n",
    "# Dummy Data for learning simulation/verification:\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/dummy_X.mat', struct_as_record=True)['X']\n",
    "# Ct_target = sio.loadmat('scraping/dummy_Ct.mat', struct_as_record=True)['Ct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Verify the dimensions are correct and shuffle the data (for Stochastic Gradient Descent (SGD)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init_offset_cancelled.shape = (128105, 250)\n",
      "Ct_target.shape = (128105, 6)\n",
      "N_data   = 128105\n",
      "D_input  = 250\n",
      "D_output = 6\n",
      "X_extrapolate_test.shape = (198563, 250)\n",
      "Ctt_extrapolate_test.shape = (198563, 6)\n",
      "N_train_dataset = 108889\n",
      "N_valid_dataset = 9608\n",
      "N_test_dataset  = 9608\n"
     ]
    }
   ],
   "source": [
    "N_data_extrapolate_test = Ctt_extrapolate_test.shape[0]\n",
    "permutation_extrapolate_test = np.random.permutation(N_data_extrapolate_test)\n",
    "permutation_extrapolate_test_select = permutation_extrapolate_test[1:1000]\n",
    "X_extrapt = X_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "Ctt_extrapt = Ctt_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "\n",
    "print('X_init_offset_cancelled.shape =', X_init_offset_cancelled.shape)\n",
    "print('Ct_target.shape =', Ct_target.shape)\n",
    "\n",
    "N_data = Ct_target.shape[0]\n",
    "D_input = X_init_offset_cancelled.shape[1]\n",
    "D_output = Ct_target.shape[1]\n",
    "print('N_data   =', N_data)\n",
    "print('D_input  =', D_input)\n",
    "print('D_output =', D_output)\n",
    "\n",
    "print('X_extrapolate_test.shape =', X_extrapolate_test.shape)\n",
    "print('Ctt_extrapolate_test.shape =', Ctt_extrapolate_test.shape)\n",
    "\n",
    "random.seed(38)\n",
    "np.random.seed(38)\n",
    "\n",
    "X_init_offset_cancelled = X_init_offset_cancelled\n",
    "X_init_offset_cancelled_all = X_init_offset_cancelled_all\n",
    "\n",
    "permutation = np.random.permutation(N_data)\n",
    "X_shuffled = X_init_offset_cancelled[permutation,:]\n",
    "Ct_target_shuffled = Ct_target[permutation,:]\n",
    "\n",
    "fraction_train_dataset = 0.85\n",
    "fraction_test_dataset  = 0.075\n",
    "\n",
    "N_train_dataset = np.round(fraction_train_dataset * N_data).astype(int)\n",
    "N_test_dataset = np.round(fraction_test_dataset * N_data).astype(int)\n",
    "N_valid_dataset = N_data - N_train_dataset - N_test_dataset\n",
    "print('N_train_dataset =', N_train_dataset)\n",
    "print('N_valid_dataset =', N_valid_dataset)\n",
    "print('N_test_dataset  =', N_test_dataset)\n",
    "\n",
    "X_train_dataset = X_shuffled[0:N_train_dataset,:]\n",
    "Ct_train = Ct_target_shuffled[0:N_train_dataset,:]\n",
    "X_valid_dataset = X_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "Ct_valid = Ct_target_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "X_test_dataset = X_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]\n",
    "Ct_test = Ct_target_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def computeNMSE(predictions, labels):\n",
    "    mse = np.mean(np.square(predictions-labels), axis=0);\n",
    "    var_labels = np.var(labels, axis=0)\n",
    "    nmse = np.divide(mse, var_labels)\n",
    "    return (nmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Feed-Forward Neural Network Model\n",
    "---------\n",
    "\n",
    "Here it goes:\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 410199.031250\n",
      "Minibatch NMSE:  [ 1.02269995  1.00042939  1.03722811  1.20382571  1.07267439  1.02224565]\n",
      "Validation NMSE:  [ 1.00239086  1.00763464  1.00272524  1.05912113  1.0062393   1.00431585]\n",
      "Extrapolation NMSE:  [ 1.00331879  1.00204957  1.00173402  1.05114949  1.00556254  1.00349092]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 500: 260185.093750\n",
      "Minibatch NMSE:  [ 1.02453887  0.99849874  1.00254297  1.09013903  1.01389551  1.01238632]\n",
      "Validation NMSE:  [ 0.99756587  0.97299707  0.99992788  0.96087623  0.99206263  1.00205827]\n",
      "Extrapolation NMSE:  [ 1.00100684  0.9729467   0.99969423  0.96163899  0.99577153  1.00161958]\n",
      "Minibatch loss at step 1000: 221612.656250\n",
      "Minibatch NMSE:  [ 0.98659223  1.0063355   1.0079844   0.97288865  1.01838291  1.08074546]\n",
      "Validation NMSE:  [ 1.00104117  0.97312152  1.00029361  0.97034651  0.98150223  0.99816978]\n",
      "Extrapolation NMSE:  [ 0.99994671  0.97126484  1.0000726   0.97697604  0.99485332  1.00039756]\n",
      "Minibatch loss at step 1500: 230090.859375\n",
      "Minibatch NMSE:  [ 1.1036588   1.16494572  1.00370085  0.86436385  0.9523052   0.99104095]\n",
      "Validation NMSE:  [ 1.00044334  0.97290373  1.00005078  0.97097611  0.97821563  0.99580777]\n",
      "Extrapolation NMSE:  [ 0.99978846  0.96701509  1.00024128  0.97831738  0.99601704  1.00093508]\n",
      "Minibatch loss at step 2000: 226563.468750\n",
      "Minibatch NMSE:  [ 1.00769663  1.01199913  1.03977311  1.16957712  0.9607951   1.00646877]\n",
      "Validation NMSE:  [ 1.00122702  0.97579402  0.99978709  0.97342515  0.97477561  0.99469405]\n",
      "Extrapolation NMSE:  [ 1.00103664  0.97248977  1.00097895  0.98458904  0.99486512  1.00128067]\n",
      "Minibatch loss at step 2500: 408992.437500\n",
      "Minibatch NMSE:  [ 1.01936674  0.92598778  1.01291728  1.03344727  1.01233971  0.99278802]\n",
      "Validation NMSE:  [ 1.00091982  0.97583747  0.99992669  0.97032505  0.97004199  0.99240637]\n",
      "Extrapolation NMSE:  [ 1.00079167  0.97170371  1.00100434  0.98354596  0.99534875  0.99918509]\n",
      "Minibatch loss at step 3000: 480370.593750\n",
      "Minibatch NMSE:  [ 1.00294185  0.96461767  1.01457059  0.97508043  0.9419449   0.99036306]\n",
      "Validation NMSE:  [ 1.00100958  0.97372216  1.00043094  0.95803726  0.97136408  0.99154598]\n",
      "Extrapolation NMSE:  [ 1.00134158  0.96718431  1.00159633  0.97408253  0.9925431   0.99910146]\n",
      "Minibatch loss at step 3500: 285960.312500\n",
      "Minibatch NMSE:  [ 1.02590823  0.92725652  0.99308735  0.98232174  0.95949674  0.98158407]\n",
      "Validation NMSE:  [ 0.99989474  0.97157043  0.99984145  0.94729292  0.97087646  0.99264097]\n",
      "Extrapolation NMSE:  [ 0.99901485  0.96627533  1.00097573  0.96877396  0.98130012  0.99440789]\n",
      "Minibatch loss at step 4000: 496073.718750\n",
      "Minibatch NMSE:  [ 1.02803338  0.98901731  1.09562647  0.94802445  0.9831723   1.01416039]\n",
      "Validation NMSE:  [ 1.00062156  0.97364563  0.99959147  0.94295448  0.96073705  0.98901165]\n",
      "Extrapolation NMSE:  [ 0.99948275  0.96690643  1.00107551  0.96851295  0.97940749  0.99466819]\n",
      "Minibatch loss at step 4500: 328679.312500\n",
      "Minibatch NMSE:  [ 1.00749886  1.00008142  1.00516784  0.89498705  1.00990045  1.02421367]\n",
      "Validation NMSE:  [ 1.00057292  0.97267866  0.99969476  0.94425309  0.96048403  0.98693472]\n",
      "Extrapolation NMSE:  [ 0.99992567  0.96467555  1.00204098  0.97493428  0.98794347  0.99703479]\n",
      "Minibatch loss at step 5000: 534457.562500\n",
      "Minibatch NMSE:  [ 1.02456999  0.99028975  1.01046395  0.85548574  1.02754593  1.00937462]\n",
      "Validation NMSE:  [ 1.00013375  0.97325659  0.99923909  0.94128102  0.95888096  0.98617274]\n",
      "Extrapolation NMSE:  [ 0.99984467  0.96668768  1.00060654  0.96832174  0.98741764  0.99697071]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 5500: 281303.156250\n",
      "Minibatch NMSE:  [ 1.00476956  1.00800323  1.07768035  1.01733172  1.0927465   1.02181315]\n",
      "Validation NMSE:  [ 1.0002588   0.97657043  0.99902028  0.93677652  0.95643145  0.9850654 ]\n",
      "Extrapolation NMSE:  [ 0.99996924  0.96746439  1.00120735  0.97652203  0.98540527  0.9963218 ]\n",
      "Minibatch loss at step 6000: 406866.812500\n",
      "Minibatch NMSE:  [ 1.00642836  0.95099998  1.00373387  1.07572365  1.03096724  0.99673748]\n",
      "Validation NMSE:  [ 1.00031197  0.97647876  0.99847323  0.94418156  0.95684415  0.98664516]\n",
      "Extrapolation NMSE:  [ 1.00026906  0.96726394  1.00185716  0.97820306  0.98884451  0.99325031]\n",
      "Minibatch loss at step 6500: 450987.437500\n",
      "Minibatch NMSE:  [ 1.02686739  1.00535941  1.00332308  0.85228413  0.98353237  1.02393675]\n",
      "Validation NMSE:  [ 1.00000668  0.97660404  0.99914283  0.94766825  0.95345223  0.98186266]\n",
      "Extrapolation NMSE:  [ 0.99908608  0.9697603   1.00236285  0.98520428  0.98488057  0.99223971]\n",
      "Minibatch loss at step 7000: 316280.843750\n",
      "Minibatch NMSE:  [ 0.99250823  0.95557785  1.08363438  0.9836719   1.16939306  1.14301431]\n",
      "Validation NMSE:  [ 0.99962074  0.97598702  0.99907589  0.94127792  0.95338494  0.98250037]\n",
      "Extrapolation NMSE:  [ 0.99971586  0.96857858  1.00373781  0.97931254  0.98581278  0.99098718]\n",
      "Minibatch loss at step 7500: 405942.000000\n",
      "Minibatch NMSE:  [ 1.01162195  1.01995659  0.99773133  0.8919481   0.99327087  1.03359902]\n",
      "Validation NMSE:  [ 0.99986827  0.97720361  0.99861556  0.94367743  0.95164859  0.98088628]\n",
      "Extrapolation NMSE:  [ 0.9982059   0.9701125   1.00308168  0.98448461  0.98655128  0.99266946]\n",
      "Minibatch loss at step 8000: 547618.250000\n",
      "Minibatch NMSE:  [ 1.02425492  1.02202606  1.05892551  0.97178811  0.99920493  0.98350358]\n",
      "Validation NMSE:  [ 1.00042856  0.9793461   0.99858034  0.95163441  0.94985986  0.97844028]\n",
      "Extrapolation NMSE:  [ 0.99912238  0.97329456  1.00285041  0.98619008  0.99298     0.99483031]\n",
      "Minibatch loss at step 8500: 475869.187500\n",
      "Minibatch NMSE:  [ 0.99946862  1.05427289  1.02338302  0.99935275  0.94111091  1.01125622]\n",
      "Validation NMSE:  [ 0.99959791  0.9761709   0.99826437  0.93692911  0.95135957  0.98358011]\n",
      "Extrapolation NMSE:  [ 0.99911427  0.96910626  1.00273681  0.97585112  0.98894167  0.99537969]\n",
      "Minibatch loss at step 9000: 307523.468750\n",
      "Minibatch NMSE:  [ 1.00567234  1.01173592  0.99782777  0.82025903  0.88805729  0.95678043]\n",
      "Validation NMSE:  [ 0.9994728   0.97536987  0.99867296  0.93511516  0.94924265  0.98151875]\n",
      "Extrapolation NMSE:  [ 0.99814683  0.96752262  1.00333285  0.98632294  0.98958647  0.99616629]\n",
      "Minibatch loss at step 9500: 378922.656250\n",
      "Minibatch NMSE:  [ 1.02669322  1.02712715  1.00226259  0.98458838  0.91110259  0.99363101]\n",
      "Validation NMSE:  [ 0.99972171  0.97705275  0.99836498  0.94146901  0.94798571  0.97981447]\n",
      "Extrapolation NMSE:  [ 0.99967641  0.97136521  1.003317    0.97799402  0.98275447  0.99312794]\n",
      "Minibatch loss at step 10000: 277987.562500\n",
      "Minibatch NMSE:  [ 1.13377917  1.03572679  1.05552566  0.94918603  0.91517615  1.05047464]\n",
      "Validation NMSE:  [ 0.99967384  0.97702783  0.99863094  0.93465346  0.95039707  0.98145932]\n",
      "Extrapolation NMSE:  [ 0.99933207  0.96954471  1.0036515   0.97599578  0.98019141  0.99167383]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 10500: 276625.687500\n",
      "Minibatch NMSE:  [ 1.01129305  1.03626752  1.01001704  1.02617633  0.92563415  1.02184343]\n",
      "Validation NMSE:  [ 0.99980277  0.97816747  0.99798208  0.93113869  0.94709921  0.98023838]\n",
      "Extrapolation NMSE:  [ 1.00019872  0.97065854  1.00412655  0.98083621  0.98478186  0.99148166]\n",
      "Minibatch loss at step 11000: 568428.375000\n",
      "Minibatch NMSE:  [ 1.02065063  1.04481387  1.00358939  1.03562558  1.00815964  1.13038313]\n",
      "Validation NMSE:  [ 1.00018048  0.97792649  0.99818903  0.93522644  0.94735098  0.9792031 ]\n",
      "Extrapolation NMSE:  [ 0.99964195  0.96813291  1.00489199  0.9862482   0.98289806  0.98968107]\n",
      "Minibatch loss at step 11500: 286634.000000\n",
      "Minibatch NMSE:  [ 1.00374746  1.02392852  1.02303219  0.93877071  0.91487861  0.96812695]\n",
      "Validation NMSE:  [ 1.00009501  0.9775331   0.99837255  0.93209445  0.94425273  0.97940946]\n",
      "Extrapolation NMSE:  [ 1.0004164   0.97023493  1.00248075  0.98260725  0.98345315  0.98899573]\n",
      "Minibatch loss at step 12000: 27948912.000000\n",
      "Minibatch NMSE:  [ 1.01052344  1.02616537  1.0039103   0.87477028  1.01518035  1.00851786]\n",
      "Validation NMSE:  [ 1.0000149   0.9761008   0.99810171  0.93709993  0.94616079  0.98007065]\n",
      "Extrapolation NMSE:  [ 0.99912167  0.97204953  1.00320864  0.98105526  0.98753268  0.99441141]\n",
      "Minibatch loss at step 12500: 315926.281250\n",
      "Minibatch NMSE:  [ 1.00275695  0.99000639  0.99698371  0.98760802  0.92141891  0.99924248]\n",
      "Validation NMSE:  [ 0.99982953  0.97564709  0.99785668  0.94042504  0.94293952  0.9767769 ]\n",
      "Extrapolation NMSE:  [ 0.99899322  0.96897173  1.00377965  0.99425006  0.9919042   0.99374986]\n",
      "Minibatch loss at step 13000: 286115.875000\n",
      "Minibatch NMSE:  [ 0.99745709  0.98244435  1.02785707  0.97425038  0.92508006  0.95330453]\n",
      "Validation NMSE:  [ 0.99959534  0.97624385  0.99781436  0.93574578  0.94501626  0.97912616]\n",
      "Extrapolation NMSE:  [ 0.99848861  0.97098577  1.00599861  0.98283142  0.99235696  0.99444282]\n",
      "Minibatch loss at step 13500: 335591.250000\n",
      "Minibatch NMSE:  [ 0.99976039  0.97539139  1.03169775  0.97810942  1.0070529   0.97814238]\n",
      "Validation NMSE:  [ 0.99968207  0.97719812  0.99792206  0.93513995  0.93907076  0.97630954]\n",
      "Extrapolation NMSE:  [ 1.00001574  0.97196847  1.00296295  0.98992449  0.98606706  0.99148959]\n",
      "Minibatch loss at step 14000: 305304.906250\n",
      "Minibatch NMSE:  [ 1.00831747  0.9903931   1.01188529  0.96519047  0.90045345  0.96553957]\n",
      "Validation NMSE:  [ 0.99979562  0.9773047   0.99778569  0.93295157  0.94064069  0.97460318]\n",
      "Extrapolation NMSE:  [ 0.99949306  0.97293085  1.00499535  0.9858765   0.98738217  0.99170381]\n",
      "Minibatch loss at step 14500: 264153.687500\n",
      "Minibatch NMSE:  [ 1.11676061  0.99083644  1.01606965  0.95524955  0.98096699  0.980865  ]\n",
      "Validation NMSE:  [ 0.99999744  0.97821414  0.99828768  0.9385491   0.93863422  0.97641617]\n",
      "Extrapolation NMSE:  [ 0.99978518  0.97540629  1.00465751  0.99178481  0.98319805  0.98776394]\n",
      "Minibatch loss at step 15000: 317416.562500\n",
      "Minibatch NMSE:  [ 1.00113809  1.03860795  0.99201572  0.99060875  0.90089703  0.96409899]\n",
      "Validation NMSE:  [ 0.99944538  0.97780859  0.99751139  0.93323362  0.94163179  0.97552407]\n",
      "Extrapolation NMSE:  [ 0.99916965  0.97326183  1.0045284   0.9914633   0.98936933  0.99246275]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 15500: 390897.406250\n",
      "Minibatch NMSE:  [ 0.99843901  0.9758051   1.00547731  1.01083684  0.94130039  0.98067504]\n",
      "Validation NMSE:  [ 0.99990314  0.97743046  0.99794722  0.93119895  0.94153714  0.97646242]\n",
      "Extrapolation NMSE:  [ 1.00045192  0.97334999  1.00611436  0.98673111  0.98230398  0.98900068]\n",
      "Minibatch loss at step 16000: 363604.187500\n",
      "Minibatch NMSE:  [ 1.01153398  0.99208003  1.00979233  1.06093132  0.99499232  0.98276329]\n",
      "Validation NMSE:  [ 0.99979109  0.97834283  0.99716032  0.9327774   0.93575293  0.97187662]\n",
      "Extrapolation NMSE:  [ 0.9999761   0.97552013  1.00503242  0.98859823  0.98446733  0.99094617]\n",
      "Minibatch loss at step 16500: 567459.812500\n",
      "Minibatch NMSE:  [ 0.99955255  1.06288981  1.03251231  0.93823683  0.91422468  1.01041675]\n",
      "Validation NMSE:  [ 0.99987859  0.97822845  0.99736983  0.93275613  0.93725997  0.97402823]\n",
      "Extrapolation NMSE:  [ 0.99897671  0.973948    1.00728142  0.98705262  0.99335527  0.99126798]\n",
      "Minibatch loss at step 17000: 249500.625000\n",
      "Minibatch NMSE:  [ 1.00237799  0.95104361  1.00168312  0.88771319  0.94176382  0.9473747 ]\n",
      "Validation NMSE:  [ 0.99932301  0.97752374  0.99757928  0.92793238  0.94164217  0.97518957]\n",
      "Extrapolation NMSE:  [ 0.99965912  0.9734233   1.00501633  0.97678477  0.98809397  0.99484402]\n",
      "Minibatch loss at step 17500: 261309.875000\n",
      "Minibatch NMSE:  [ 1.0037483   1.01721728  1.02033842  0.92686784  0.83852136  0.94709784]\n",
      "Validation NMSE:  [ 0.99943501  0.97696954  0.99705285  0.92308325  0.93870503  0.97422266]\n",
      "Extrapolation NMSE:  [ 0.99914461  0.9736855   1.00547326  0.98309231  1.00182295  0.99593359]\n",
      "Minibatch loss at step 18000: 357685.593750\n",
      "Minibatch NMSE:  [ 1.04415822  1.01693225  0.92339396  0.91667855  0.91132396  0.96936244]\n",
      "Validation NMSE:  [ 0.99979597  0.97771549  0.99807471  0.93054336  0.94175148  0.9752019 ]\n",
      "Extrapolation NMSE:  [ 1.00042462  0.97556794  1.00482869  0.98027718  1.00102532  0.99537098]\n",
      "Minibatch loss at step 18500: 385394.187500\n",
      "Minibatch NMSE:  [ 1.01028526  1.00504863  0.98661011  0.9417749   0.80504787  0.89527005]\n",
      "Validation NMSE:  [ 0.9993338   0.97691649  0.99761331  0.93987274  0.93812102  0.97109026]\n",
      "Extrapolation NMSE:  [ 0.99979848  0.97737175  1.00446916  0.98559415  0.99445814  0.99409759]\n",
      "Minibatch loss at step 19000: 303048.062500\n",
      "Minibatch NMSE:  [ 0.9938159   0.96623319  0.99282432  0.75960815  0.88646203  0.96729773]\n",
      "Validation NMSE:  [ 1.00025451  0.97828799  0.99777365  0.93905926  0.93470651  0.96958303]\n",
      "Extrapolation NMSE:  [ 1.00068188  0.9774484   1.00562906  0.98782247  0.99470383  0.99140418]\n",
      "Minibatch loss at step 19500: 352565.406250\n",
      "Minibatch NMSE:  [ 1.00143468  0.99824452  1.00041521  1.0087465   1.0692631   1.03932226]\n",
      "Validation NMSE:  [ 1.00010598  0.97629279  0.99817026  0.93690729  0.93354428  0.96779692]\n",
      "Extrapolation NMSE:  [ 1.00085795  0.97270775  1.00798094  0.99158365  1.01013708  1.00481033]\n",
      "Minibatch loss at step 20000: 353530.062500\n",
      "Minibatch NMSE:  [ 1.02069414  0.99074137  1.04987907  0.91913033  0.911264    0.97444254]\n",
      "Validation NMSE:  [ 0.99960929  0.97634083  0.9977712   0.92915934  0.93752259  0.97365278]\n",
      "Extrapolation NMSE:  [ 0.99997646  0.97356588  1.00357807  0.98167574  0.97927868  0.98682094]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 20500: 420134.937500\n",
      "Minibatch NMSE:  [ 0.99292254  0.92820406  1.01550937  0.99941683  0.93453205  0.95546526]\n",
      "Validation NMSE:  [ 0.99978358  0.97533262  0.99790621  0.93185455  0.93873858  0.97042465]\n",
      "Extrapolation NMSE:  [ 0.99947524  0.97290599  1.00514185  0.9807862   0.98071575  0.98949689]\n",
      "Minibatch loss at step 21000: 665815.125000\n",
      "Minibatch NMSE:  [ 0.99787968  1.01097584  0.98853338  0.89234179  0.93311006  0.89023483]\n",
      "Validation NMSE:  [ 0.99946576  0.97572148  0.99759108  0.93013287  0.93758792  0.97011632]\n",
      "Extrapolation NMSE:  [ 0.99958694  0.97313845  1.00494361  0.98869365  0.99432653  0.99554944]\n",
      "Minibatch loss at step 21500: 299408.250000\n",
      "Minibatch NMSE:  [ 1.00070465  1.00408518  1.02676404  0.95281953  0.96004522  1.01124942]\n",
      "Validation NMSE:  [ 0.99954432  0.97435731  0.99794632  0.93269354  0.93264836  0.96707195]\n",
      "Extrapolation NMSE:  [ 0.99938184  0.97013092  1.00824916  0.9904232   1.0009836   0.99637616]\n",
      "Minibatch loss at step 22000: 303434.562500\n",
      "Minibatch NMSE:  [ 1.02304578  1.09029031  1.02386904  0.88015789  0.95726103  0.96523982]\n",
      "Validation NMSE:  [ 0.99982423  0.97681159  0.99768788  0.93471515  0.9337002   0.96676892]\n",
      "Extrapolation NMSE:  [ 0.99911982  0.97387713  1.005651    0.98326957  1.00114977  0.99922019]\n",
      "Minibatch loss at step 22500: 250117.859375\n",
      "Minibatch NMSE:  [ 1.00086999  0.99180353  1.04069769  0.90459949  0.91037124  0.9617995 ]\n",
      "Validation NMSE:  [ 1.00003231  0.97893584  0.99774092  0.93456507  0.93034399  0.96319503]\n",
      "Extrapolation NMSE:  [ 0.99904406  0.97466815  1.00955653  0.97998929  1.00463605  1.00406432]\n",
      "Minibatch loss at step 23000: 628189.937500\n",
      "Minibatch NMSE:  [ 0.98980463  0.94883364  0.98011172  0.85725242  0.96683544  1.02247787]\n",
      "Validation NMSE:  [ 1.00016665  0.97677034  0.99801636  0.93341702  0.93187487  0.96681702]\n",
      "Extrapolation NMSE:  [ 0.99990124  0.97503096  1.00484705  0.97728175  0.997302    0.99850971]\n",
      "Minibatch loss at step 23500: 186283.468750\n",
      "Minibatch NMSE:  [ 0.98173356  1.05183518  0.98025274  0.89217013  0.75545239  0.91841614]\n",
      "Validation NMSE:  [ 0.9993462   0.97438979  0.99782652  0.92628527  0.93488443  0.96652913]\n",
      "Extrapolation NMSE:  [ 0.99942017  0.97389603  1.00541759  0.97207975  1.00425386  1.00293827]\n",
      "Minibatch loss at step 24000: 403283.093750\n",
      "Minibatch NMSE:  [ 1.0061959   0.97318345  0.99931538  0.93821156  0.93901432  0.96662879]\n",
      "Validation NMSE:  [ 0.99999696  0.97614783  0.99767393  0.93021345  0.93196762  0.96448845]\n",
      "Extrapolation NMSE:  [ 1.00053251  0.97503012  1.0069989   0.97423679  1.00158179  1.00217032]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-deaf43dc191d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# in the list passed to sess.run() and the value tensors will be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# returned in the tuple from the call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# write log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 64\n",
    "num_steps  = 700001\n",
    "\n",
    "# Number of units in hidden layer\n",
    "N_HIDDEN1_UNITS = 250\n",
    "N_HIDDEN2_UNITS = 125\n",
    "N_HIDDEN3_UNITS = 64\n",
    "N_HIDDEN4_UNITS = 32\n",
    "\n",
    "# L2 Regularizer constant\n",
    "beta1 = 0.0000000001\n",
    "\n",
    "logs_path = \"/tmp/ffnn/\"\n",
    "\n",
    "def defineFeedForwardNeuralNetworkModel(input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, num_hidden4_units, output_size):\n",
    "    # Hidden 1 Layer\n",
    "    with tf.variable_scope('hidden1', reuse=False):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 2 Layer\n",
    "    with tf.variable_scope('hidden2', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 3 Layer\n",
    "    with tf.variable_scope('hidden3', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units], initializer=tf.constant_initializer(0))\n",
    "    # Hidden 4 Layer\n",
    "    with tf.variable_scope('hidden4', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, num_hidden4_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden4_units], initializer=tf.constant_initializer(0))\n",
    "    # Linear (Output) Layer\n",
    "    with tf.variable_scope('linear', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden4_units, output_size], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [output_size], initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Build prediction graph.\n",
    "def performFeedForwardNeuralNetworkPrediction(train_dataset, input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, num_hidden4_units, output_size, dropout_keep_prob):\n",
    "    \"\"\"Build the Feed-Forward Neural Network model for prediction.\n",
    "    Args:\n",
    "        train_dataset: training dataset's placeholder.\n",
    "        num_hidden1_units: Size of the 1st hidden layer.\n",
    "    Returns:\n",
    "        outputs: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1 = tf.nn.relu(tf.matmul(train_dataset, weights) + biases)\n",
    "#         hidden1 = tf.matmul(train_dataset, weights) + biases\n",
    "        hidden1_drop = tf.nn.dropout(hidden1, dropout_keep_prob)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1_drop, weights) + biases)\n",
    "        hidden2_drop = tf.nn.dropout(hidden2, dropout_keep_prob)\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2_drop, weights) + biases)\n",
    "        hidden3_drop = tf.nn.dropout(hidden3, dropout_keep_prob)\n",
    "    # Hidden 4\n",
    "    with tf.variable_scope('hidden4', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, num_hidden4_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden4_units])\n",
    "        hidden4 = tf.nn.relu(tf.matmul(hidden3_drop, weights) + biases)\n",
    "        hidden4_drop = tf.nn.dropout(hidden4, dropout_keep_prob)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden4_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        outputs = tf.matmul(hidden4_drop, weights) + biases\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Build training graph.\n",
    "def performFeedForwardNeuralNetworkTraining(outputs, labels, initial_learning_rate, input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, num_hidden4_units, output_size):\n",
    "    \"\"\"Build the training graph.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Output tensor, float - [BATCH_SIZE, output_size].\n",
    "        labels : Labels tensor, float - [BATCH_SIZE, output_size].\n",
    "        initial_learning_rate: The initial learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates L2 prediction loss.\n",
    "    pred_l2_loss = tf.nn.l2_loss(outputs - labels, name='my_pred_l2_loss')\n",
    "    \n",
    "    # Create an operation that calculates L2 loss.\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        hidden2_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        hidden3_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Hidden 4\n",
    "    with tf.variable_scope('hidden4', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, num_hidden4_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden4_units])\n",
    "        hidden4_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden4_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        output_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    \n",
    "    loss = tf.reduce_mean(pred_l2_loss, name='my_pred_l2_loss_mean') + (beta1 * (hidden1_layer_l2_loss + hidden2_layer_l2_loss + hidden3_layer_l2_loss + hidden4_layer_l2_loss + output_layer_l2_loss))\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Exponentially-decaying learning rate:\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, 0.1)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "#     train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     train_op = tf.train.MomentumOptimizer(learning_rate, momentum=learning_rate/4.0, use_nesterov=True).minimize(loss, global_step=global_step)\n",
    "    train_op = tf.train.AdagradOptimizer(initial_learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return train_op, loss, learning_rate\n",
    "\n",
    "# Save model.\n",
    "def saveFeedForwardNeuralNetworkToMATLABMatFile(input_size, num_hidden1_units, num_hidden2_units, num_hidden3_units, num_hidden4_units, output_size):\n",
    "    model_params={}\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        model_params['weights_1']=weights.eval()\n",
    "        model_params['biases_1']=biases.eval()\n",
    "    # Hidden 2\n",
    "    with tf.variable_scope('hidden2', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, num_hidden2_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden2_units])\n",
    "        model_params['weights_2']=weights.eval()\n",
    "        model_params['biases_2']=biases.eval()\n",
    "    # Hidden 3\n",
    "    with tf.variable_scope('hidden3', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden2_units, num_hidden3_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden3_units])\n",
    "        model_params['weights_3']=weights.eval()\n",
    "        model_params['biases_3']=biases.eval()\n",
    "    # Hidden 4\n",
    "    with tf.variable_scope('hidden4', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden3_units, num_hidden4_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden4_units])\n",
    "        model_params['weights_4']=weights.eval()\n",
    "        model_params['biases_4']=biases.eval()\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden4_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        model_params['weights_out']=weights.eval()\n",
    "        model_params['biases_out']=biases.eval()\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "# Build the complete graph for feeding inputs, training, and saving checkpoints.\n",
    "ff_nn_graph = tf.Graph()\n",
    "with ff_nn_graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=[batch_size, D_input], name=\"tf_train_dataset_placeholder\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=[batch_size, D_output], name=\"tf_train_labels_placeholder\")\n",
    "    tf_train_all_dataset = tf.constant(X_train_dataset, name=\"tf_train_all_dataset_constant\")\n",
    "    tf_valid_dataset = tf.constant(X_valid_dataset, name=\"tf_valid_dataset_constant\")\n",
    "    tf_test_dataset = tf.constant(X_test_dataset, name=\"tf_test_dataset_constant\")\n",
    "    tf_whole_dataset = tf.constant(X_init_offset_cancelled, name=\"tf_whole_dataset_constant\")\n",
    "    tf_whole_all_dataset = tf.constant(X_init_offset_cancelled_all, name=\"tf_whole_all_dataset_constant\")\n",
    "    tf_extrapolate_test_dataset = tf.constant(X_extrapt, name=\"tf_extrapolate_test_dataset_constant\")\n",
    "    \n",
    "    # Currently turn off dropouts:\n",
    "    tf_train_dropout_keep_prob = 0.77\n",
    "    \n",
    "    # Define the Neural Network model.\n",
    "    defineFeedForwardNeuralNetworkModel(D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output)\n",
    "    \n",
    "    # Build the Prediction Graph (that computes predictions from the inference model).\n",
    "    tf_outputs = performFeedForwardNeuralNetworkPrediction(tf_train_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, tf_train_dropout_keep_prob)\n",
    "    \n",
    "    # Build the Training Graph (that calculate and apply gradients).\n",
    "    train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.1, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output)\n",
    "#     train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.00001, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output)\n",
    "    \n",
    "    # Create a summary:\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "    \n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf_outputs\n",
    "    train_all_prediction = performFeedForwardNeuralNetworkPrediction(tf_train_all_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    valid_prediction = performFeedForwardNeuralNetworkPrediction(tf_valid_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    test_prediction  = performFeedForwardNeuralNetworkPrediction(tf_test_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    whole_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    whole_all_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_all_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "    extrapolate_test_prediction = performFeedForwardNeuralNetworkPrediction(tf_extrapolate_test_dataset, D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output, 1.0)\n",
    "\n",
    "# Run training for num_steps and save checkpoint at the end.\n",
    "with tf.Session(graph=ff_nn_graph) as session:\n",
    "    # Run the Op to initialize the variables.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(num_steps):\n",
    "        # Read a batch of input dataset and labels.\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (Ct_train.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = X_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = Ct_train[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value, predictions, summary = session.run([train_op, loss, train_prediction, summary_op], feed_dict=feed_dict)\n",
    "        \n",
    "        # write log\n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, loss_value))\n",
    "            print(\"Minibatch NMSE: \", computeNMSE(predictions, batch_labels))\n",
    "            print(\"Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "            print(\"Extrapolation NMSE: \", computeNMSE(extrapolate_test_prediction.eval(), Ctt_extrapt))\n",
    "        if (step % 5000 == 0):\n",
    "            model_params = saveFeedForwardNeuralNetworkToMATLABMatFile(D_input, N_HIDDEN1_UNITS, N_HIDDEN2_UNITS, N_HIDDEN3_UNITS, N_HIDDEN4_UNITS, D_output)\n",
    "            print(\"Logging model_params.mat ...\")\n",
    "            sio.savemat('model_params/model_params.mat', model_params)\n",
    "            \n",
    "            whole_prediction_result = whole_prediction.eval()\n",
    "            whole_prediction_result_dict={}\n",
    "            whole_prediction_result_dict['whole_prediction_result'] = whole_prediction_result\n",
    "            print(\"Logging Ct_fit_onset.mat ...\")\n",
    "            sio.savemat('scraping/Ct_fit_onset.mat', whole_prediction_result_dict)\n",
    "            whole_all_prediction_result = whole_all_prediction.eval()\n",
    "            whole_all_prediction_result_dict={}\n",
    "            whole_all_prediction_result_dict['whole_all_prediction_result'] = whole_all_prediction_result\n",
    "            print(\"Logging Ct_fit_all.mat ...\")\n",
    "            sio.savemat('scraping/Ct_fit_all.mat', whole_all_prediction_result_dict)\n",
    "    print(\"Final Training NMSE  : \", computeNMSE(train_all_prediction.eval(), Ct_train))\n",
    "    print(\"Final Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "    print(\"Final Test NMSE      : \", computeNMSE(test_prediction.eval(), Ct_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
