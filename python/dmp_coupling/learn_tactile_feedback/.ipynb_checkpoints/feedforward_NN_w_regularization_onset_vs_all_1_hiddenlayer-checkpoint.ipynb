{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Feedforward Neural Network with Regularization\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First load the data dumped by MATLAB (*.mat file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [],
   "source": [
    "# X_init_offset_cancelled = sio.loadmat('scraping/X_init_offset_cancelled_scraping.mat', struct_as_record=True)['X_init_offset_cancelled']\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_phasePSI_scraping.mat', struct_as_record=True)['Xioc_phasePSI']\n",
    "#X_234_all\n",
    "X_init_offset_cancelled_all= sio.loadmat('scraping/X_gauss_basis_func_scraping.mat', struct_as_record=True)['X_gauss_basis_func'].astype(np.float32)\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/Xioc_PD_ratio_mean_3std_scraping.mat', struct_as_record=True)['Xioc_PD_ratio_mean_3std']\n",
    "# Ct_target = sio.loadmat('scraping/Ct_target_scraping.mat', struct_as_record=True)['Ct_target']\n",
    "\n",
    "#X_234\n",
    "X_init_offset_cancelled = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_train.mat', struct_as_record=True)['X_gauss_basis_func_train'].astype(np.float32)\n",
    "#Ct_target_234\n",
    "Ct_target = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_train.mat', struct_as_record=True)['Ct_target_filt_train'].astype(np.float32)\n",
    "\n",
    "# Dataset for Extrapolation Test\n",
    "#X_5toend\n",
    "X_extrapolate_test = sio.loadmat('scraping/X_gauss_basis_func_scraping_elim_3_test.mat', struct_as_record=True)['X_gauss_basis_func_test'].astype(np.float32)\n",
    "#Ct_5toend\n",
    "Ctt_extrapolate_test = sio.loadmat('scraping/Ct_target_filt_scraping_elim_3_test.mat', struct_as_record=True)['Ct_target_filt_test'].astype(np.float32)\n",
    "\n",
    "# Dummy Data for learning simulation/verification:\n",
    "# X_init_offset_cancelled = sio.loadmat('scraping/dummy_X.mat', struct_as_record=True)['X']\n",
    "# Ct_target = sio.loadmat('scraping/dummy_Ct.mat', struct_as_record=True)['Ct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Verify the dimensions are correct and shuffle the data (for Stochastic Gradient Descent (SGD)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_init_offset_cancelled.shape = (128105, 250)\n",
      "Ct_target.shape = (128105, 6)\n",
      "N_data   = 128105\n",
      "D_input  = 250\n",
      "D_output = 6\n",
      "X_extrapolate_test.shape = (198563, 250)\n",
      "Ctt_extrapolate_test.shape = (198563, 6)\n",
      "N_train_dataset = 108889\n",
      "N_valid_dataset = 9608\n",
      "N_test_dataset  = 9608\n"
     ]
    }
   ],
   "source": [
    "N_data_extrapolate_test = Ctt_extrapolate_test.shape[0]\n",
    "permutation_extrapolate_test = np.random.permutation(N_data_extrapolate_test)\n",
    "permutation_extrapolate_test_select = permutation_extrapolate_test[1:1000]\n",
    "X_extrapt = X_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "Ctt_extrapt = Ctt_extrapolate_test[permutation_extrapolate_test_select, :]\n",
    "\n",
    "print('X_init_offset_cancelled.shape =', X_init_offset_cancelled.shape)\n",
    "print('Ct_target.shape =', Ct_target.shape)\n",
    "\n",
    "N_data = Ct_target.shape[0]\n",
    "D_input = X_init_offset_cancelled.shape[1]\n",
    "D_output = Ct_target.shape[1]\n",
    "print('N_data   =', N_data)\n",
    "print('D_input  =', D_input)\n",
    "print('D_output =', D_output)\n",
    "\n",
    "print('X_extrapolate_test.shape =', X_extrapolate_test.shape)\n",
    "print('Ctt_extrapolate_test.shape =', Ctt_extrapolate_test.shape)\n",
    "\n",
    "random.seed(38)\n",
    "np.random.seed(38)\n",
    "\n",
    "X_init_offset_cancelled = X_init_offset_cancelled\n",
    "X_init_offset_cancelled_all = X_init_offset_cancelled_all\n",
    "\n",
    "permutation = np.random.permutation(N_data)\n",
    "X_shuffled = X_init_offset_cancelled[permutation,:]\n",
    "Ct_target_shuffled = Ct_target[permutation,:]\n",
    "\n",
    "fraction_train_dataset = 0.85\n",
    "fraction_test_dataset  = 0.075\n",
    "\n",
    "N_train_dataset = np.round(fraction_train_dataset * N_data).astype(int)\n",
    "N_test_dataset = np.round(fraction_test_dataset * N_data).astype(int)\n",
    "N_valid_dataset = N_data - N_train_dataset - N_test_dataset\n",
    "print('N_train_dataset =', N_train_dataset)\n",
    "print('N_valid_dataset =', N_valid_dataset)\n",
    "print('N_test_dataset  =', N_test_dataset)\n",
    "\n",
    "X_train_dataset = X_shuffled[0:N_train_dataset,:]\n",
    "Ct_train = Ct_target_shuffled[0:N_train_dataset,:]\n",
    "X_valid_dataset = X_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "Ct_valid = Ct_target_shuffled[N_train_dataset:(N_train_dataset+N_valid_dataset),:]\n",
    "X_test_dataset = X_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]\n",
    "Ct_test = Ct_target_shuffled[(N_train_dataset+N_valid_dataset):N_data,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def computeNMSE(predictions, labels):\n",
    "    mse = np.mean(np.square(predictions-labels), axis=0);\n",
    "    var_labels = np.var(labels, axis=0)\n",
    "    nmse = np.divide(mse, var_labels)\n",
    "    return (nmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Feed-Forward Neural Network Model\n",
    "---------\n",
    "\n",
    "Here it goes:\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 410199.031250\n",
      "Minibatch NMSE:  [ 1.02269995  1.00042939  1.03722811  1.20382571  1.07267439  1.02224565]\n",
      "Validation NMSE:  [ 1.00239134  1.00763118  1.00272357  1.05911934  1.00619698  1.00431359]\n",
      "Extrapolation NMSE:  [ 1.00378561  1.00033092  1.02442539  1.04303765  1.00015128  1.00846493]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 500: 233574.015625\n",
      "Minibatch NMSE:  [ 0.96292245  0.8500731   0.9687137   0.86105114  0.94733775  0.9236514 ]\n",
      "Validation NMSE:  [ 0.78105903  0.86590213  0.9872281   0.87581813  0.8768      0.90225875]\n",
      "Extrapolation NMSE:  [ 0.78403485  0.86184454  1.0201205   0.89274877  0.99619877  1.0032866 ]\n",
      "Minibatch loss at step 1000: 182960.812500\n",
      "Minibatch NMSE:  [ 0.77471578  0.90589648  1.06141424  0.82078636  0.80106121  0.96909827]\n",
      "Validation NMSE:  [ 0.76714617  0.86391735  0.9834469   0.85068423  0.85466218  0.88091666]\n",
      "Extrapolation NMSE:  [ 0.77141303  0.85902619  1.01928997  0.88080925  0.94735259  0.95894462]\n",
      "Minibatch loss at step 1500: 196754.531250\n",
      "Minibatch NMSE:  [ 1.01370311  1.13019741  1.03973818  0.71181774  0.79558009  0.89874583]\n",
      "Validation NMSE:  [ 0.78943074  0.87118417  0.98590398  0.84236246  0.83179492  0.86078036]\n",
      "Extrapolation NMSE:  [ 0.79332471  0.87075204  0.99864131  0.88662642  0.9702304   0.97503984]\n",
      "Minibatch loss at step 2000: 198779.593750\n",
      "Minibatch NMSE:  [ 0.91706407  0.95813149  1.01447225  0.99546731  0.85213447  0.86854887]\n",
      "Validation NMSE:  [ 0.78210497  0.85588688  0.98518419  0.8296333   0.81860393  0.85133749]\n",
      "Extrapolation NMSE:  [ 0.78095514  0.85127926  1.01506722  0.88303876  0.96210885  0.98019129]\n",
      "Minibatch loss at step 2500: 388437.187500\n",
      "Minibatch NMSE:  [ 1.25705779  1.21550739  1.06533062  1.076949    0.94854593  0.93807817]\n",
      "Validation NMSE:  [ 0.78151757  0.85820723  0.98515898  0.82570958  0.81320131  0.84839505]\n",
      "Extrapolation NMSE:  [ 0.79261887  0.85548961  1.00673544  0.87514645  0.93875581  0.95663011]\n",
      "Minibatch loss at step 3000: 321280.437500\n",
      "Minibatch NMSE:  [ 0.65672046  0.78801346  0.96085411  0.74454665  0.59325784  0.69771951]\n",
      "Validation NMSE:  [ 0.77316117  0.85920376  0.98043489  0.8241564   0.80162442  0.83604729]\n",
      "Extrapolation NMSE:  [ 0.77840263  0.86116302  1.01205039  0.88519406  0.93690318  0.95133018]\n",
      "Minibatch loss at step 3500: 288939.500000\n",
      "Minibatch NMSE:  [ 1.33705497  1.03706014  0.95386374  0.88246363  0.99184787  0.9829216 ]\n",
      "Validation NMSE:  [ 0.74847537  0.84456223  0.98051614  0.81895703  0.78999871  0.82830095]\n",
      "Extrapolation NMSE:  [ 0.74319345  0.83709663  1.00121701  0.87851328  0.9739539   0.97813112]\n",
      "Minibatch loss at step 4000: 470364.718750\n",
      "Minibatch NMSE:  [ 0.69751108  0.96282506  1.10866535  0.81882888  0.96481746  0.93861622]\n",
      "Validation NMSE:  [ 0.7913456   0.86835068  0.98273456  0.81398869  0.78340894  0.82205164]\n",
      "Extrapolation NMSE:  [ 0.79747951  0.87023914  0.99959606  0.88717043  0.95740277  0.97400635]\n",
      "Minibatch loss at step 4500: 309276.656250\n",
      "Minibatch NMSE:  [ 0.86026245  0.91118187  1.01098633  0.79880553  0.91217142  1.05620217]\n",
      "Validation NMSE:  [ 0.77285153  0.85733879  0.97856998  0.81704199  0.78146875  0.81816667]\n",
      "Extrapolation NMSE:  [ 0.77992666  0.85394067  1.00488043  0.88410431  0.95208156  0.95956862]\n",
      "Minibatch loss at step 5000: 397177.406250\n",
      "Minibatch NMSE:  [ 0.78020394  0.81211394  0.97050446  0.76323366  0.72411406  0.79370493]\n",
      "Validation NMSE:  [ 0.78379893  0.86226481  0.97591835  0.81430936  0.76885194  0.81115371]\n",
      "Extrapolation NMSE:  [ 0.79187     0.85899347  1.00523221  0.88228458  0.95908237  0.96369696]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 5500: 270065.875000\n",
      "Minibatch NMSE:  [ 0.83143276  0.94807756  1.03557599  0.94183624  1.01373088  1.05956364]\n",
      "Validation NMSE:  [ 0.78585899  0.86334085  0.97401607  0.80960411  0.76018965  0.80342937]\n",
      "Extrapolation NMSE:  [ 0.78843594  0.85817182  0.99774438  0.88370073  0.95682758  0.97818351]\n",
      "Minibatch loss at step 6000: 342832.625000\n",
      "Minibatch NMSE:  [ 0.86322832  0.79340577  0.97211397  0.81072253  0.76587015  1.1012305 ]\n",
      "Validation NMSE:  [ 0.7827912   0.85993189  0.9773466   0.80885941  0.75706869  0.80129409]\n",
      "Extrapolation NMSE:  [ 0.79185164  0.85647362  0.99865615  0.87087059  0.92521101  0.95242107]\n",
      "Minibatch loss at step 6500: 332619.750000\n",
      "Minibatch NMSE:  [ 0.99904549  1.05532157  0.99279624  0.56192422  0.68920225  0.82849473]\n",
      "Validation NMSE:  [ 0.77655041  0.85553998  0.97363085  0.80768877  0.75133938  0.79361373]\n",
      "Extrapolation NMSE:  [ 0.77628416  0.84781212  1.00789499  0.88070601  0.93847293  0.95677823]\n",
      "Minibatch loss at step 7000: 229056.562500\n",
      "Minibatch NMSE:  [ 0.99993008  0.87213808  1.04090548  0.91774523  0.80369169  0.8353774 ]\n",
      "Validation NMSE:  [ 0.77107608  0.85360688  0.97339928  0.80577809  0.74595612  0.79177111]\n",
      "Extrapolation NMSE:  [ 0.77346748  0.84651417  0.99692154  0.87867415  0.96240056  0.97701365]\n",
      "Minibatch loss at step 7500: 283899.437500\n",
      "Minibatch NMSE:  [ 0.66543037  0.75169796  0.95767581  0.7671265   0.67110324  0.69906461]\n",
      "Validation NMSE:  [ 0.78312266  0.85751939  0.97292614  0.80506277  0.74317658  0.78780192]\n",
      "Extrapolation NMSE:  [ 0.77914959  0.85172576  0.99253356  0.8748132   0.95665878  0.97817469]\n",
      "Minibatch loss at step 8000: 411030.281250\n",
      "Minibatch NMSE:  [ 0.73157889  0.9227922   1.04008365  0.87637353  0.66510105  0.83448762]\n",
      "Validation NMSE:  [ 0.79212254  0.85915083  0.97239101  0.80186635  0.74389899  0.78938925]\n",
      "Extrapolation NMSE:  [ 0.79691267  0.85460591  0.99532062  0.87267429  0.93836272  0.96282965]\n",
      "Minibatch loss at step 8500: 455364.562500\n",
      "Minibatch NMSE:  [ 0.92838919  0.92458904  0.99807781  0.94515038  0.89348316  1.00093544]\n",
      "Validation NMSE:  [ 0.79311734  0.86413598  0.9723596   0.80139148  0.73815423  0.78652525]\n",
      "Extrapolation NMSE:  [ 0.79937327  0.86108911  0.99482429  0.87457746  0.9443686   0.96888655]\n",
      "Minibatch loss at step 9000: 279395.625000\n",
      "Minibatch NMSE:  [ 0.71709514  0.85669774  0.96188807  0.71102035  0.7634753   1.059111  ]\n",
      "Validation NMSE:  [ 0.78323829  0.85768223  0.97088861  0.79990768  0.7322818   0.78145206]\n",
      "Extrapolation NMSE:  [ 0.77843624  0.8504169   1.00282717  0.87876177  0.94392717  0.97001469]\n",
      "Minibatch loss at step 9500: 376152.187500\n",
      "Minibatch NMSE:  [ 1.01988888  1.03954864  0.99441314  0.80460256  0.95658582  0.93518806]\n",
      "Validation NMSE:  [ 0.77803451  0.8539151   0.97403157  0.79759997  0.73032552  0.77813637]\n",
      "Extrapolation NMSE:  [ 0.77190393  0.84266895  1.00454926  0.87196749  0.93345481  0.95723403]\n",
      "Minibatch loss at step 10000: 225749.281250\n",
      "Minibatch NMSE:  [ 1.03981006  0.99053651  1.01948655  0.98550528  0.70420027  0.78096282]\n",
      "Validation NMSE:  [ 0.78350198  0.85797095  0.97448677  0.79944915  0.728046    0.77539641]\n",
      "Extrapolation NMSE:  [ 0.78062403  0.84791571  0.99560368  0.86661386  0.94533068  0.96211606]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 10500: 195574.218750\n",
      "Minibatch NMSE:  [ 1.06159294  0.98446947  0.99078995  0.76476002  0.61119443  0.77672321]\n",
      "Validation NMSE:  [ 0.78202373  0.85927778  0.97339135  0.79704952  0.7243014   0.77433431]\n",
      "Extrapolation NMSE:  [ 0.77918798  0.85304356  0.99661881  0.87179655  0.96283376  0.98411137]\n",
      "Minibatch loss at step 11000: 441632.750000\n",
      "Minibatch NMSE:  [ 0.53657937  0.89233768  0.98107052  0.91052735  0.77929544  0.82078159]\n",
      "Validation NMSE:  [ 0.77985507  0.85319799  0.97425926  0.79630053  0.72377825  0.77445298]\n",
      "Extrapolation NMSE:  [ 0.7804175   0.84448135  0.99738252  0.86902374  0.93728983  0.96429074]\n",
      "Minibatch loss at step 11500: 251285.031250\n",
      "Minibatch NMSE:  [ 0.71706957  0.83043212  0.94757754  0.89220285  0.78231269  0.87630618]\n",
      "Validation NMSE:  [ 0.78891665  0.85553253  0.97222155  0.79837108  0.72122407  0.7719844 ]\n",
      "Extrapolation NMSE:  [ 0.78589028  0.84970284  0.99949074  0.87706751  0.94649589  0.96627837]\n",
      "Minibatch loss at step 12000: 27649936.000000\n",
      "Minibatch NMSE:  [ 0.7509225   0.84018308  1.08331645  0.74909967  1.00483942  0.8728646 ]\n",
      "Validation NMSE:  [ 0.76739466  0.84807998  0.97386003  0.79575586  0.7205162   0.77340752]\n",
      "Extrapolation NMSE:  [ 0.76194209  0.83604509  0.99847597  0.8732028   0.9829396   0.99615574]\n",
      "Minibatch loss at step 12500: 271528.031250\n",
      "Minibatch NMSE:  [ 0.79331267  0.88180828  0.92942983  0.8787992   0.77265841  0.88728547]\n",
      "Validation NMSE:  [ 0.80091709  0.86850619  0.9740876   0.7965349   0.71623194  0.76811802]\n",
      "Extrapolation NMSE:  [ 0.80365646  0.86247921  0.99535912  0.88092691  0.93915594  0.96984613]\n",
      "Minibatch loss at step 13000: 256104.000000\n",
      "Minibatch NMSE:  [ 0.69353122  0.89647484  1.01563907  0.88513225  0.79913032  0.95755088]\n",
      "Validation NMSE:  [ 0.79236132  0.85940701  0.97407454  0.79533035  0.7146436   0.76569396]\n",
      "Extrapolation NMSE:  [ 0.79407638  0.85401845  0.99687427  0.87636942  0.95354295  0.97556978]\n",
      "Minibatch loss at step 13500: 247346.562500\n",
      "Minibatch NMSE:  [ 0.85268778  0.91004431  0.93256927  0.76847041  0.65552092  0.86215156]\n",
      "Validation NMSE:  [ 0.79703563  0.85669768  0.97493505  0.79322505  0.71385509  0.76560903]\n",
      "Extrapolation NMSE:  [ 0.79693907  0.85374749  1.00498104  0.87288499  0.94949716  0.96523631]\n",
      "Minibatch loss at step 14000: 186359.500000\n",
      "Minibatch NMSE:  [ 0.74435824  0.76537448  1.04983366  0.64401513  0.4966878   0.68724763]\n",
      "Validation NMSE:  [ 0.79162902  0.85430825  0.97255445  0.79400444  0.70835763  0.7602036 ]\n",
      "Extrapolation NMSE:  [ 0.78845912  0.84667051  0.99789959  0.87904364  0.96899343  0.98684406]\n",
      "Minibatch loss at step 14500: 237278.328125\n",
      "Minibatch NMSE:  [ 0.75559127  0.77375573  1.00600469  0.83535463  0.85989845  0.95568395]\n",
      "Validation NMSE:  [ 0.78731239  0.85332131  0.97438872  0.7927112   0.70821559  0.76045793]\n",
      "Extrapolation NMSE:  [ 0.78306526  0.8427037   0.99644899  0.8696059   0.94199008  0.96911705]\n",
      "Minibatch loss at step 15000: 211777.109375\n",
      "Minibatch NMSE:  [ 1.01009703  1.04009581  0.97167808  0.80419219  0.53005338  0.69498295]\n",
      "Validation NMSE:  [ 0.79576558  0.85762119  0.97240388  0.79374653  0.7059263   0.75829583]\n",
      "Extrapolation NMSE:  [ 0.79176468  0.84880292  1.00542331  0.87759733  0.93937784  0.95796019]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 15500: 360303.718750\n",
      "Minibatch NMSE:  [ 0.89123678  0.96095312  0.96258414  0.74503917  0.88810837  0.92089868]\n",
      "Validation NMSE:  [ 0.79019523  0.85580307  0.97277397  0.7931655   0.70448899  0.75698328]\n",
      "Extrapolation NMSE:  [ 0.78681356  0.84893841  1.00374627  0.88250291  0.96912766  0.98424095]\n",
      "Minibatch loss at step 16000: 284316.875000\n",
      "Minibatch NMSE:  [ 0.7498076   0.84852964  0.96777129  1.06217968  0.68059397  0.81692404]\n",
      "Validation NMSE:  [ 0.79648095  0.85727429  0.97250694  0.79235011  0.70267886  0.75580484]\n",
      "Extrapolation NMSE:  [ 0.78823024  0.85092318  1.00047863  0.88106793  0.96430027  0.99025381]\n",
      "Minibatch loss at step 16500: 369438.375000\n",
      "Minibatch NMSE:  [ 0.95365465  1.0706377   1.02369392  0.87142849  0.50093192  0.75560427]\n",
      "Validation NMSE:  [ 0.80218333  0.85788113  0.9729206   0.79232472  0.70167691  0.75662601]\n",
      "Extrapolation NMSE:  [ 0.79616612  0.85368824  0.99940974  0.87415087  0.96505433  0.98301923]\n",
      "Minibatch loss at step 17000: 186595.781250\n",
      "Minibatch NMSE:  [ 0.74625552  0.85000247  1.00125635  0.72119838  0.59183627  0.87512839]\n",
      "Validation NMSE:  [ 0.80470586  0.86121482  0.97412342  0.79133254  0.6987257   0.75258976]\n",
      "Extrapolation NMSE:  [ 0.80214715  0.85705221  0.99823558  0.87447381  0.96547574  0.98366261]\n",
      "Minibatch loss at step 17500: 215920.437500\n",
      "Minibatch NMSE:  [ 0.90439868  0.92758763  0.93121862  0.9202826   0.64925879  0.71806264]\n",
      "Validation NMSE:  [ 0.80175298  0.85881668  0.97096694  0.79219067  0.69625312  0.75107992]\n",
      "Extrapolation NMSE:  [ 0.79536068  0.85231054  1.00160289  0.87972993  0.95952398  0.98271292]\n",
      "Minibatch loss at step 18000: 270971.500000\n",
      "Minibatch NMSE:  [ 0.87458557  0.94836628  0.97528911  0.79722959  0.69605422  0.66890991]\n",
      "Validation NMSE:  [ 0.79586816  0.85681278  0.97290474  0.78863513  0.696585    0.75205296]\n",
      "Extrapolation NMSE:  [ 0.79142141  0.84846574  1.00509143  0.87592953  0.95494205  0.97876596]\n",
      "Minibatch loss at step 18500: 345737.468750\n",
      "Minibatch NMSE:  [ 1.04226565  0.95338702  0.95654362  0.91714537  0.72115642  0.76247108]\n",
      "Validation NMSE:  [ 0.80718207  0.86238253  0.97360831  0.79204726  0.69672447  0.75011241]\n",
      "Extrapolation NMSE:  [ 0.80039555  0.85401207  1.00044131  0.87406516  0.94873297  0.96881378]\n",
      "Minibatch loss at step 19000: 254215.140625\n",
      "Minibatch NMSE:  [ 1.37841058  1.24895155  1.01910317  0.72829521  0.68318391  0.83975416]\n",
      "Validation NMSE:  [ 0.79749441  0.85672629  0.97252524  0.7894786   0.6940465   0.75074899]\n",
      "Extrapolation NMSE:  [ 0.78950065  0.84886879  1.00475395  0.87868869  0.95781744  0.97987276]\n",
      "Minibatch loss at step 19500: 291336.937500\n",
      "Minibatch NMSE:  [ 1.03123581  0.98842484  1.04169405  0.85219646  0.85178804  0.89240879]\n",
      "Validation NMSE:  [ 0.80574876  0.86090004  0.97252476  0.78855985  0.69328541  0.74972743]\n",
      "Extrapolation NMSE:  [ 0.79800546  0.85242707  1.00460625  0.87683547  0.96680868  0.99076664]\n",
      "Minibatch loss at step 20000: 287941.906250\n",
      "Minibatch NMSE:  [ 0.84050018  0.86572933  1.01482177  0.79249799  0.68471545  0.89622349]\n",
      "Validation NMSE:  [ 0.81231344  0.86590987  0.97166294  0.78848028  0.69402766  0.74915397]\n",
      "Extrapolation NMSE:  [ 0.80903912  0.86018586  1.00367773  0.88034755  0.94361544  0.9794423 ]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 20500: 276124.531250\n",
      "Minibatch NMSE:  [ 0.98103601  0.94511575  1.03286624  0.71414101  0.6131928   0.58192384]\n",
      "Validation NMSE:  [ 0.80481327  0.86148024  0.97476602  0.78894913  0.68960327  0.74632001]\n",
      "Extrapolation NMSE:  [ 0.79905671  0.85572994  1.00227892  0.88019794  0.97975838  0.99962485]\n",
      "Minibatch loss at step 21000: 680041.125000\n",
      "Minibatch NMSE:  [ 0.81208479  0.94834936  0.97677588  0.77606374  0.9886108   0.76213527]\n",
      "Validation NMSE:  [ 0.81643468  0.86693537  0.97397935  0.78686845  0.68777293  0.74335003]\n",
      "Extrapolation NMSE:  [ 0.8097381   0.8611787   0.99986762  0.88109797  0.97056973  0.99550396]\n",
      "Minibatch loss at step 21500: 194976.343750\n",
      "Minibatch NMSE:  [ 1.01315773  0.95550811  0.94613808  0.67654103  0.54147148  0.71865177]\n",
      "Validation NMSE:  [ 0.79785597  0.85611445  0.97207099  0.78840876  0.68740326  0.74314523]\n",
      "Extrapolation NMSE:  [ 0.78654498  0.84727407  1.00413144  0.87285781  0.9559353   0.97823548]\n",
      "Minibatch loss at step 22000: 248928.140625\n",
      "Minibatch NMSE:  [ 1.22212839  1.05038214  0.9148525   0.76224846  0.72585225  0.88217211]\n",
      "Validation NMSE:  [ 0.82017857  0.86691576  0.97322524  0.78842551  0.687621    0.74378091]\n",
      "Extrapolation NMSE:  [ 0.81262791  0.86272401  1.00177765  0.87588358  0.9595089   0.98025841]\n",
      "Minibatch loss at step 22500: 209975.640625\n",
      "Minibatch NMSE:  [ 0.81610948  1.00825918  1.09213102  0.92270392  0.71734816  0.77607685]\n",
      "Validation NMSE:  [ 0.80367064  0.85888869  0.97310019  0.78646994  0.68311739  0.7413187 ]\n",
      "Extrapolation NMSE:  [ 0.79376274  0.85092336  1.00507617  0.88205135  0.98649824  1.00359571]\n",
      "Minibatch loss at step 23000: 378668.250000\n",
      "Minibatch NMSE:  [ 0.75870633  1.04533541  0.99820638  0.97982168  0.56224954  0.55995661]\n",
      "Validation NMSE:  [ 0.79878527  0.85615182  0.97419852  0.78530973  0.68433064  0.74256068]\n",
      "Extrapolation NMSE:  [ 0.79011559  0.8449834   0.99884975  0.87456459  0.972956    0.9917326 ]\n",
      "Minibatch loss at step 23500: 160249.375000\n",
      "Minibatch NMSE:  [ 1.01464593  1.11483443  0.96437633  0.74663383  0.65010101  0.77764124]\n",
      "Validation NMSE:  [ 0.82013375  0.86340159  0.97249758  0.7856288   0.6847049   0.73948193]\n",
      "Extrapolation NMSE:  [ 0.81091255  0.85650444  1.00474691  0.87935072  0.96317005  0.98340327]\n",
      "Minibatch loss at step 24000: 287945.187500\n",
      "Minibatch NMSE:  [ 0.71528631  0.76666588  0.93897933  0.74901432  0.62359285  0.7639997 ]\n",
      "Validation NMSE:  [ 0.79606843  0.85490817  0.9721759   0.78502995  0.68170995  0.73844898]\n",
      "Extrapolation NMSE:  [ 0.7842505   0.84335828  1.00287557  0.88353527  0.98467672  1.00540447]\n",
      "Minibatch loss at step 24500: 247994.250000\n",
      "Minibatch NMSE:  [ 0.60572678  0.77529508  0.99047065  0.92026186  0.70852202  0.9837113 ]\n",
      "Validation NMSE:  [ 0.81779003  0.86398804  0.97179163  0.78345436  0.68084407  0.73931491]\n",
      "Extrapolation NMSE:  [ 0.80795085  0.85854417  1.00069499  0.88255525  0.98045349  1.00168645]\n",
      "Minibatch loss at step 25000: 254485.156250\n",
      "Minibatch NMSE:  [ 0.77367973  0.70625919  0.94056726  0.83056074  0.83578432  0.95545441]\n",
      "Validation NMSE:  [ 0.81067699  0.85640961  0.97088349  0.78240228  0.68078327  0.73668748]\n",
      "Extrapolation NMSE:  [ 0.80043513  0.84781331  1.00166023  0.87379438  0.96230042  0.98543447]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 25500: 234881.000000\n",
      "Minibatch NMSE:  [ 0.91144323  0.9067679   0.96778423  0.77730173  0.67162418  0.91200048]\n",
      "Validation NMSE:  [ 0.82031161  0.86434478  0.97296464  0.78419524  0.6803714   0.73851305]\n",
      "Extrapolation NMSE:  [ 0.81372428  0.85780412  1.00082278  0.87963903  0.97358423  0.99323821]\n",
      "Minibatch loss at step 26000: 337619.531250\n",
      "Minibatch NMSE:  [ 0.68742931  0.75685179  0.95026445  0.87053585  0.67175639  0.62208223]\n",
      "Validation NMSE:  [ 0.81559825  0.86213231  0.97030187  0.78265595  0.67657936  0.7359603 ]\n",
      "Extrapolation NMSE:  [ 0.80754584  0.85362953  1.00377202  0.88603044  0.96634609  0.99235994]\n",
      "Minibatch loss at step 26500: 291156.000000\n",
      "Minibatch NMSE:  [ 0.86646748  0.95509309  0.9834621   0.68209773  0.64890456  0.69392687]\n",
      "Validation NMSE:  [ 0.80639935  0.85866529  0.97250319  0.78236645  0.67874032  0.73769921]\n",
      "Extrapolation NMSE:  [ 0.7976352   0.84935755  1.00586951  0.87646061  0.95687681  0.98653054]\n",
      "Minibatch loss at step 27000: 257666.671875\n",
      "Minibatch NMSE:  [ 1.10479581  0.86065125  1.13096511  0.79043591  0.8363117   0.83918369]\n",
      "Validation NMSE:  [ 0.81542432  0.86242872  0.97320634  0.78345418  0.67786747  0.73640811]\n",
      "Extrapolation NMSE:  [ 0.80466563  0.85233116  1.00330174  0.87576562  0.96346188  0.97998106]\n",
      "Minibatch loss at step 27500: 396698.875000\n",
      "Minibatch NMSE:  [ 0.94701296  0.98284608  1.01357996  0.90651459  0.71223325  0.88974732]\n",
      "Validation NMSE:  [ 0.8126806   0.86266267  0.97319686  0.78151733  0.67740399  0.73787975]\n",
      "Extrapolation NMSE:  [ 0.80323792  0.85343421  1.00549316  0.88121545  0.99583703  1.00601244]\n",
      "Minibatch loss at step 28000: 166586.093750\n",
      "Minibatch NMSE:  [ 0.81188136  0.84991026  1.05332458  0.76144642  0.71493345  0.89243424]\n",
      "Validation NMSE:  [ 0.80433387  0.85562921  0.97263789  0.78076303  0.67451298  0.73296762]\n",
      "Extrapolation NMSE:  [ 0.79478329  0.84517699  1.00802422  0.87860769  0.985497    1.00603235]\n",
      "Minibatch loss at step 28500: 204530.468750\n",
      "Minibatch NMSE:  [ 1.30126369  1.03435683  0.94643772  0.72983998  0.49755019  0.66229552]\n",
      "Validation NMSE:  [ 0.80726528  0.85663015  0.97157037  0.78197271  0.67890108  0.73738688]\n",
      "Extrapolation NMSE:  [ 0.79444069  0.84489298  1.00792921  0.87734717  0.9673419   0.99059087]\n",
      "Minibatch loss at step 29000: 204603.281250\n",
      "Minibatch NMSE:  [ 0.92818648  0.99887484  1.02994657  0.7741468   0.56528956  0.84634876]\n",
      "Validation NMSE:  [ 0.81645411  0.86285418  0.97404188  0.7811172   0.6763739   0.73640811]\n",
      "Extrapolation NMSE:  [ 0.81117916  0.85465354  1.00478816  0.88208115  0.97886056  1.00646043]\n",
      "Minibatch loss at step 29500: 225024.968750\n",
      "Minibatch NMSE:  [ 1.08582735  0.97899216  1.05412722  0.87584037  0.76713407  0.70693469]\n",
      "Validation NMSE:  [ 0.81894922  0.86460567  0.9739356   0.77952731  0.67385936  0.73340768]\n",
      "Extrapolation NMSE:  [ 0.81006348  0.85708612  1.00542271  0.88628399  0.97287619  1.00100923]\n",
      "Minibatch loss at step 30000: 311280.312500\n",
      "Minibatch NMSE:  [ 0.65302831  0.78916305  0.9539718   0.69082749  0.75612748  0.70482981]\n",
      "Validation NMSE:  [ 0.79458541  0.85270494  0.97224271  0.78109348  0.67006123  0.72803211]\n",
      "Extrapolation NMSE:  [ 0.77931279  0.83874667  1.01003146  0.8781057   0.97368211  0.99571103]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 30500: 282018.781250\n",
      "Minibatch NMSE:  [ 0.80934215  0.79048353  1.01743162  0.92456216  0.53291327  0.63836527]\n",
      "Validation NMSE:  [ 0.81942517  0.86399555  0.97302765  0.78005695  0.67146474  0.7300095 ]\n",
      "Extrapolation NMSE:  [ 0.81070179  0.85723251  1.00603199  0.87895364  0.96455109  0.98362607]\n",
      "Minibatch loss at step 31000: 274024.500000\n",
      "Minibatch NMSE:  [ 0.76814193  0.71710211  1.0096519   0.7154789   0.5580287   0.64057994]\n",
      "Validation NMSE:  [ 0.81694704  0.86482751  0.97293872  0.78109157  0.66915739  0.72982609]\n",
      "Extrapolation NMSE:  [ 0.80830491  0.85696721  1.00613236  0.8889209   0.98618335  1.00982583]\n",
      "Minibatch loss at step 31500: 231590.234375\n",
      "Minibatch NMSE:  [ 0.67649019  0.88178873  1.03451276  1.33072853  0.55139202  0.66528243]\n",
      "Validation NMSE:  [ 0.80329722  0.85771477  0.97332275  0.77869064  0.67201298  0.73280507]\n",
      "Extrapolation NMSE:  [ 0.7913425   0.84771323  1.00347733  0.8795563   0.96944642  0.9915334 ]\n",
      "Minibatch loss at step 32000: 181994.953125\n",
      "Minibatch NMSE:  [ 0.93122619  0.99597538  1.00188124  0.85111433  0.43252772  0.60710067]\n",
      "Validation NMSE:  [ 0.81335455  0.86051327  0.97148818  0.78073299  0.66832292  0.72580886]\n",
      "Extrapolation NMSE:  [ 0.79943216  0.8509742   1.00805461  0.88089299  0.95845938  0.9798516 ]\n",
      "Minibatch loss at step 32500: 229578.093750\n",
      "Minibatch NMSE:  [ 0.77192974  0.90814257  1.041605    0.78278357  1.0006988   0.98111457]\n",
      "Validation NMSE:  [ 0.79693115  0.85323185  0.97035593  0.77659529  0.66696894  0.72641349]\n",
      "Extrapolation NMSE:  [ 0.77799565  0.83925635  1.00850654  0.88226867  0.97119039  0.98937345]\n",
      "Minibatch loss at step 33000: 192188.031250\n",
      "Minibatch NMSE:  [ 0.84867293  0.87648988  1.02614701  0.7197597   0.77908319  0.89251208]\n",
      "Validation NMSE:  [ 0.82014942  0.86663002  0.97199631  0.77774233  0.66767871  0.72587746]\n",
      "Extrapolation NMSE:  [ 0.80805814  0.85745317  1.0021255   0.88600105  0.98089892  1.00291336]\n",
      "Minibatch loss at step 33500: 381026.000000\n",
      "Minibatch NMSE:  [ 0.75151396  0.9744615   0.98228711  0.50432122  0.81014037  0.80895621]\n",
      "Validation NMSE:  [ 0.81821764  0.86368161  0.9726662   0.77700758  0.6675092   0.72737551]\n",
      "Extrapolation NMSE:  [ 0.80788958  0.85443598  1.00808203  0.87742132  0.97011364  0.99408984]\n",
      "Minibatch loss at step 34000: 321194.375000\n",
      "Minibatch NMSE:  [ 0.67211342  0.7561447   0.95692062  0.9919225   0.53327388  0.69662827]\n",
      "Validation NMSE:  [ 0.82204145  0.86554146  0.97132462  0.77748281  0.66748208  0.72679192]\n",
      "Extrapolation NMSE:  [ 0.81225747  0.8569718   1.00541198  0.87987673  0.97730941  1.00000775]\n",
      "Minibatch loss at step 34500: 221207.875000\n",
      "Minibatch NMSE:  [ 0.82405931  0.80420882  0.94054365  0.62954921  0.82929319  0.77548581]\n",
      "Validation NMSE:  [ 0.81870848  0.86287957  0.97005248  0.77666062  0.66382784  0.7244457 ]\n",
      "Extrapolation NMSE:  [ 0.80681676  0.85376751  1.00582433  0.88349062  0.97223771  0.99392146]\n",
      "Minibatch loss at step 35000: 271775.687500\n",
      "Minibatch NMSE:  [ 0.99787158  1.02816033  0.96711624  1.14376879  1.17498982  1.08868301]\n",
      "Validation NMSE:  [ 0.81594938  0.86355251  0.97192228  0.77697742  0.66481763  0.72629565]\n",
      "Extrapolation NMSE:  [ 0.80393624  0.85295725  1.00532162  0.87516195  0.97505665  0.99362707]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 35500: 199813.734375\n",
      "Minibatch NMSE:  [ 0.87234855  1.04635513  1.07284141  0.84487587  0.36170858  0.39487049]\n",
      "Validation NMSE:  [ 0.82321513  0.86817414  0.9708333   0.77963263  0.6649611   0.7241115 ]\n",
      "Extrapolation NMSE:  [ 0.81048137  0.85579109  1.00183511  0.87617004  0.96392107  0.98279303]\n",
      "Minibatch loss at step 36000: 369932.500000\n",
      "Minibatch NMSE:  [ 0.84132653  0.83441812  0.99978977  0.86335903  0.59699351  0.70451778]\n",
      "Validation NMSE:  [ 0.80836397  0.85827577  0.97013175  0.77607042  0.6647495   0.72594625]\n",
      "Extrapolation NMSE:  [ 0.79325551  0.84633994  1.00426197  0.8791616   0.98498333  1.00069261]\n",
      "Minibatch loss at step 36500: 559919.062500\n",
      "Minibatch NMSE:  [ 1.13502979  0.9941597   0.9826737   0.8439666   0.72808582  0.80077386]\n",
      "Validation NMSE:  [ 0.80945057  0.85786378  0.97042412  0.77567357  0.66143572  0.72131306]\n",
      "Extrapolation NMSE:  [ 0.79386383  0.84403944  1.00604773  0.87727749  0.97090596  0.99621528]\n",
      "Minibatch loss at step 37000: 290491.687500\n",
      "Minibatch NMSE:  [ 0.7057634   0.7434631   0.96307677  0.80732149  0.60212642  0.80532092]\n",
      "Validation NMSE:  [ 0.81411064  0.86003804  0.96855372  0.77486908  0.66559857  0.72666651]\n",
      "Extrapolation NMSE:  [ 0.79971021  0.84827149  1.00552118  0.87573576  0.97137117  0.99465978]\n",
      "Minibatch loss at step 37500: 253632.062500\n",
      "Minibatch NMSE:  [ 1.07733357  1.02955425  1.03738558  0.65328223  0.97319019  0.92514271]\n",
      "Validation NMSE:  [ 0.82372957  0.86623567  0.97253472  0.77632409  0.66037959  0.72368509]\n",
      "Extrapolation NMSE:  [ 0.81335503  0.85565418  1.00361955  0.88477302  0.98625016  1.00631166]\n",
      "Minibatch loss at step 38000: 524750.000000\n",
      "Minibatch NMSE:  [ 1.00800169  0.9282034   0.9491387   0.77517533  0.80639094  0.96785313]\n",
      "Validation NMSE:  [ 0.8200314   0.86471665  0.97075003  0.77384931  0.65993017  0.72141933]\n",
      "Extrapolation NMSE:  [ 0.80766308  0.85357177  1.00400841  0.88547009  0.97781199  1.00042474]\n",
      "Minibatch loss at step 38500: 361510.812500\n",
      "Minibatch NMSE:  [ 1.01573372  0.97596467  0.97550505  0.65721059  0.68713713  0.95235926]\n",
      "Validation NMSE:  [ 0.8130666   0.86140198  0.97172326  0.77534163  0.65926617  0.72073525]\n",
      "Extrapolation NMSE:  [ 0.79961282  0.84835351  1.0072118   0.88076258  0.99286348  1.01143754]\n",
      "Minibatch loss at step 39000: 231376.828125\n",
      "Minibatch NMSE:  [ 0.83778423  0.87155598  1.00084627  1.01748431  0.6827606   0.72530681]\n",
      "Validation NMSE:  [ 0.82494932  0.86669916  0.97215587  0.77511996  0.66036558  0.72103107]\n",
      "Extrapolation NMSE:  [ 0.81320369  0.85734123  1.00584769  0.88074493  0.97021973  0.99202901]\n",
      "Minibatch loss at step 39500: 198231.359375\n",
      "Minibatch NMSE:  [ 0.63281351  0.78850979  0.98393804  0.73779255  0.52683818  0.77096182]\n",
      "Validation NMSE:  [ 0.81041348  0.85788876  0.97096848  0.77590275  0.65690643  0.71782959]\n",
      "Extrapolation NMSE:  [ 0.79295379  0.84368461  1.00951731  0.88716924  0.973634    0.99739271]\n",
      "Minibatch loss at step 40000: 189187.734375\n",
      "Minibatch NMSE:  [ 0.80413741  0.85400105  0.96416932  0.8132121   0.50448799  0.7729966 ]\n",
      "Validation NMSE:  [ 0.81798714  0.86212319  0.97196037  0.77393621  0.65871078  0.72009462]\n",
      "Extrapolation NMSE:  [ 0.80463415  0.84892243  1.00281239  0.87611812  0.96963888  0.99301785]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 40500: 354503.656250\n",
      "Minibatch NMSE:  [ 0.83729124  0.97373277  0.99104846  0.8696143   0.65537012  0.77304184]\n",
      "Validation NMSE:  [ 0.82368326  0.86432755  0.96897936  0.77514809  0.65709221  0.71744168]\n",
      "Extrapolation NMSE:  [ 0.80899292  0.85101593  1.00430119  0.87838572  0.97204465  0.99130583]\n",
      "Minibatch loss at step 41000: 220288.000000\n",
      "Minibatch NMSE:  [ 0.97140396  0.96906346  0.97212172  0.82646358  0.50556844  0.86900902]\n",
      "Validation NMSE:  [ 0.81325048  0.86352473  0.97039849  0.77337986  0.65703255  0.71833032]\n",
      "Extrapolation NMSE:  [ 0.79794705  0.84960586  1.00524747  0.88136911  0.97630483  0.99807805]\n",
      "Minibatch loss at step 41500: 307096.062500\n",
      "Minibatch NMSE:  [ 0.77464461  0.80623287  1.08700681  0.91394836  0.4014532   0.51247144]\n",
      "Validation NMSE:  [ 0.8147108   0.86234301  0.97074085  0.77353537  0.65752059  0.71760505]\n",
      "Extrapolation NMSE:  [ 0.79821914  0.848701    1.00684571  0.88169718  0.98639435  1.00702941]\n",
      "Minibatch loss at step 42000: 346190.812500\n",
      "Minibatch NMSE:  [ 0.83759063  0.91215134  0.96225756  0.92444909  0.75484562  0.91844678]\n",
      "Validation NMSE:  [ 0.82062322  0.8644864   0.97096884  0.77359152  0.6548394   0.71593404]\n",
      "Extrapolation NMSE:  [ 0.80744028  0.851762    1.00662291  0.8761133   0.98096424  1.00070345]\n",
      "Minibatch loss at step 42500: 278830.937500\n",
      "Minibatch NMSE:  [ 1.0612886   0.98532408  1.01443303  0.76950449  0.77776217  0.65199047]\n",
      "Validation NMSE:  [ 0.82501245  0.8671096   0.97154516  0.77497393  0.65605432  0.71607208]\n",
      "Extrapolation NMSE:  [ 0.81238526  0.85617781  1.00631177  0.88171583  0.98059231  0.99835956]\n",
      "Minibatch loss at step 43000: 145810.125000\n",
      "Minibatch NMSE:  [ 0.73476613  0.77289009  1.01527238  0.62617958  0.55412084  0.70486128]\n",
      "Validation NMSE:  [ 0.82413483  0.8665204   0.96851516  0.77331209  0.65317047  0.71583217]\n",
      "Extrapolation NMSE:  [ 0.80888802  0.85527188  1.00591779  0.88303542  0.98502839  1.00027812]\n",
      "Minibatch loss at step 43500: 289821.125000\n",
      "Minibatch NMSE:  [ 1.10464799  0.83852112  0.92994148  0.73066747  0.75226563  0.7890448 ]\n",
      "Validation NMSE:  [ 0.81678933  0.86473995  0.96959406  0.77169639  0.65344864  0.71567684]\n",
      "Extrapolation NMSE:  [ 0.80138636  0.85027516  1.00652325  0.87602299  0.97039223  0.99435455]\n",
      "Minibatch loss at step 44000: 393793.812500\n",
      "Minibatch NMSE:  [ 1.35076749  1.06265795  0.97977287  0.93403476  0.62323868  0.69618106]\n",
      "Validation NMSE:  [ 0.81983107  0.86551571  0.96848518  0.77364635  0.65462756  0.7150811 ]\n",
      "Extrapolation NMSE:  [ 0.80235636  0.85026461  1.00110221  0.87605166  0.96993935  0.98882252]\n",
      "Minibatch loss at step 44500: 282256.312500\n",
      "Minibatch NMSE:  [ 0.84907228  0.92669451  1.00048459  0.64803207  0.72584426  0.78762734]\n",
      "Validation NMSE:  [ 0.81850481  0.86465693  0.9691478   0.77140808  0.65473861  0.71748585]\n",
      "Extrapolation NMSE:  [ 0.80149817  0.85169828  1.00240982  0.88202232  0.98169035  1.00301111]\n",
      "Minibatch loss at step 45000: 364803.875000\n",
      "Minibatch NMSE:  [ 1.03774345  0.85773468  0.97608769  0.84416676  0.93741637  1.14232099]\n",
      "Validation NMSE:  [ 0.82087398  0.86449623  0.96898103  0.77250201  0.6520974   0.71290106]\n",
      "Extrapolation NMSE:  [ 0.80461925  0.8514204   1.00374365  0.8801406   0.97594482  0.99748707]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 45500: 161341.734375\n",
      "Minibatch NMSE:  [ 0.85864377  0.9351154   0.95774174  0.73143548  0.45604783  0.71940416]\n",
      "Validation NMSE:  [ 0.83155006  0.86994314  0.96933466  0.77193648  0.65609717  0.717978  ]\n",
      "Extrapolation NMSE:  [ 0.8182801   0.85996401  1.00463808  0.882375    0.98389912  1.01231515]\n",
      "Minibatch loss at step 46000: 426504.968750\n",
      "Minibatch NMSE:  [ 0.90003175  0.91834229  0.95942706  0.95675057  0.85725534  0.99187142]\n",
      "Validation NMSE:  [ 0.82799506  0.86793935  0.97148299  0.77227491  0.65114379  0.71314985]\n",
      "Extrapolation NMSE:  [ 0.81550997  0.85822606  1.00539768  0.88689512  0.9770838   1.00386155]\n",
      "Minibatch loss at step 46500: 155532.734375\n",
      "Minibatch NMSE:  [ 1.0830698   0.92380083  1.04532325  0.70025545  0.89908016  0.75619316]\n",
      "Validation NMSE:  [ 0.82271683  0.86546093  0.9704175   0.76955843  0.65120417  0.71187335]\n",
      "Extrapolation NMSE:  [ 0.80808127  0.85431254  1.00874233  0.88210535  0.97380888  0.99735862]\n",
      "Minibatch loss at step 47000: 364608.937500\n",
      "Minibatch NMSE:  [ 0.77972406  0.95142555  1.03796518  0.94535071  0.54075718  0.69990778]\n",
      "Validation NMSE:  [ 0.80437195  0.85584623  0.96838683  0.77123994  0.65168053  0.71319973]\n",
      "Extrapolation NMSE:  [ 0.78469485  0.83882374  1.00990605  0.8776201   0.98633951  1.00486493]\n",
      "Minibatch loss at step 47500: 292714.437500\n",
      "Minibatch NMSE:  [ 1.05993295  1.03312087  0.98762238  0.89332902  0.5656547   0.73154765]\n",
      "Validation NMSE:  [ 0.82884085  0.86725825  0.97134221  0.77098054  0.65109026  0.7110427 ]\n",
      "Extrapolation NMSE:  [ 0.81590176  0.85785317  1.00527883  0.88278192  0.96827346  0.98467791]\n",
      "Minibatch loss at step 48000: 283383.093750\n",
      "Minibatch NMSE:  [ 1.44222903  1.08005679  1.02972078  0.94214219  0.66009384  0.71531922]\n",
      "Validation NMSE:  [ 0.81242758  0.85999006  0.96997541  0.77205521  0.64827174  0.70965934]\n",
      "Extrapolation NMSE:  [ 0.79648399  0.84603721  1.00936639  0.88748461  0.99184036  1.012573  ]\n",
      "Minibatch loss at step 48500: 194675.015625\n",
      "Minibatch NMSE:  [ 0.79730034  0.94849157  0.95324934  1.01843405  0.60729867  0.73899078]\n",
      "Validation NMSE:  [ 0.82224017  0.86528254  0.9721114   0.77145356  0.64925009  0.71012104]\n",
      "Extrapolation NMSE:  [ 0.81045133  0.85459954  1.00543177  0.88593113  0.96769571  0.99412984]\n",
      "Minibatch loss at step 49000: 2021519.375000\n",
      "Minibatch NMSE:  [ 0.67994714  0.90801382  1.08078647  0.63784248  0.93281037  0.76998597]\n",
      "Validation NMSE:  [ 0.81259257  0.85867596  0.9696396   0.77119756  0.64816576  0.70929176]\n",
      "Extrapolation NMSE:  [ 0.79695904  0.84599108  1.00896406  0.8822453   0.96995407  0.99016136]\n",
      "Minibatch loss at step 49500: 241027.343750\n",
      "Minibatch NMSE:  [ 0.90232283  0.95369905  0.91800743  0.74679446  0.41333577  0.72836429]\n",
      "Validation NMSE:  [ 0.80943674  0.85951924  0.9705137   0.77003771  0.64863938  0.71060205]\n",
      "Extrapolation NMSE:  [ 0.79196215  0.84598964  1.00842667  0.88949817  0.97414631  0.99599403]\n",
      "Minibatch loss at step 50000: 279958.812500\n",
      "Minibatch NMSE:  [ 0.63330418  0.7740888   1.01750755  0.6946339   0.56273246  0.57200533]\n",
      "Validation NMSE:  [ 0.8276633   0.87042463  0.97017473  0.77025062  0.64678669  0.70817339]\n",
      "Extrapolation NMSE:  [ 0.81326681  0.86077517  1.00214195  0.88870305  0.98024464  1.00607133]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 50500: 284309.406250\n",
      "Minibatch NMSE:  [ 0.91938037  0.94058168  0.97916776  0.92707592  0.91476744  0.93261147]\n",
      "Validation NMSE:  [ 0.82040644  0.86493397  0.96977925  0.77002144  0.64731228  0.71029061]\n",
      "Extrapolation NMSE:  [ 0.80586869  0.85397106  1.00695956  0.87818062  0.97160286  0.99541253]\n",
      "Minibatch loss at step 51000: 311354.218750\n",
      "Minibatch NMSE:  [ 1.10072863  0.99978918  1.05108452  0.85733509  0.63741755  0.82967609]\n",
      "Validation NMSE:  [ 0.82682931  0.86768627  0.97013044  0.77187514  0.6485523   0.71029192]\n",
      "Extrapolation NMSE:  [ 0.81284821  0.85777706  1.00596261  0.88619912  0.99587369  1.01866972]\n",
      "Minibatch loss at step 51500: 171196.062500\n",
      "Minibatch NMSE:  [ 0.72551435  0.93797421  1.04077327  0.78572398  0.68265724  0.66365552]\n",
      "Validation NMSE:  [ 0.81941086  0.86321807  0.96852744  0.77025336  0.64446193  0.7077558 ]\n",
      "Extrapolation NMSE:  [ 0.80422831  0.85117966  1.00977743  0.88696414  0.98130971  1.00195253]\n",
      "Minibatch loss at step 52000: 331119.562500\n",
      "Minibatch NMSE:  [ 0.70724314  0.8301037   0.99999511  0.77253872  0.80495518  0.89991903]\n",
      "Validation NMSE:  [ 0.82613379  0.86705548  0.97087312  0.77116376  0.64635193  0.70868039]\n",
      "Extrapolation NMSE:  [ 0.81304741  0.8562324   1.00695264  0.88039333  0.97803813  1.00191033]\n",
      "Minibatch loss at step 52500: 234753.078125\n",
      "Minibatch NMSE:  [ 0.86879611  0.84582961  0.97741395  0.82630444  0.48190674  0.6351406 ]\n",
      "Validation NMSE:  [ 0.8182683   0.86435074  0.96844333  0.77281314  0.64599276  0.70746619]\n",
      "Extrapolation NMSE:  [ 0.80104268  0.84862888  1.0075897   0.87894398  0.96560991  0.98734993]\n",
      "Minibatch loss at step 53000: 216773.515625\n",
      "Minibatch NMSE:  [ 1.05305636  1.02180552  1.05646527  0.64630371  0.65965021  0.68035954]\n",
      "Validation NMSE:  [ 0.82508898  0.86725366  0.96868145  0.76911032  0.64643282  0.71153921]\n",
      "Extrapolation NMSE:  [ 0.80945963  0.85557067  1.00587654  0.88016301  0.99166548  1.00767076]\n",
      "Minibatch loss at step 53500: 233173.046875\n",
      "Minibatch NMSE:  [ 1.1096493   1.06355965  0.98855865  0.8536613   0.55405664  0.56848228]\n",
      "Validation NMSE:  [ 0.81914198  0.8629626   0.96962327  0.77015257  0.64425719  0.70606178]\n",
      "Extrapolation NMSE:  [ 0.80328441  0.85078758  1.00747037  0.88046956  0.97794974  1.00035667]\n",
      "Minibatch loss at step 54000: 179242.078125\n",
      "Minibatch NMSE:  [ 0.74432689  0.7684384   0.98952127  0.78878069  0.46016586  0.66194618]\n",
      "Validation NMSE:  [ 0.81729853  0.86047733  0.96846741  0.770329    0.64761168  0.70985979]\n",
      "Extrapolation NMSE:  [ 0.80115443  0.84754366  1.01151288  0.88055331  0.99162316  1.01250434]\n",
      "Minibatch loss at step 54500: 195035.250000\n",
      "Minibatch NMSE:  [ 1.04520655  0.98483026  1.03070533  0.79086488  0.54542583  0.7531026 ]\n",
      "Validation NMSE:  [ 0.82819116  0.86679286  0.9703241   0.77043957  0.64557803  0.70838934]\n",
      "Extrapolation NMSE:  [ 0.81422371  0.85658008  1.00532699  0.88545167  0.9927637   1.01047778]\n",
      "Minibatch loss at step 55000: 226255.921875\n",
      "Minibatch NMSE:  [ 0.76730531  0.84052461  1.04069626  1.01343358  0.69924819  0.8429985 ]\n",
      "Validation NMSE:  [ 0.82201183  0.86386967  0.96911573  0.76839107  0.64415497  0.7062524 ]\n",
      "Extrapolation NMSE:  [ 0.8049432   0.85127336  1.00694227  0.88462335  0.98009926  0.99804515]\n",
      "Logging model_params.mat ...\n",
      "Logging Ct_fit_onset.mat ...\n",
      "Logging Ct_fit_all.mat ...\n",
      "Minibatch loss at step 55500: 255833.625000\n",
      "Minibatch NMSE:  [ 1.05766094  1.00737667  0.97955787  0.9016484   0.9690761   1.04922688]\n",
      "Validation NMSE:  [ 0.81395668  0.85888028  0.96957475  0.76956034  0.64237052  0.70353776]\n",
      "Extrapolation NMSE:  [ 0.79611772  0.84591269  1.00765896  0.87914264  0.99714476  1.01525819]\n",
      "Minibatch loss at step 56000: 203372.843750\n",
      "Minibatch NMSE:  [ 1.14724708  1.11506033  0.96490514  0.74434066  0.65177059  0.67957824]\n",
      "Validation NMSE:  [ 0.83078671  0.86813575  0.97038037  0.77092046  0.64262921  0.70510733]\n",
      "Extrapolation NMSE:  [ 0.81647617  0.85849434  1.00544453  0.88213092  0.9780736   0.9973008 ]\n",
      "Minibatch loss at step 56500: 351150.093750\n",
      "Minibatch NMSE:  [ 0.97694987  0.96809679  0.96043289  1.06900454  0.70630944  0.78699112]\n",
      "Validation NMSE:  [ 0.8209691   0.86356056  0.96969867  0.7689141   0.64226484  0.70443332]\n",
      "Extrapolation NMSE:  [ 0.80349046  0.84978843  1.00775933  0.88826132  0.98532265  1.00378966]\n",
      "Minibatch loss at step 57000: 156537.078125\n",
      "Minibatch NMSE:  [ 0.88712734  1.00401378  1.02457726  1.01956797  0.76817465  0.83467567]\n",
      "Validation NMSE:  [ 0.82575577  0.86636513  0.970505    0.76768035  0.6426844   0.70479816]\n",
      "Extrapolation NMSE:  [ 0.81268811  0.85450715  1.00474739  0.88389683  0.98089862  1.00461757]\n",
      "Minibatch loss at step 57500: 295644.312500\n",
      "Minibatch NMSE:  [ 1.22632396  1.02673137  0.94659704  0.88264245  0.76631123  0.9292475 ]\n",
      "Validation NMSE:  [ 0.82583606  0.86373889  0.96930343  0.76898128  0.64225918  0.70318609]\n",
      "Extrapolation NMSE:  [ 0.81092626  0.85260308  1.00876915  0.88544375  0.97840816  0.99424714]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1fba536143bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# in the list passed to sess.run() and the value tensors will be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# returned in the tuple from the call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# write log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "batch_size = 64\n",
    "num_steps  = 700001\n",
    "\n",
    "# Number of units in hidden layer\n",
    "N_HIDDEN1_UNITS = 75\n",
    "\n",
    "# L2 Regularizer constant\n",
    "beta1 = 0.00000001\n",
    "\n",
    "logs_path = \"/tmp/ffnn/\"\n",
    "\n",
    "def defineFeedForwardNeuralNetworkModel(input_size, num_hidden1_units, output_size):\n",
    "    # Hidden 1 Layer\n",
    "    with tf.variable_scope('hidden1', reuse=False):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units], initializer=tf.constant_initializer(0))\n",
    "    # Linear (Output) Layer\n",
    "    with tf.variable_scope('linear', reuse=False):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, output_size], initializer=tf.random_normal_initializer(0.0, 1e-7))\n",
    "        biases = tf.get_variable('biases', [output_size], initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Build prediction graph.\n",
    "def performFeedForwardNeuralNetworkPrediction(train_dataset, input_size, num_hidden1_units, output_size, dropout_keep_prob):\n",
    "    \"\"\"Build the Feed-Forward Neural Network model for prediction.\n",
    "    Args:\n",
    "        train_dataset: training dataset's placeholder.\n",
    "        num_hidden1_units: Size of the 1st hidden layer.\n",
    "    Returns:\n",
    "        outputs: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1 = tf.nn.relu(tf.matmul(train_dataset, weights) + biases)\n",
    "#         hidden1 = tf.matmul(train_dataset, weights) + biases\n",
    "        hidden1_drop = tf.nn.dropout(hidden1, dropout_keep_prob)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        outputs = tf.matmul(hidden1_drop, weights) + biases\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Build training graph.\n",
    "def performFeedForwardNeuralNetworkTraining(outputs, labels, initial_learning_rate, input_size, num_hidden1_units, output_size):\n",
    "    \"\"\"Build the training graph.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Output tensor, float - [BATCH_SIZE, output_size].\n",
    "        labels : Labels tensor, float - [BATCH_SIZE, output_size].\n",
    "        initial_learning_rate: The initial learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates L2 prediction loss.\n",
    "    pred_l2_loss = tf.nn.l2_loss(outputs - labels, name='my_pred_l2_loss')\n",
    "    \n",
    "    # Create an operation that calculates L2 loss.\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        hidden1_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        output_layer_l2_loss = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    \n",
    "    loss = tf.reduce_mean(pred_l2_loss, name='my_pred_l2_loss_mean') + (beta1 * (hidden1_layer_l2_loss + output_layer_l2_loss))\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Exponentially-decaying learning rate:\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, 0.1)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "#     train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     train_op = tf.train.MomentumOptimizer(learning_rate, momentum=learning_rate/4.0, use_nesterov=True).minimize(loss, global_step=global_step)\n",
    "    train_op = tf.train.AdagradOptimizer(initial_learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return train_op, loss, learning_rate\n",
    "\n",
    "# Save model.\n",
    "def saveFeedForwardNeuralNetworkToMATLABMatFile(input_size, num_hidden1_units, output_size):\n",
    "    model_params={}\n",
    "    # Hidden 1\n",
    "    with tf.variable_scope('hidden1', reuse=True):\n",
    "        weights = tf.get_variable('weights', [input_size, num_hidden1_units])\n",
    "        biases = tf.get_variable('biases', [num_hidden1_units])\n",
    "        model_params['weights_1']=weights.eval()\n",
    "        model_params['biases_1']=biases.eval()\n",
    "    # Linear (Output)\n",
    "    with tf.variable_scope('linear', reuse=True):\n",
    "        weights = tf.get_variable('weights', [num_hidden1_units, output_size])\n",
    "        biases = tf.get_variable('biases', [output_size])\n",
    "        model_params['weights_out']=weights.eval()\n",
    "        model_params['biases_out']=biases.eval()\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "# Build the complete graph for feeding inputs, training, and saving checkpoints.\n",
    "ff_nn_graph = tf.Graph()\n",
    "with ff_nn_graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=[batch_size, D_input], name=\"tf_train_dataset_placeholder\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=[batch_size, D_output], name=\"tf_train_labels_placeholder\")\n",
    "    tf_train_all_dataset = tf.constant(X_train_dataset, name=\"tf_train_all_dataset_constant\")\n",
    "    tf_valid_dataset = tf.constant(X_valid_dataset, name=\"tf_valid_dataset_constant\")\n",
    "    tf_test_dataset = tf.constant(X_test_dataset, name=\"tf_test_dataset_constant\")\n",
    "    tf_whole_dataset = tf.constant(X_init_offset_cancelled, name=\"tf_whole_dataset_constant\")\n",
    "    tf_whole_all_dataset = tf.constant(X_init_offset_cancelled_all, name=\"tf_whole_all_dataset_constant\")\n",
    "    tf_extrapolate_test_dataset = tf.constant(X_extrapt, name=\"tf_extrapolate_test_dataset_constant\")\n",
    "    \n",
    "    # Currently turn off dropouts:\n",
    "    tf_train_dropout_keep_prob = 0.7\n",
    "    \n",
    "    # Define the Neural Network model.\n",
    "    defineFeedForwardNeuralNetworkModel(D_input, N_HIDDEN1_UNITS, D_output)\n",
    "    \n",
    "    # Build the Prediction Graph (that computes predictions from the inference model).\n",
    "    tf_outputs = performFeedForwardNeuralNetworkPrediction(tf_train_dataset, D_input, N_HIDDEN1_UNITS, D_output, tf_train_dropout_keep_prob)\n",
    "    \n",
    "    # Build the Training Graph (that calculate and apply gradients).\n",
    "    train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.1, D_input, N_HIDDEN1_UNITS, D_output)\n",
    "#     train_op, loss, learning_rate = performFeedForwardNeuralNetworkTraining(tf_outputs, tf_train_labels, 0.00001, D_input, N_HIDDEN1_UNITS, D_output)\n",
    "    \n",
    "    # Create a summary:\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "    \n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf_outputs\n",
    "    train_all_prediction = performFeedForwardNeuralNetworkPrediction(tf_train_all_dataset, D_input, N_HIDDEN1_UNITS, D_output, 1.0)\n",
    "    valid_prediction = performFeedForwardNeuralNetworkPrediction(tf_valid_dataset, D_input, N_HIDDEN1_UNITS, D_output, 1.0)\n",
    "    test_prediction  = performFeedForwardNeuralNetworkPrediction(tf_test_dataset, D_input, N_HIDDEN1_UNITS, D_output, 1.0)\n",
    "    whole_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_dataset, D_input, N_HIDDEN1_UNITS, D_output, 1.0)\n",
    "    whole_all_prediction  = performFeedForwardNeuralNetworkPrediction(tf_whole_all_dataset, D_input, N_HIDDEN1_UNITS, D_output, 1.0)\n",
    "    extrapolate_test_prediction = performFeedForwardNeuralNetworkPrediction(tf_extrapolate_test_dataset, D_input, N_HIDDEN1_UNITS, D_output, 1.0)\n",
    "\n",
    "# Run training for num_steps and save checkpoint at the end.\n",
    "with tf.Session(graph=ff_nn_graph) as session:\n",
    "    # Run the Op to initialize the variables.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in range(num_steps):\n",
    "        # Read a batch of input dataset and labels.\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (Ct_train.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = X_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = Ct_train[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        _, loss_value, predictions, summary = session.run([train_op, loss, train_prediction, summary_op], feed_dict=feed_dict)\n",
    "        \n",
    "        # write log\n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, loss_value))\n",
    "            print(\"Minibatch NMSE: \", computeNMSE(predictions, batch_labels))\n",
    "            print(\"Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "            print(\"Extrapolation NMSE: \", computeNMSE(extrapolate_test_prediction.eval(), Ctt_extrapt))\n",
    "        if (step % 5000 == 0):\n",
    "            model_params = saveFeedForwardNeuralNetworkToMATLABMatFile(D_input, N_HIDDEN1_UNITS, D_output)\n",
    "            print(\"Logging model_params.mat ...\")\n",
    "            sio.savemat('model_params/model_params.mat', model_params)\n",
    "            \n",
    "            whole_prediction_result = whole_prediction.eval()\n",
    "            whole_prediction_result_dict={}\n",
    "            whole_prediction_result_dict['whole_prediction_result'] = whole_prediction_result\n",
    "            print(\"Logging Ct_fit_onset.mat ...\")\n",
    "            sio.savemat('scraping/Ct_fit_onset.mat', whole_prediction_result_dict)\n",
    "            whole_all_prediction_result = whole_all_prediction.eval()\n",
    "            whole_all_prediction_result_dict={}\n",
    "            whole_all_prediction_result_dict['whole_all_prediction_result'] = whole_all_prediction_result\n",
    "            print(\"Logging Ct_fit_all.mat ...\")\n",
    "            sio.savemat('scraping/Ct_fit_all.mat', whole_all_prediction_result_dict)\n",
    "    print(\"Final Training NMSE  : \", computeNMSE(train_all_prediction.eval(), Ct_train))\n",
    "    print(\"Final Validation NMSE: \", computeNMSE(valid_prediction.eval(), Ct_valid))\n",
    "    print(\"Final Test NMSE      : \", computeNMSE(test_prediction.eval(), Ct_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
